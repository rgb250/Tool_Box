\subsection{Tools}

\paragraph{Bayesian concept learning}
Let be $\mathcal{D}$ the data, $h$ the hypothesis taken in account
\subparagraph{Likelihood}
$p\left(\mathcal{D|h}\right)$ the probability to get the observed data considering the 
hypothesis $h$.
\subparagraph{Prior}
$p(h)$ the probability of our hypothesis, many prior can be used, and this 
\textbf{subjective} aspect of Bayesian reasoning is a source of much controversy.

\subparagraph{Posterior}
The posterior is simply the likelihood times the prior, normalized.
\begin{center}
$p\left(h|\mathcal{D}\right) = \dfrac{p\left(\mathcal{D}|h\right)\times p(h)}{
    \su{h'\in\mathcal{H}}{}p\left(\mathcal{D}, h'\right)
}$
\end{center}
\subparagraph{MAP estimate}
As the denominator of the posterior formula does not change across the different 
hypothesis:
\begin{center}
    $\hat{h}_{MAP} = \displaystyle\argmax_{h} p\left(\mathcal{D}|h\right)p(h) = 
    \displaystyle\argmax_{h} \log\left(p\left(\mathcal{D}|h\right)p(h)\right) + \log\left(
       p(h)\right)
    $
\end{center}



\paragraph{Summarizing posterior distributions}
\subparagraph{MAP (Maximum A Posteriori)}
Although most appropriate choice for:
$
\begin{cases}
	\text{Real valued quantity} &\rightarrow \text{\emph{posterior median or mean}}\\
	\text{Discrete} &\rightarrow \text{\emph{vector of posterior marginals}}
\end{cases}
$\\
The most popular choice is \tB{\emph{posterior mode}} aka \tR{MAP}, because it reduces to
optimization problems for which efficient algorithms often exist.\\
Some point to be aware about MAP:
\begin{itemize}
	\item \sB{No measure of uncertainty}
	\item \tB{Plugging in the MAP estimate can result in overfitting}
	\item \tB{The mode is an untypical point}, unlike the mean or median the mode is a
		point of measure 0, it does not take the volume of the space into account.
	\item \tB{MAP estimation is not invariant to reparameterization}, for example 
		passing from centimeters to inches can break things.)\\ The MLE does not
		suffer from this since the likelihood is a function not a probability
		density
\end{itemize}

Remember that we have 

\subparagraph{Credible Intervals}
With point estimates, we want a measure of confidence. 
\tB{
$$ C_{\alpha}\left(\mathcal{D}\right) = (l, u): \Prob{{l\leq \theta \leq u | \mathcal{D}}}
$$}
In general, credible intervals are usually what people want to compute but confidence
intervals are usually what they actually compute, because most people are taught 
frequentist statistics but not Bayesian statistics.\\
Sometimes with central intervals there might be points be outside the CI which have higher
probability density.\\
More formally $p^{*}$ such that: 
\begin{center}
	$1-\alpha = 
	\Su{{\theta:p(\theta|\mathcal{D})>p^{*}}}{}p(\theta|\mathcal{D})d\theta$
\end{center}
Then the \sB{HPD} such that:
\begin{center}
	$\mathcal{D}=\left\{\theta: p(\theta|\mathcal{D})\geq p^{*}\right\}$
\end{center}
\paragraph{Bayesian model selection}
A more efficient approach than cross-validation, meaning fitting \emph{k} times each
model, is to compute the posterior over models.
$$
p(m|\mathcal{D}) = \dfrac{p(\mathcal{D}|m)p(m)}{\su{{m\in\mathcal{M}}}{}p(m|\mathcal{D})}
$$
From this we can compute the \tB{MAP model $\hat{m} = \displaystyle \argmax_{m}
p(m|\mathcal{D})$}\\
Then we have the \tB{marginal likelihood}: $p(\mathcal{D}|\hat{m}) = \Su{}{}p(
\mathcal{D}|\hat{m})p(\theta|\hat{m})d\theta$
\subparagraph{Baysian Occam's razor}
In integrating out the parameters rather than maximizing them we are automatically 
protected from overfitting: model with more parameters do not necessarily have higher 
marginal likelihood.\\
A way to understand the Bayesian Occam's razor effect is to remember that probabilities 
must sum to one, meaning $\su{{\mathcal{D}'}}{}p(\mathcal{D}'|m)=1$. Complex models, which
can predict many things, must spread their probability mass thinly, and hence will not
obtain as large a probability for any given data set as simpler models.

\subparagraph{Computing the marginal likelihood (evidence)}
For a fixed model we often write:
$$p(\bm{\theta}|\mathcal{D},m) \propto p(\bm{\theta}|m)p(\mathcal{D}|\bm{\theta},m)$$
This valid since $p(\mathcal{D}|m)$ is constant. However when comparing models we need
to know how to compute the marginal likelihood, $p(\mathcal{D}|m)$. In general this can
be quite hard, since we have to integrate over all possible parameter values, but when
we have a conjugate prior, it is easy to compute.\\
Let $p(\bm{\theta})=\dfrac{q(\bm{\theta})}{Z_{0}}$ be our prior, where $q(\bm{\theta})$
is an unnormalized distribution, and $Z_{0}$ is the normalization constant of the prior.
Let $p(\mathcal{D}|\bm{\theta})=\dfrac{q(\mathcal{D}|\bm{\theta})}{Z_{l}}$ be the 
likelihood, where $Z_{l}$ contains any constant factors in the likelihood. Finally let
$p(\bm{\theta}|\mathcal{D})=\dfrac{q(\bm{\theta}|\mathcal{D})}{Z_{N}}$ be our posterior
where $q(\bm{\theta}|\mathcal{D})=q(\mathcal{D}|\bm{\theta})q(\bm{\theta})$ is the 
unnormalized posterior, and $Z_{N}$ is the normalization constant of the posterior.\\
We have:
$
\begin{cases}
	p(\bm{\theta})= \dfrac{p(\mathcal{D}|\bm{\theta})p(\bm{\theta})}{p(\mathcal{D})}\\
	\dfrac{q(\bm{\theta}|\mathcal{D})}{Z_{N}} = \dfrac{q(\mathcal{D}|\bm{\theta})
	q(\bm{\theta})}{Z_{l}Z_{0}p(\mathcal{D})}\\
	p(\mathcal{D}) = \dfrac{Z_{N}}{Z_{0}Z_{l}}
\end{cases}
$

In general $p(\mathcal{D}|m) = \Su{}{}p(\mathcal{D}|\bm{\theta})p(\bm{\theta}|m)d\bm{
\theta}$ can be quite difficult to compute. 
Simpler approach
\begin{itemize}
	\item \textbf{BIC} simple approximation:
		\tB{$BIC \triangleq \log(p(\mathcal{D}|\bm{\hat{\theta}})) - 
		\dfrac{dof(\bm{\hat{\theta})}}{2}\log(N)\approx\log{p(\mathcal{D})}$}
	\item \textbf{AIC}:
		\tB{$AIC(m,\mathcal{D})\triangleq\log(p(\mathcal{D})\bm{\hat{\theta}}_{MLE
		}) -dof(m)$}\\
		This is derived from Frequentits framework and cannot be interpreted as 
		an approximation to the marginal likelihood. The penalty of AIC is less
		than BIC, it causes AIC pick more complex models. That can be better for 
		predictive accuracy.
	\item Effect of the prior.\\
		If the prior is unknown, the correct Bayesian procedure is to put a prior
		on the prior. That is we should put a prior on the hyper-parameter 
		$\alpha$ as well as the parameters $\bm{w}$. To compute the marginal 
		likelihood we should integrate out all unknowns, we should compute:
		\tB{$\Su{}{}\Su{}{}p(\mathcal{D}|\bm{w})p(\bm{w}|\alpha,m)p(\alpha|m)
		d\bm{w}d\alpha$}
		A computational shortcut is to optimize $\alpha$ rather than integrating
		it out. That is, we use \tV{$p(\mathcal{D}|m)\approx\Su{}{}p(\mathcal{D}
		\bm{w})p(\bm{w}|\alpha,m)d\bm{w}$}.
		where \tV{$\hat{\alpha} = \displaystyle \argmax_{\alpha} p(\mathcal{D}|
			\alpha,m) = \displaystyle \argmax_{\alpha}\Su{}{}p(\mathcal{D}|
		\bm{w})p(\bm{w}|\hat{\alpha},m)d\bm{w}$}
\end{itemize}
\subparagraph{Bayes Factors}
When prior on models is uniform, then model selection is equivalent to picking the model
with the highest marginal likelihood. Now suppose we just have two models we are 
considering, call them the null hypothesis, $M_{0}$ and the alternative hypothesis,
$M_{1}$.\\
\tB{$$BF_{1,0} \triangleq \dfrac{p(\mathcal{D}|M_{1})}{p(\mathcal{D}|M_{0})}=
	\dfrac{\frac{p(M_{1}|\mathcal{D})}{p(M_{0}|\mathcal{D})}}{\frac{p(M_{1})}{
	p(M_{0})}}
$$}
This is like a likelihood ratio, except we integrate out the parameters, which allows us
to compare models of different complexity. 
\begin{table}
	\begin{tabular}{|cl|}
		\hline
	\textbf{Bayes Factor} $\bm{BF(1,0)}$ & \textbf{Interpretation}\\
		\hline
		$BF<\frac{1}{100}$ & Decisive evidence for $M_{0}$\\
		\hline
		$BF<\frac{1}{10}$ & Strong evidence for $M_{0}$\\
		\hline
		$\frac{1}{10}<BF<\frac{1}{3}$ & Modest evidence for $M_{0}$\\
		\hline
		$\frac{1}{3}<BF<1$ & Weak evidence for $M_{0}$\\
		\hline
		$1<BF<3$ & Weak evidence for $M_{1}$\\
		\hline
		$3<BF<10$ & Modest evidence for $M_{1}$\\
		\hline
		$BF>10 $ &  Strong evidence for $M_{1}$\\
		\hline
		$BF>100$ & Decisive evidence for $M_{1}$\\
		\hline
	\end{tabular}
\end{table}

\subparagraph{Jeffreys-Lindley paradox}
Problems can arise when we use improper priors (i.e. priors that do not integrate to 1)
for model selection/ hypothesis testing, even though such priors may be acceptable for 
other purposes. Thus it is important to use proper priors when doing model selection.

\paragraph{Priors}
The most controversial aspect of Bayesian statistics is its reliance on priors
\subparagraph{Uninformative priors}
If we do not have strong evidence on what $\theta$ should be, it is common to use an
uninformative priors, to "let the data speak for itself".\\
One might think that the most uninformative prior would be the uniform distribution: 
$Beta(1, 1)$, but the posterior would then be: $\E{\theta|\mathcal{D}} =
\dfrac{N_{1}+1}{N_{1}+N_{0}+2}$, whereas the MLE is $\dfrac{N_{1}}{N_{1}+N_{0}}$.\\
As by decreasing the magnitude of the pseudo counts, we can lessen the impact of the 
prior, we can argue that the most non-informative prior is: 
$$\lm{\epsilon}{0} Beta(\epsilon, \epsilon) = Beta(0, 0)$$
Called the \emph{Haldane prior}, it is an improper prior.\\
In general it is advisable to perform a some kind of sensitivity analysis, in which one
checks how much one's conclusions or prediction change in response to change in the 
modelling assumptions which includes the choice of the prior and the likelihood as well.
If the conclusion are relatively insensitive to the modelling assumption, one can have
more confidence in the results.
\subparagraph{Jeffreys priors}
Harold Jeffreys designed a general purpose technique for creating non-informative priors.
The key observation is that if $p(\phi)$ is non-informative then any re-parametrization
of the prior, such as $\theta=h(\phi)$ for some function $h$ should also be 
non-informative.
\begin{itemize}
	\item Start with a variable change: $p_{\theta}(\theta) = p_{\phi}(\phi)\left|\dfrac{d\phi}{d\theta}\right|$
	\item Consider the following constraint: $p_{\phi}(\phi)\propto
		\sqrt{\mathcal{I}(\phi)}$, where $\mathcal{I}(\phi)$ is the Fisher 
		information.\\ $\mathcal{I}(\phi) \triangleq - \E{2 \times 
		\dfrac{d\log\left(p(X|\phi)\right)}{d\phi}}$. This a measure of the
		curvature of the expected negative log likelihood and hence a measure of
		stability of the MLE.
	\item Now $\dfrac{d\log(p(x|\theta))}{d\theta} = 
		\dfrac{d\log(p(X|\phi))}{d\phi}\dfrac{d\phi}{d\theta}$
	\item $\mathcal{I}(\theta) = \mathcal{I}(\phi)
		\left(\dfrac{d\phi}{d\theta}\right)^{2}$
	\item $\sqrt{\mathcal{I}(\theta)} = \sqrt{\mathcal{I}(\phi)}\left|\dfrac{d\phi}
		{d\theta}\right|$
	\item Finally $p_{\theta}(\theta) = p_{\phi}(\phi)\left|\dfrac{d\phi}
		{d\theta}\right| \propto \sqrt{\mathcal{I}(\phi)}\left|\dfrac{d\phi}
		{d\theta}\right| = \sqrt{\mathcal{I}(\theta)}$
\end{itemize}

\subparagraph{Robust priors}
To prevent an undue influence on the result, we build priors having heavy tails, which 
avoids forcing things to be too close to the prior mean.

\subparagraph{Mixture of conjugate priors}
Conjugate priors simplify the computation of robust priors, but are often not robust, and 
not flexible enough to encode our prior knowledge. However it turns out that a mixture of
conjugate priors is also conjugate, and seem to be a good compromise.

\paragraph{Hierarchical Bayes}
A key requirement for computing the posterior $p(\theta|\mathcal{D})$ is the 
specification of a prior $p(\theta|\eta)$ where $\eta$ are the hyper-parameters. A 
Bayesian approach is to put a prior on our priors. This is an example of a \textbf{
hierarchical Bayesian Model}.

\paragraph{Empirical Bayes}
In hierarchical Bayesian models, we need to compute the posterior on multiple levels of
latent variables. For example, in a two-level model, we need to compute:
$p(\eta, \theta|\mathcal{D}) \propto p(\mathcal{D}|\theta)p(\theta|\eta)p(\eta)$\\
We can approximate the posterior on the hyper-parameters with a point-estimate, 
$p(\eta|\mathcal{D}\approx \delta_{\hat{\eta}}(\eta))$ where $\hat{\eta}=\argmax_{\eta}
p(\eta|\mathcal{D})$. Since $\eta$ is typically much smaller than $\theta$ in 
dimensionality, it is less prone to overfitting, so we can safely use a uniform prior on 
$\eta$. Then the estimate becomes: 
$$ \hat{\eta} = \argmax_{\eta} p(\mathcal{D}|\eta) = \argmax_{\eta} \Su{}{}
p(\mathcal{D}|\theta)p(\theta|\mathcal{\eta})d\theta $$
This overall approach is called \textbf{Empirical Bayes}\\
Empirical Bayes violates the principle that the prior should be chosen independently of 
the data. However, we can just view it as a computationally cheap approximation to 
inference in a hierarchical Bayesian model, just as we viewed MAP estimation as an approximation to inference in the one level model $\theta \rightarrow \mathcal{D}$. In fact, we
can construct a hierarchy in which the more integrals one performs, the "more Bayesian" 
one becomes:
\begin{center}
	\begin{tabular}{|*{2}{l|}}
		\hline
		\textbf{Method} & \textbf{Definition} \\
		\hline
		Maximum likelihood & $\hat{\theta} = \argmax_{\theta} 
		p(\mathcal{D}|\theta)$ \\
		\hline
		MAP estimation & $\hat{\theta} = \argmax_{\theta} 
		p(\mathcal{D}|\theta)p(\theta|\eta)$ \\
		ML-II (Empirical Bayes) & $\hat{\eta}=\argmax_{\eta}\Su{}{}
		p(\mathcal{D}|\theta)p(\theta|\eta)d\theta = \argmax_{\eta}p(\mathcal{D}|
		\eta)$ \\
		\hline
		MAP-II & $\hat{\eta}=\argmax_{\eta}\Su{}{}
		p(\mathcal{D}|\theta)p(\theta|\eta)p(\eta)d\theta = \argmax_{\eta}p(
		\mathcal{D}| \eta)p(\eta)$\\
		\hline
		Full Bayes & $p(\theta, \eta|\mathcal{D}) \approx p(\mathcal{D}|\theta)
		p(\theta|\eta)p(\eta)$\\
		\hline
	\end{tabular}
\end{center}

\paragraph{Bayesian decision theory}
We can formalize any given statistical decision problem as a game against nature (as 
opposed to a game against other strategic players, which is the topic of game theory).
In this game, nature picks a state or parameter or label, $y\in \mathcal{Y}$, unknown to 
us, and then generates an observation, $\bm{x}\in\mathcal{X}$ which we get to see. We then
have to make a decision, that is, we have to choose an action $a$ from some \textbf{action
space} $\mathcal{A}$.Finally we incur some \textbf{loss}, $L(y, a)$, which measures how
compatible our action $a$ is with nature's hidden state $y$.\\
Our goal is to devise a decision procedure or policy, $\delta: \mathcal{X}\rightarrow
\mathcal{A}$ which specifies the optimal action for each possible input which specifies the optimal action for each possible input, meaning the action that minimizes the expected 
loss:
$$ \delta(\bm{x}) = \argmin_{{a\in \mathcal{A}}} \E{{L(y, a)}}$$
In the Bayesian vision, the expected value of $y$ given the data we have seen so far, 
whereas in the frequentist vision the expected value refers to $x$ and $y$ that we expect
to see in the future.\\
In the Bayesian visionj the optimal action having observed $\bm{x}$ is defined as the 
action $a$ that minimizes the \textbf{posterior expected loss}:
$$ \rho(a|\bm{x})\triangleq\mathbb{E}_{p(y|x)}\left(L(y, a)\right) = \su{y}{}L(y, a)
p(y|x)$$
Hence the Bayes estimator also called Bayes decision rule is given by:
$$\delta(\bm{x}) = \argmax_{a\in\mathcal{A}}\rho(\bm{a}|\bm{x})$$

\subparagraph{Bayes estimators for common loss functions}
\begin{itemize}
    \item \textbf{MAP} estimate minimizes 0-1 loss: $L(y, a) = \mathbb{I}_{y\neq a}
		\begin{cases}
			0 \text{ if } a = y\\
			1 \text{ else}
		\end{cases}$
    \item \textbf{Reject option}, in classification problems where $p(y|\bm{x})$ is very 
		uncertain we may prefer to choose a reject action, in which we refuse to 
		classify the example as any of the specified classes. Let choosing $a=C+1$
		correspond to picking the reject action, and choosing $a\in\{1,...,C\}$
		correspond to picking one of the classes.\\
		$L(y=j, a=i) = 
		\begin{cases}
			0 &\text{ if } i=j \text{ and } i,j\in\{1,...,C\}\\
			\lambda_{r} &\text{ if } i=C+1 \\
			\lambda_{s} &\text{ otherwise}
		\end{cases} $\\
		where $\lambda_{r}$ is the cost of the reject action, and $\lambda_{s}$ is
		the cost of a substitution error. 
    \item \textbf{Squared Error ($l_{2}$)} for a continuous parameters. $L(y, a) =
        (y-a)^{2}$
    \item \textbf{Absolute Error ($l_{1}$)} more robust against outliers. $L(y,a)=
		\lvert y-a\rvert$. The optimal point is the median.
    \item \textbf{Supervised learning} considering a prediction function $\delta: 
        \mathcal{X} \rightarrow \mathcal{Y}$ and some cost function $l(y, \delta(x))$. 
        Then the loss incurred by taking action $\delta$ when the unknown state of nature
        is $\theta$ (the parameters of the data generating the mechanism).
        $L(\bm{\theta}, \delta) \triangleq 
        \mathbb{E}_{(\bm{x}, y)~p(\bm{x},y|\bm{\theta})}\left(
        l(y, \delta(\bm{x}))\right)=\su{\bm{x}}{}\su{y}{}L\left(y,\delta(\bm{x}) 
        p(\bm{x},y|\bm{\theta})\right)$
    \item \textbf{False positive vs False negative trade-off} for binary decision problems
        three are 2 types of errors:
        \begin{enumerate}
            \item {false positive} (false alarm) if $\hat{y}=1 \wedge y=0$
            \item {false negative} (missed detection) if $\hat{y}=0 \wedge y=1$
        \end{enumerate}
        We can consider the loss matrix:\\
        \begin{tabular}{|*{3}{c|}}
            \hline
            \textbf{Headers} & $\bm{y=1}$ & $\bm{y=0}$\\
            \hline
            $\bm{\hat{y}=1}$ & 0 & $L_{FP}$\\
            \hline
            $\bm{\hat{y}=0}$ & $L_{FN}$ & 0\\
            \hline
        \end{tabular}
        where $L_{FN}$ is the cost of a false negative and $L_{FP}$ the cost of a false
        positive.

    \item \textbf{ROC curves} From the below table \\
        \begin{tabular}{|cc|*{3}{c|}}
            \hline
            \multicolumn{2}{|c}{\textbf{Headers}} & \multicolumn{2}{|c|}{\textbf{Truth}} &
            \textbf{Count}\\
            \hline
            \multirow{2}{*}{\textbf{Estimate}} & 1 & $TP$ & $FP$ & $\hat{N}_{+}=TP + FP$\\
                                               & 0 & $FN$ & $TN$ & $\hat{N}_{-}=FN + TN$\\
            \hline
            \multicolumn{2}{|c|}{\textbf{Count}} & $N_{+}=TP+FN$ & $N_{-}=FP+TN$ 
                                               & $N=N_{+}+N_{-}=\hat{N}_{+}+\hat{N}_{-}$\\
            \hline
        \end{tabular}\\
        we can generate the \emph{confusion matrix} is the below table\\
        \begin{tabular}{|*{3}{c|}}
            \hline
            \textbf{Headers} & $\bm{y=1}$ & $\bm{y=0}$\\
            \hline
            $\bm{\hat{y}=1}$ & $\dfrac{TP}{N}$ (sensitivity/recall) 
                           & $\dfrac{FP}{N}$ (error type I/ false alarm) \\
            \hline
            $\bm{\hat{y}=0}$ & $\dfrac{FN}{N}$ (error type II/ missed detection) 
                           & $\dfrac{TN}{N}$ (specificity) \\
            \hline
        \end{tabular}
    \item \textbf{Precision recall curves}
        When trying to detect a rare event the number of negatives is very large, hence
        comparing \emph{sensitivity} and \emph{the error of type I} is not very 
        informative. We would then like to use a measure that only talks about positives.
        \begin{itemize}
            \item \textbf{precision} $=\dfrac{TP}{\hat{N}_{+}}$
            \item \textbf{recall} $=\dfrac{TP}{N_{+}}$
        \end{itemize}
        A \textbf{precision recall curve} is a plot of \textit{precision} vs 
        \textit{recall}.
    \item \textbf{F-scores} is the \emph{harmonic mean of precision and recall}:\\
        $F_{1} \triangleq \dfrac{2}{\frac{1}{precision} + \frac{1}{recall}}$
\end{itemize}



\paragraph{Hypothesis Testing}
A \emph{Bayesian} statistics, probabilities represent subjective beliefs. 
Bayesian hypothesis testing provides rules for calculating how you should update your 
beliefs about different hypotheses in light of the evidence you see.\\


\paragraph{Posterior belief}
We use distributions to represent model parameters, we are uncertain about.\\
We start out with \emph{prior distribution}, representing our belief before we have seen 
our data.\\
We then see some data, and the data will be more consistent with some parameters than 
others.\\
The rules of Bayesian inference tell us how to update our beliefs about the parameters now
that we've seen the data to obtain posterior beliefs.\\
Bayesian distributions are easy to interpret. The mode of the distribution is the most 
likely value of the parameter.
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=\textwidth]{./chaps/10sec/images/2_posterior_belief.png}
	\end{center}
	\caption{Link between prior, likelihood and posterior}
	\label{fig:2_posterior_belief}
\end{figure}


\paragraph{Bayesian credible interval}
If $95\%$ of the distribution is between 2 values $\beta_{low}, \beta_{high}$ then 
according to the model there's a $95\%$ probability that the parameter is somewhere 
between these 2 values.\\
If we use uniform prior, the posterior distribution only depends on the data, and so we
end up with parameters that match the frequentist maximum-likelihood estimates: the 
posterior mode is the same as the maximum-likelihood value, and the credible intervals 
match the confidence intervals.
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=\textwidth]{./chaps/10sec/images/1_credible_interval.png}
	\end{center}
	\caption{Bayesian Credible Intervals}
	\label{fig:1_credible_interval}
\end{figure}

\paragraph{Credible interval tests}
$95\%$ credible interval are commonly used as a simple way to decide whether an effect is
real or not: if the credible interval does not include 0, the effect is genuine. If it 
includes 0, it might not be.\\
If we use uniform prior, we can infer on the \emph{p-value}.

\paragraph{Posterior Sign tests}
Once we've calculated our posterior distribution for $\beta$, we can interrogate it 
directly. For instance if $99\%$ of the posterior distribution is above 0, we are $99\%$
sure that $\beta>0$ ($\prob{\beta>0}=0.99$) and $1\%$ sure that $\beta<0$.\\
However for example if our posterior distribution is centered on $0$ we find that 
$\prob{\beta>0}=\prob{\beta<0}=0.5$. The reason for this is that we are considering only
2 hypotheses, while ignoring that $\beta=0$. To test the hypothesis that $\beta=0$ we 
need to calculate the Bayes Factor.\\
Interestingly, the posterior sign test is closely 
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=\textwidth]{./chaps/10sec/images/3_sign_test.png}
	\end{center}
	\caption{Assessing certainty}
	\label{fig: 3_sign_test}
\end{figure}

\paragraph{Bayes Factors}
\subparagraph{Prerequisites}
$\probC{H_{0}}{Data}$ is a average over all possible values of $\beta$, weighted by how
likely each value in according to the prior.
$ \begin{cases}
	\probC{H_{1}}{Data} = \su{{i=1}}{n}\probC{\beta_{i}}{Data}\prob{\beta_{i}}\text{
		Discrete}\\
	\probC{H_{1}}{Data} = \Su{}{}\probC{\beta_{i}}{Data}\prob{\beta_{i}}\text{
		Continuous}
\end{cases} $
Setting an appropriate prior is complicated, usually initial parameters are left.

\subparagraph{Definition}
\begin{center}
	$BF_{10}=\dfrac{\probC{H_{1}}{Data}}{\probC{H_{0}}{Data}}$\\
	$BF_{01}=\dfrac{1}{BF_{10}}$\\
\end{center}
Note that in general it is easier to find evidence in favour $H_{0}$ if $H_{1}$ 
distribution is broad and easier to find evidence against $H_{0}$ if $H_{1}$ is narrow.
\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Bayes Factor} & \textbf{Strength of evidence}\\
		\hline
		$BF=1$ & No evidence \\
		\hline
		$BF>1$ & Anecdotal evidence \\
		\hline
		$BF>3$ & Moderate \\
		\hline
		$BF>10$ & Strong \\
		\hline
		$BF>30$ & Very Strong \\
		\hline
		$BF>100$ & Extreme evidence \\
		\hline
	\end{tabular}
\end{center}

\subparagraph{Interpreting}
\textbf{Aim} : Finding how likely it is that $H_{1}$ is true and $H_{0}$ false.\\
Procedure: 
\begin{enumerate}
	\item Compute $BF_{10}$ using maximum-likelihood.
	\item Deciding how likely we thought it was the one or other hypothesis was true
		before seeing any data: \emph{the prior odds} $\prob{H_{1}}{H_{0}}$.\\
		Then there are 2 kinds of priors: prior beliefs about which hypothesis or 
		model is true, and prior beliefs about the values of the parameters in 
		each model.
	\item With the 2 above information compute the posterior distribution with Bayes'
		theorem\\ $\probC{Data}{H_{1}}=\dfrac{\probC{H_{1}}{Data}\prob{H_{1}}}{
		\probC{H_{0}}{Data}\prob{H_{0}}+\probC{H_{1}}{Data}\prob{H_{1}}}$
	\item 
\end{enumerate}
For example we get the following table:
\begin{center}
	\begin{tabular}{|*{4}{c|}}
		\hline
		$\mathbf{BF_{10}}$ & $\mathbf{BF_{01}}$ & \textbf{Prior} $\prob{H_{1}}$ &
		\textbf{Posterior} $\probC{Data}{H_{1}}$\\
		\hline
		$0.05$ & $20$ & $50\%$ & $4.8\%$\\
		\hline
		$0.1$ & $10$ & $50\%$ & $9.1\%$\\
		\hline
		$0.33$ & $3$ & $50\%$ & $25\%$\\
		\hline
		$1$ & $1$ & $50\%$ & $50\%$\\
		\hline
		$3$ & $0.33$ & $50\%$ & $75\%$\\
		\hline
		$10$ & $0.1$ & $50\%$ & $90.9\%$\\
		\hline
		$20$ & $0.05$ & $50\%$ & $95.2\%$\\
		\hline
	\end{tabular}
\end{center}


\paragraph{Multiple comparisons}
\subparagraph{Density Ratios}
\subparagraph{Posterior Estimates}
The more tests we run the more likely it is to we'll find at least one that is significant
even though the null hypothesis is true. We can then apply a Bonferroni correction.\\
Let's say we are running $k$ tests, we can either adjust: 
\begin{itemize}
	\item the threshold $\alpha_{adj} = \dfrac{\alpha}{k}$ OR
	\item the \emph{p-value} $p_{ajd} = k\times p$
\end{itemize}

