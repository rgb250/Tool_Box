Refers to the process by which principal components are computed, and
the subsequent use of these components in understanding the data.

\paragraph{Definition of principal components}
PCA finds a low-dimensional representation of a data set that contains
as much as possible of the variation. PCA seeks a small number of 
dimensions that are interesting as possible.\\

The \emph{first principal component} of a set of features 
$\prtH{X}{i}{1}{p}$ is the \emph{normalized} linear combination of the
features:
\begin{center}
\enc{$
Z_{1}=\su{{i=1}}{p}\phi_{i1}X_{i}
$}
\end{center}
that has the largest variance. \sB{By normalized}, we mean that 
\tB{$\su{{j=1}}{p}\phi_{j1}^{2}=1$} \\

The first principal component loading vector solves the optimization
problem:
\begin{center}
\enc{
$
\max\limits{\left(\phi_{i1}\right)_{1\leq i\leq n}}\left\{ \dfrac{1}{n}\su{{i=1}}{n}\left( \su{{j=1}}{p}\phi_{j1}x_{ij} \right)^{2} \right\}\text{ subject to }\su{{j=1}}{p}\phi_{j1}^{2}=1
$}
\end{center}
It turns out that constructing $Z_{2}$ to be uncorrelated with $Z_{1}$
is equivalent to constraining the direction $\phi_{2}$ to be orthogonal
to the direction $\phi_{1}$\\

The principal components of a set of data in $\mathbb{R}^{p}$ provide a sequence of best linear
approximations t that data, of all ranks $q\leq p$\\
Consider the rank-\emph{q} linear model:
$$ f(\lambda)=\mu + \bm{V}_{q}\lambda$$
where $\mu$ is a location vector in $\mathbb{R}^{p}$, $\bm{V}_{q}$ is a $p\times q$ orthogonal
unit vectors, and $\lambda$ is a $q$ vector of parameters. Fitting such a model to the data by
least squares amounts to minimizing the \textit{reconstruction error}:
$$ \min\limits_{\mu,\{\lambda_{i}\},\bm{V}_{q}}\su{{i=1}}{N}\norm{x_{i}-\mu-\bm{V}_{q}\lambda_{i}}
^{2}$$

\paragraph{Another interpretation of principal components}
An alternative interpretation for principal components can also be 
useful: principal components provide low-dimensional linear surfaces
that are \emph{closet} to the observations.\\
With the first principal component we seek a single dimension of the 
data that lies as close as possible to all of the data points.

\paragraph{More on PCA}
\subparagraph{Scaling the variables}
The results obtained when we perform PCA will also depend on whether
the variables have been individually scaled.
\subparagraph{Uniqueness of the principal component}
The sign has no effects as the direction does not change.
\subparagraph{The proportion of variance explained}
We are interested in knowing the \emph{Proportion of Variance
Explained} PVE by each principal component. \\
The total variance present in a daa set (assuming that the variables
have been centered to have mean zero) is defined as:
$$
\su{{j=1}}{p}\V{X_{j}}=\su{{j=1}}{p}\dfrac{1}{n}\su{{i=1}}{n}x_{ij}^{2}
$$

The variance explained by the $m^{th}$ principal component is:
$$
\dfrac{1}{n}\su{{i=1}}{n}z_{im}^{2}=\dfrac{1}{n}\su{{i=1}}{n}\left( \su{{j=1}}{p}\phi_{jm}x_{ij} \right)^{2}
$$

PVE:
$$
\dfrac{\su{{i=1}}{n}\left( \su{{j=1}}{p}\phi_{jm}x_{ij} \right)^{2}}{\su{{j=1}}{p}\su{{i=1}}{n}x_{ij}^{2}}
$$

\subparagraph{Deciding how many principal component to use}
We tipycally decide on the number of principal components required
to visualize the data by examining a \emph{scree pot}


\subparagraph{Python Code}
\begin{python}
import pandas as pd
import sklearn
from sklearn.decomposition import PCA

y, X = df.iloc[:, 0], df.iloc[:, 1:]
pca = PCA(
   n_components=2,
   svd_solver='full' # {'auto', 'full', 'arpack', 'randomized'}
   )
pca.fit(X)
print(pca.explained_variance_ratio_)
\end{python}
