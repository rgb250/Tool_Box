\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduction to Statistical Learning}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}Statistical Learning}{5}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}What is statistical Learning?}{5}{subsection.1.1.1}%
\contentsline {paragraph}{What is Statistical Learning?}{5}{section*.2}%
\contentsline {paragraph}{Why estimate $f$}{6}{section*.5}%
\contentsline {subparagraph}{Prediction}{6}{section*.6}%
\contentsline {subparagraph}{Inference}{6}{section*.7}%
\contentsline {paragraph}{How do we estimate $f$}{6}{section*.8}%
\contentsline {subparagraph}{Aim}{7}{section*.9}%
\contentsline {subparagraph}{Parametric methods}{7}{section*.10}%
\contentsline {subparagraph}{Non-Parametric methods}{7}{section*.12}%
\contentsline {paragraph}{The trade-off between Prediction Accuracy and model Interpretability}{8}{section*.13}%
\contentsline {paragraph}{Supervised versus unsupervised learning}{8}{section*.15}%
\contentsline {paragraph}{Regression versus Classification problems}{8}{section*.17}%
\contentsline {subsection}{\numberline {1.1.2}Assessing Model Accuracy}{8}{subsection.1.1.2}%
\contentsline {paragraph}{Measuring the quality of a fit}{8}{section*.18}%
\contentsline {paragraph}{The Bias-Variance Trade-Off}{9}{section*.20}%
\contentsline {subsection}{\numberline {1.1.3}Bias, Variance and Model Complexity}{10}{subsection.1.1.3}%
\contentsline {paragraph}{The classification setting}{10}{section*.22}%
\contentsline {section}{\numberline {1.2}Linear Regression}{10}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Simple linear regression}{11}{subsection.1.2.1}%
\contentsline {paragraph}{Estimating the Coefficients}{11}{section*.23}%
\contentsline {paragraph}{Assessing the Accuracy of the Coefficient Estimates}{11}{section*.26}%
\contentsline {paragraph}{Assessing the accuracy of the model}{13}{section*.30}%
\contentsline {subparagraph}{Residual Standard Error}{13}{section*.31}%
\contentsline {subparagraph}{$R^{2}$ statistic}{14}{section*.32}%
\contentsline {subsection}{\numberline {1.2.2}Multiple linear regression}{14}{subsection.1.2.2}%
\contentsline {paragraph}{Estimating the Regression Coefficients}{14}{section*.34}%
\contentsline {paragraph}{Hat Matrix}{15}{section*.37}%
\contentsline {subparagraph}{Range and Kernel of the Hat Matrix}{15}{section*.38}%
\contentsline {subparagraph}{Residual and Fitted Values}{15}{section*.39}%
\contentsline {subparagraph}{Geometric interpretation}{15}{section*.40}%
\contentsline {subparagraph}{Further information}{15}{section*.41}%
\contentsline {paragraph}{Some important questions when we perform multiple linear regression}{16}{section*.42}%
\contentsline {subparagraph}{Is there a relationship between the predictors and the responses?}{16}{section*.43}%
\contentsline {paragraph}{The Gauss-Markov Theorem}{17}{section*.45}%
\contentsline {paragraph}{Regression by Successive Orthogonalisation (Gram-Schmidt)}{17}{section*.46}%
\contentsline {subparagraph}{Algorithm}{18}{figure.caption.48}%
\contentsline {subparagraph}{Python code}{19}{section*.49}%
\contentsline {subparagraph}{R code}{19}{section*.50}%
\contentsline {paragraph}{Multiple Outputs}{19}{section*.51}%
\contentsline {subparagraph}{Model fit}{19}{section*.52}%
\contentsline {subparagraph}{Prediction}{20}{section*.55}%
\contentsline {subsection}{\numberline {1.2.3}Assumption Checking}{20}{subsection.1.2.3}%
\contentsline {paragraph}{Non-linearity of the Data}{22}{section*.56}%
\contentsline {paragraph}{Correlation of error terms}{22}{section*.57}%
\contentsline {subparagraph}{Definition Autocorrelation}{23}{section*.59}%
\contentsline {subparagraph}{Autocorrelation Function (ACF)}{23}{section*.60}%
\contentsline {subparagraph}{Breusch-Godfrey test}{24}{section*.61}%
\contentsline {paragraph}{Non-constant variance of error terms}{25}{section*.62}%
\contentsline {paragraph}{Generalized Linear Regression Model and Heteroscedasticity}{26}{section*.64}%
\contentsline {subparagraph}{Generalized Linear Regression Model }{26}{section*.65}%
\contentsline {subparagraph}{Definition Heteroscedasticity}{26}{section*.66}%
\contentsline {paragraph}{Weighted Least Squares (WLS)}{27}{section*.67}%
\contentsline {paragraph}{Generalized Least Squares (GLS)}{28}{section*.68}%
\contentsline {subparagraph}{Known covariance matrix}{28}{section*.69}%
\contentsline {subparagraph}{Unknown covariance matrix}{28}{section*.70}%
\contentsline {paragraph}{Heteroscedasticity}{29}{figure.caption.72}%
\contentsline {subparagraph}{Breusch and Pagan test}{32}{section*.73}%
\contentsline {subparagraph}{Outliers}{32}{section*.74}%
\contentsline {subparagraph}{High leverage points}{34}{section*.76}%
\contentsline {subparagraph}{Collinearity}{36}{section*.78}%
\contentsline {subsection}{\numberline {1.2.4}ANOVA \& ANCOVA}{37}{subsection.1.2.4}%
\contentsline {paragraph}{ANOVA}{37}{section*.82}%
\contentsline {subparagraph}{ANOVA models}{37}{section*.83}%
\contentsline {subparagraph}{\emph {One-way} ANOVA}{38}{section*.84}%
\contentsline {subparagraph}{Two-way-ANOVA}{40}{section*.87}%
\contentsline {paragraph}{ANCOVA}{41}{section*.88}%
\contentsline {subparagraph}{When is ANCOVA used?}{41}{section*.89}%
\contentsline {subparagraph}{Relation to Repeated Measures ANOVA}{41}{section*.90}%
\contentsline {subparagraph}{Adjusted Means}{42}{section*.91}%
\contentsline {subparagraph}{Should I use ANCOVA or Regression?}{42}{section*.92}%
\contentsline {subparagraph}{Sum of Squares}{42}{section*.93}%
\contentsline {section}{\numberline {1.3}Classification}{42}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Why not linear regression ?}{42}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Logistic regression}{43}{subsection.1.3.2}%
\contentsline {paragraph}{The logistic model}{43}{section*.94}%
\contentsline {paragraph}{The logistic model}{43}{section*.96}%
\contentsline {subparagraph}{Assumptions}{43}{section*.97}%
\contentsline {subparagraph}{Requirement}{43}{section*.98}%
\contentsline {subparagraph}{Formula}{43}{section*.99}%
\contentsline {paragraph}{Estimating the Regression Coefficients}{44}{section*.100}%
\contentsline {paragraph}{Marketing Plan}{44}{section*.101}%
\contentsline {paragraph}{Multiple Logistic Regression}{44}{section*.102}%
\contentsline {paragraph}{Logistic regression for $p$>2 response classes}{44}{section*.103}%
\contentsline {paragraph}{Fitting Logistic Regression Models}{44}{section*.104}%
\contentsline {paragraph}{Quadratic Approximations and Inference}{46}{section*.105}%
\contentsline {subsection}{\numberline {1.3.3}Linear discriminant analysis}{46}{subsection.1.3.3}%
\contentsline {paragraph}{Using Bayes Theorem for classification}{47}{section*.106}%
\contentsline {paragraph}{Linear Discriminant Analysis for p=1}{47}{section*.107}%
\contentsline {paragraph}{Linear Discriminant Analysis for p>1}{47}{section*.108}%
\contentsline {subparagraph}{Assumptions}{47}{section*.109}%
\contentsline {subparagraph}{Formulas}{48}{section*.110}%
\contentsline {paragraph}{Quadratic Discriminant Analysis}{48}{section*.111}%
\contentsline {paragraph}{Regularized Discriminant Analysis}{49}{section*.112}%
\contentsline {paragraph}{Computations for LDA}{49}{section*.113}%
\contentsline {paragraph}{Reduced-Rank Discriminant Analysis}{50}{section*.114}%
\contentsline {subsection}{\numberline {1.3.4}A comparison of classification methods}{50}{subsection.1.3.4}%
\contentsline {paragraph}{Logistic regression, LDA, QDA and KNN}{50}{section*.115}%
\contentsline {subparagraph}{Logistic regression VS LDA}{51}{section*.116}%
\contentsline {subparagraph}{KNN}{51}{section*.117}%
\contentsline {subparagraph}{QDA}{51}{section*.118}%
\contentsline {subsection}{\numberline {1.3.5}Separating Hyperplanes}{51}{subsection.1.3.5}%
\contentsline {paragraph}{Rosenblatt's Perceptron Learning Algorithm}{52}{section*.120}%
\contentsline {paragraph}{Optimal Separating Hyperplanes}{52}{section*.121}%
\contentsline {section}{\numberline {1.4}Resampling Methods}{53}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Cross-validation}{53}{subsection.1.4.1}%
\contentsline {paragraph}{The Validation Set Approach}{53}{section*.122}%
\contentsline {paragraph}{Leave-One-Out Cross-Validation}{53}{section*.123}%
\contentsline {paragraph}{k-Fold Cross-Validation}{54}{section*.124}%
\contentsline {paragraph}{Biais-Variance Trade-Off for k-Fold Cross-Validation}{54}{section*.125}%
\contentsline {paragraph}{Cross-Validation on Classification Problems}{55}{section*.126}%
\contentsline {subsection}{\numberline {1.4.2}The Bootstrap}{55}{subsection.1.4.2}%
\contentsline {section}{\numberline {1.5}Linear model selection and regularization}{55}{section.1.5}%
\contentsline {subsection}{\numberline {1.5.1}Subset selection}{55}{subsection.1.5.1}%
\contentsline {paragraph}{Best Subset Selection}{55}{section*.128}%
\contentsline {subparagraph}{Algorithm}{56}{section*.129}%
\contentsline {paragraph}{Stepwise Selection}{57}{section*.130}%
\contentsline {subparagraph}{Forward Stepwise Selection}{57}{section*.131}%
\contentsline {subparagraph}{Backward Stepwise Selection}{58}{section*.132}%
\contentsline {subparagraph}{Hybrid approach}{58}{section*.133}%
\contentsline {paragraph}{Optimism of the Training Error Rate}{58}{section*.134}%
\contentsline {paragraph}{The effective number of parameters}{59}{section*.135}%
\contentsline {paragraph}{Choosing the Optimal Model}{59}{section*.136}%
\contentsline {subparagraph}{$C_{p}$, AIC, BIC, and adjusted $R^{2}$}{59}{section*.137}%
\contentsline {paragraph}{Vapnik-Chervonenkis Dimension}{61}{section*.138}%
\contentsline {subparagraph}{Validation and Cross-Validation}{61}{section*.139}%
\contentsline {paragraph}{Bootstrap Methods}{61}{section*.140}%
\contentsline {subsection}{\numberline {1.5.2}Shrinkage methods}{62}{subsection.1.5.2}%
\contentsline {paragraph}{Ridge Regression}{62}{section*.141}%
\contentsline {subparagraph}{Definition}{62}{section*.142}%
\contentsline {subparagraph}{Why does Ridge Regression improve over Least Squares?}{64}{section*.144}%
\contentsline {paragraph}{The Lasso}{64}{section*.145}%
\contentsline {subparagraph}{Another formulation for Ridge Regression and the Lasso}{64}{section*.146}%
\contentsline {paragraph}{The variable selection property of the Lasso}{64}{section*.147}%
\contentsline {subparagraph}{Comparing the Lasso and Ridge Regression}{65}{section*.149}%
\contentsline {subparagraph}{Bayesian Interpretation}{65}{section*.150}%
\contentsline {paragraph}{Elastic-net}{67}{section*.153}%
\contentsline {paragraph}{Selecting the Tuning Parameter}{67}{section*.154}%
\contentsline {paragraph}{Least Angle Regression}{67}{section*.155}%
\contentsline {subparagraph}{Least Angle Regression}{67}{section*.156}%
\contentsline {subparagraph}{Least Angle Regression: Lasso Modification}{68}{section*.157}%
\contentsline {subsection}{\numberline {1.5.3}Dimension reduction methods}{68}{subsection.1.5.3}%
\contentsline {paragraph}{Principal Component Analysis}{68}{section*.158}%
\contentsline {subparagraph}{Intuitive}{68}{section*.159}%
\contentsline {subparagraph}{Methods}{68}{section*.160}%
\contentsline {subparagraph}{Mathematically}{69}{section*.161}%
\contentsline {subparagraph}{Principal Components Regression and Ridge Regression}{70}{section*.163}%
\contentsline {paragraph}{Partial Least Squares}{71}{section*.165}%
\contentsline {subparagraph}{Partial Least Squares}{71}{section*.166}%
\contentsline {subsection}{\numberline {1.5.4}Multiple Outcome Shrinkage and Selection}{72}{subsection.1.5.4}%
\contentsline {subparagraph}{Reduced-rank regression}{72}{section*.168}%
\contentsline {subsection}{\numberline {1.5.5}More on the Lasso and Related Path algorithms}{73}{subsection.1.5.5}%
\contentsline {paragraph}{Incremental Forward Stagewise Regression}{73}{section*.169}%
\contentsline {subparagraph}{Incremental Forward Stagewise Regression - $FS_{\epsilon }$}{73}{section*.170}%
\contentsline {subparagraph}{Least Angle Regression $FS_{0}$}{74}{section*.172}%
\contentsline {paragraph}{Piecewise-Linear Path Algorithms}{74}{section*.173}%
\contentsline {paragraph}{The Dantzig Selector}{74}{section*.174}%
\contentsline {paragraph}{The Grouped Lasso}{74}{section*.175}%
\contentsline {subsection}{\numberline {1.5.6}Consideration in high dimensions}{75}{subsection.1.5.6}%
\contentsline {paragraph}{High-Dimensional Data}{75}{section*.176}%
\contentsline {paragraph}{Regression in High Dimensions}{75}{section*.177}%
\contentsline {section}{\numberline {1.6}Moving beyond linearity}{75}{section.1.6}%
\contentsline {subsection}{\numberline {1.6.1}Polynomial Regression}{75}{subsection.1.6.1}%
\contentsline {paragraph}{Definition}{75}{section*.178}%
\contentsline {subsection}{\numberline {1.6.2}Regression splines}{75}{subsection.1.6.2}%
\contentsline {paragraph}{Piecewise Polynomials}{75}{section*.179}%
\contentsline {paragraph}{The spline Basis Representation}{76}{section*.180}%
\contentsline {paragraph}{Choosing the Number and Locations of the Knots}{77}{section*.182}%
\contentsline {paragraph}{Comparison to Polynomial Regression}{78}{section*.183}%
\contentsline {subsection}{\numberline {1.6.3}Smoothing splines}{78}{subsection.1.6.3}%
\contentsline {paragraph}{Definition}{78}{section*.184}%
\contentsline {paragraph}{Choosing the smoothing parameter $\lambda $}{79}{section*.185}%
\contentsline {paragraph}{Degrees of freedom refer to the number of free parameters}{80}{section*.186}%
\contentsline {paragraph}{Automatic Selection of the Smoothing Parameters}{81}{section*.187}%
\contentsline {subparagraph}{The Bias-Variance Tradeoff}{81}{section*.188}%
\contentsline {subsection}{\numberline {1.6.4}Multidimensional Splines}{81}{subsection.1.6.4}%
\contentsline {subsection}{\numberline {1.6.5}Wavelet Smoothing}{82}{subsection.1.6.5}%
\contentsline {paragraph}{Wavelet Bases and the Wavelet Transform}{83}{section*.191}%
\contentsline {paragraph}{Adaptive Wavelet Filtering}{84}{section*.192}%
\contentsline {subsection}{\numberline {1.6.6}Kernel Smoothing Methods}{84}{subsection.1.6.6}%
\contentsline {paragraph}{One-Dimensional Kernel Smoother }{84}{figure.caption.194}%
\contentsline {paragraph}{Local Linear Regression}{85}{section*.195}%
\contentsline {paragraph}{Local Polynomial Regression}{86}{section*.197}%
\contentsline {paragraph}{Selecting the Width of the Kernel}{87}{section*.199}%
\contentsline {paragraph}{Local Regression in $\mathbb {R}^{R}$}{87}{section*.200}%
\contentsline {paragraph}{Structured Local Regression Models in $\mathbb {R}^{p}$}{87}{section*.201}%
\contentsline {subparagraph}{Structured Kernels}{88}{section*.202}%
\contentsline {paragraph}{Structured Regression Functions}{88}{section*.203}%
\contentsline {subsection}{\numberline {1.6.7}Kernel Density Estimation and Classification}{88}{subsection.1.6.7}%
\contentsline {paragraph}{Kernel Density Estimation}{88}{section*.204}%
\contentsline {paragraph}{Kernel Density Classification}{89}{section*.205}%
\contentsline {subsection}{\numberline {1.6.8}Local regression}{89}{subsection.1.6.8}%
\contentsline {paragraph}{Span}{89}{section*.206}%
\contentsline {paragraph}{Algorithm: \emph {Local Regression} at $X=x_{0}$}{89}{figure.caption.208}%
\contentsline {subsection}{\numberline {1.6.9}Generalized additive models}{90}{subsection.1.6.9}%
\contentsline {paragraph}{Principle}{90}{section*.209}%
\contentsline {subparagraph}{Definition}{90}{section*.210}%
\contentsline {paragraph}{Fitting Additive Models}{90}{section*.211}%
\contentsline {subparagraph}{The Backfitting Algorithm for Additive Models}{90}{section*.212}%
\contentsline {subparagraph}{Pros and Cons of GAM's}{92}{section*.214}%
\contentsline {paragraph}{The Naive Bayes Classifier}{92}{section*.215}%
\contentsline {subsection}{\numberline {1.6.10}Radial Basis Functions and Kernels}{93}{subsection.1.6.10}%
\contentsline {subsection}{\numberline {1.6.11}Model Inference and Averaging}{93}{subsection.1.6.11}%
\contentsline {paragraph}{Maximum Likelihood Inference}{93}{section*.217}%
\contentsline {paragraph}{The EM Algorithm}{94}{section*.218}%
\contentsline {subparagraph}{EM Algorithm for 2-Component Gaussian Mixture}{94}{section*.219}%
\contentsline {paragraph}{The EM Algorithm in General}{95}{section*.220}%
\contentsline {subparagraph}{EM as a Maximization Procedure}{96}{section*.221}%
\contentsline {subparagraph}{Gibbs Sampler}{96}{section*.223}%
\contentsline {paragraph}{MCMC (Markov chain Monte Carlo) for Sampling from the Posterior}{96}{section*.224}%
\contentsline {subparagraph}{Gibbs sampling for mixtures}{97}{section*.225}%
\contentsline {paragraph}{Bagging}{97}{section*.226}%
\contentsline {paragraph}{Model Averaging and Stacking}{97}{section*.227}%
\contentsline {paragraph}{Stochastic Search: Bumping}{98}{section*.228}%
\contentsline {section}{\numberline {1.7}Tree-based-methods}{98}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}The basics of decision trees}{98}{subsection.1.7.1}%
\contentsline {paragraph}{Regression Trees}{98}{section*.229}%
\contentsline {subparagraph}{Predication via Stratification of the feature space }{98}{section*.230}%
\contentsline {subparagraph}{Tree Pruning}{99}{section*.231}%
\contentsline {paragraph}{Algorithm: Building a Regression Tree}{99}{section*.232}%
\contentsline {paragraph}{Classification Trees}{100}{section*.233}%
\contentsline {paragraph}{Advantages and Disadvantages of Trees}{100}{section*.234}%
\contentsline {paragraph}{PRIM: Bump Hunting}{101}{section*.235}%
\contentsline {subparagraph}{Example}{101}{section*.236}%
\contentsline {subparagraph}{Patient Rule Induction Method}{101}{section*.238}%
\contentsline {paragraph}{MARS: Multivariate Adaptive Regression Splines}{102}{section*.239}%
\contentsline {subparagraph}{Example}{103}{section*.240}%
\contentsline {subparagraph}{Cross-validation}{103}{section*.241}%
\contentsline {subsection}{\numberline {1.7.2}Bagging, RandomForest and Boosting}{103}{subsection.1.7.2}%
\contentsline {paragraph}{Bagging}{103}{section*.242}%
\contentsline {subparagraph}{Bootstrap aggregation (bagging) definition}{103}{section*.243}%
\contentsline {subparagraph}{Out-of-Bag Error Estimation}{104}{section*.244}%
\contentsline {paragraph}{Random Forest}{104}{section*.245}%
\contentsline {subparagraph}{Random Forest for Regression or Classification}{104}{section*.246}%
\contentsline {paragraph}{Boosting}{104}{section*.248}%
\contentsline {subparagraph}{Boosting for Regression Trees}{105}{section*.249}%
\contentsline {subparagraph}{Boosting methods}{105}{section*.250}%
\contentsline {subparagraph}{AdaBoost.M1}{106}{section*.252}%
\contentsline {paragraph}{Boosting Fits an Additive Model}{106}{section*.254}%
\contentsline {subparagraph}{Forward Stagewise Additive Modeling}{106}{section*.255}%
\contentsline {paragraph}{Forward Stagewise Additive Modeling}{107}{section*.256}%
\contentsline {paragraph}{Loss Functions and Robustness}{107}{section*.257}%
\contentsline {subparagraph}{Robust Loss Functions for Classification}{107}{section*.258}%
\contentsline {subparagraph}{Robust Loss Functions for Regression}{108}{section*.261}%
\contentsline {paragraph}{Procedures for Data Mining}{109}{figure.caption.265}%
\contentsline {paragraph}{Boosting Trees}{109}{section*.266}%
\contentsline {paragraph}{Numerical Optimization via Gradient Boosting}{109}{section*.267}%
\contentsline {subparagraph}{Steepest Descent}{110}{section*.268}%
\contentsline {section}{\numberline {1.8}Support vector machines}{111}{section.1.8}%
\contentsline {subsection}{\numberline {1.8.1}Maximal margin classifier}{111}{subsection.1.8.1}%
\contentsline {paragraph}{Definition}{111}{section*.270}%
\contentsline {paragraph}{Construction of the Maximal Margin Classifier}{111}{section*.272}%
\contentsline {subparagraph}{Computing the Support Vector Classifier}{112}{section*.274}%
\contentsline {subsection}{\numberline {1.8.2}Support vectors classifiers}{113}{subsection.1.8.2}%
\contentsline {paragraph}{Aim}{113}{section*.275}%
\contentsline {paragraph}{Details}{113}{section*.276}%
\contentsline {subsection}{\numberline {1.8.3}Support vectors machines}{114}{subsection.1.8.3}%
\contentsline {paragraph}{Classification with non-linear decision boundaries}{114}{section*.278}%
\contentsline {paragraph}{Support Vector Machine}{115}{section*.279}%
\contentsline {subparagraph}{Linear Kernel}{115}{section*.280}%
\contentsline {subparagraph}{Polynomial Kernel}{115}{section*.281}%
\contentsline {subparagraph}{Radial Kernel}{115}{section*.282}%
\contentsline {subsection}{\numberline {1.8.4}SVMs with more than 2 classes}{116}{subsection.1.8.4}%
\contentsline {paragraph}{One-Versus-One Classification}{116}{section*.285}%
\contentsline {paragraph}{One-Versus-All Classification}{116}{section*.286}%
\contentsline {subsection}{\numberline {1.8.5}Relationship to Ridge Regression}{116}{subsection.1.8.5}%
\contentsline {paragraph}{Regression and Kernels}{117}{section*.287}%
\contentsline {subsection}{\numberline {1.8.6}Linear Discriminant Analysis}{118}{subsection.1.8.6}%
\contentsline {paragraph}{Flexible Discriminant Analysis}{118}{section*.289}%
\contentsline {paragraph}{Penalized Discriminant Analysis}{119}{section*.290}%
\contentsline {section}{\numberline {1.9}Unsupervised learning}{119}{section.1.9}%
\contentsline {subsection}{\numberline {1.9.1}The challenge of unsupervised learning}{119}{subsection.1.9.1}%
\contentsline {subsection}{\numberline {1.9.2}Principal compoments analysis}{119}{subsection.1.9.2}%
\contentsline {paragraph}{Definition of principal components}{120}{section*.291}%
\contentsline {paragraph}{Another interpretation of principal components}{120}{section*.292}%
\contentsline {paragraph}{More on PCA}{120}{section*.293}%
\contentsline {subparagraph}{Scaling the variables}{120}{section*.294}%
\contentsline {subparagraph}{Uniqueness of the principal component}{120}{section*.295}%
\contentsline {subparagraph}{The proportion of variance explained}{121}{section*.296}%
\contentsline {subparagraph}{Deciding how many principal component to use}{121}{section*.297}%
\contentsline {subsection}{\numberline {1.9.3}Clustering methods}{121}{subsection.1.9.3}%
\contentsline {paragraph}{$K$-Means Clustering}{121}{section*.299}%
\contentsline {subparagraph}{Definition}{121}{section*.300}%
\contentsline {subparagraph}{Algorithm K-Means Clustering}{122}{section*.301}%
\contentsline {subparagraph}{Learning Vector Quantization}{123}{section*.304}%
\contentsline {paragraph}{Hierarchical Clustering}{123}{section*.305}%
\contentsline {subparagraph}{Interpreting a Dendogram}{124}{figure.caption.307}%
\contentsline {subparagraph}{Hierarchical Clustering}{124}{section*.308}%
\contentsline {paragraph}{Adaptive Nearest-Neighbor Methods}{124}{section*.310}%
\contentsline {section}{\numberline {1.10}Neural Networks}{125}{section.1.10}%
\contentsline {subsection}{\numberline {1.10.1}Projection Pursuit Regression}{125}{subsection.1.10.1}%
\contentsline {subsection}{\numberline {1.10.2}Neural Networks}{126}{subsection.1.10.2}%
\contentsline {subsection}{\numberline {1.10.3}Fitting Neural Networks}{127}{subsection.1.10.3}%
\contentsline {chapter}{\numberline {2}Deep Learning}{129}{chapter.2}%
\contentsline {section}{\numberline {2.1}Deep Forward Networks}{129}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Gradient based learning}{129}{subsection.2.1.1}%
\contentsline {paragraph}{Cost functions}{129}{section*.314}%
\contentsline {subparagraph}{Learning conditional distributions with Maximum Likelihood}{129}{section*.315}%
\contentsline {section}{\numberline {2.2}Regularization for Deep Learning}{129}{section.2.2}%
\contentsline {section}{\numberline {2.3}Optimization for training Deep Learning}{129}{section.2.3}%
\contentsline {section}{\numberline {2.4}Convolution Networks}{129}{section.2.4}%
\contentsline {section}{\numberline {2.5}Recurrent and Recursive Nets}{129}{section.2.5}%
