\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to Statistical Learning}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Statistical Learning}{5}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}What is statistical Learning?}{5}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Statistical Learning?}{5}{section*.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Estimation of $f$\relax }}{5}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1.1}{{1.1}{5}{Estimation of $f$\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Estimation of $f$ in $2-D$\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:1.1}{{1.2}{6}{Estimation of $f$ in $2-D$\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why estimate $f$}{6}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Prediction}{6}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Inference}{6}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How do we estimate $f$}{6}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Aim}{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Parametric methods}{7}{section*.10}\protected@file@percent }
\newlabel{fig:1.3_1}{{1.3a}{7}{Linear estimation:\\$income=\\\beta _{0}+\beta _{1}\times education+\beta _{2}\times seniority$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:1.3_1}{{a}{7}{Linear estimation:\\$income=\\\beta _{0}+\beta _{1}\times education+\beta _{2}\times seniority$\relax }{figure.caption.11}{}}
\newlabel{fig:1.3_2}{{1.3b}{7}{Best estimation of $f$ for\\ $income\approx f\left ( income,seniority \right )$\relax }{figure.caption.11}{}}
\newlabel{sub@fig:1.3_2}{{b}{7}{Best estimation of $f$ for\\ $income\approx f\left ( income,seniority \right )$\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Estimation of $f$ with 2 degrees of precision\relax }}{7}{figure.caption.11}\protected@file@percent }
\newlabel{fig:1.4}{{1.3}{7}{Estimation of $f$ with 2 degrees of precision\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subparagraph}{Non-Parametric methods}{7}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The trade-off between Prediction Accuracy and model Interpretability}{8}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces To know which methods to use.\relax }}{8}{figure.caption.14}\protected@file@percent }
\newlabel{fig:1.4}{{1.4}{8}{To know which methods to use.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Supervised versus unsupervised learning}{8}{section*.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Clustering methods is used for \emph  {unsupervised problems}.\relax }}{8}{figure.caption.16}\protected@file@percent }
\newlabel{fig:1.5}{{1.5}{8}{Clustering methods is used for \emph {unsupervised problems}.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Regression versus Classification problems}{8}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Assessing Model Accuracy}{8}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Measuring the quality of a fit}{8}{section*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Left:Data simulated from $f$, shown in black and 3 estimates of $f$ Right: Training MSE in grey and test MSE in red. Squares represent the training and test MSEs for the 3 fits shown in the left-hand pannel\relax }}{9}{figure.caption.19}\protected@file@percent }
\newlabel{fig:2.1}{{1.6}{9}{Left:Data simulated from $f$, shown in black and 3 estimates of $f$\\Right: Training MSE in grey and test MSE in red.\\Squares represent the training and test MSEs for the 3 fits shown in the left-hand pannel\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{The Bias-Variance Trade-Off}{9}{section*.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Squared bias, variance and test MSE. $\V {\epsilon }$ indicates by the dashed line.\relax }}{10}{figure.caption.21}\protected@file@percent }
\newlabel{fig:2.1}{{1.7}{10}{Squared bias, variance and test MSE.\\$\V {\epsilon }$ indicates by the dashed line.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Bias, Variance and Model Complexity}{10}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The classification setting}{10}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Linear Regression}{10}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Simple linear regression}{11}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estimating the Coefficients}{11}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces The least squares fit for the regression of sales onto TV.\relax }}{11}{figure.caption.24}\protected@file@percent }
\newlabel{fig:2.1}{{1.8}{11}{The least squares fit for the regression of sales onto TV.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Contour and 3-dimensionnal plots of the RSS on the Advertising data, using sales as the response and TV as the predictor\relax }}{11}{figure.caption.25}\protected@file@percent }
\newlabel{fig:2.2}{{1.9}{11}{Contour and 3-dimensionnal plots of the RSS on the Advertising data, using sales as the response and TV as the predictor\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Assessing the Accuracy of the Coefficient Estimates}{11}{section*.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces The left-hand pannel:red line is the popuation regression line, and in blue line the least squares estimate for $f(X)$ bassed on the observed data, shown in black. The right-hand pannel: is the same graph that left-hand pannel but with 10 other least sqaures estimates with each a distinct training data set but from the same model\relax }}{12}{figure.caption.27}\protected@file@percent }
\newlabel{fig:2.3}{{1.10}{12}{The left-hand pannel:red line is the popuation regression line, and in blue line the least squares estimate for $f(X)$ bassed on the observed data, shown in black.\\The right-hand pannel: is the same graph that left-hand pannel but with 10 other least sqaures estimates with each a distinct training data set but from the same model\relax }{figure.caption.27}{}}
\@writefile{tdo}{\contentsline {todo}{$SE\left (\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \beta _{1}$}\mathaccent "0362{\beta _{1}} \right )$ is smaller when the $x_{i}$ are more spread out}{12}{section*.28}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{9913630}{8187968}
\pgfsyspdfmark {pgfid2}{5670173}{8202075}
\pgfsyspdfmark {pgfid3}{9422108}{7976345}
\@writefile{tdo}{\contentsline {todo}{Roughly speaking when $\sigma ^{2}$ is estimated we should write $\setbox \z@ \hbox {\mathsurround \z@ $\textstyle SE\left (\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \beta _{1}$}\mathaccent "0362{\beta _{1}}\right )$}\mathaccent "0362{SE\left (\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \beta _{1}$}\mathaccent "0362{\beta _{1}}\right )}$}{13}{section*.29}\protected@file@percent }
\pgfsyspdfmark {pgfid6}{6571294}{41764786}
\pgfsyspdfmark {pgfid9}{33391901}{41778893}
\pgfsyspdfmark {pgfid10}{37143836}{41553163}
\@writefile{toc}{\contentsline {paragraph}{Assessing the accuracy of the model}{13}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Residual Standard Error}{13}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{$R^{2}$ statistic}{14}{section*.32}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{it can be shown that in the simple regression setting $R^{2}=r^{2}$}{14}{section*.33}\protected@file@percent }
\pgfsyspdfmark {pgfid11}{9913630}{28680864}
\pgfsyspdfmark {pgfid12}{5670173}{28694971}
\pgfsyspdfmark {pgfid13}{9422108}{28469241}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Multiple linear regression}{14}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estimating the Regression Coefficients}{14}{section*.34}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{No formulas are indicated in the book\ldots  }{14}{section*.35}\protected@file@percent }
\pgfsyspdfmark {pgfid16}{9913630}{18373892}
\pgfsyspdfmark {pgfid17}{5670173}{18387999}
\pgfsyspdfmark {pgfid18}{9422108}{18162269}
\newlabel{fig:2.1aLeastSquares}{{1.11a}{15}{$n$ observations\relax }{figure.caption.36}{}}
\newlabel{sub@fig:2.1aLeastSquares}{{a}{15}{$n$ observations\relax }{figure.caption.36}{}}
\newlabel{fig: 2.1bLeastSquares}{{1.11b}{15}{1 observation\relax }{figure.caption.36}{}}
\newlabel{sub@fig: 2.1bLeastSquares}{{b}{15}{1 observation\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Least squares for a linear model with $2$ predictors of $p$-dimensions.\relax }}{15}{figure.caption.36}\protected@file@percent }
\newlabel{fig:test}{{1.11}{15}{Least squares for a linear model with $2$ predictors of $p$-dimensions.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {paragraph}{Hat Matrix}{15}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Range and Kernel of the Hat Matrix}{15}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Residual and Fitted Values}{15}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Geometric interpretation}{15}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Further information}{15}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Some important questions when we perform multiple linear regression}{16}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Is there a relationship between the predictors and the responses?}{16}{section*.43}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Any statistical software can be used to compute the \emph  {$p$-value} associated with the \emph  {F-statistic}, based on this \emph  {$p$-value} we can determine whether or not reject $H_{0}$}{16}{section*.44}\protected@file@percent }
\pgfsyspdfmark {pgfid21}{13539514}{15554974}
\pgfsyspdfmark {pgfid22}{5670173}{15569081}
\pgfsyspdfmark {pgfid23}{9422108}{15343351}
\@writefile{toc}{\contentsline {paragraph}{The Gauss-Markov Theorem}{17}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression by Successive Orthogonalisation (Gram-Schmidt)}{17}{section*.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces The vector $\bm  {x}_{2}$ is regressed on the vector $\bm  {x}_{1}$, leaving the residual vector $\bm  {z}$. The regression of $\bm  {y}$ on $\bm  {z}$ gives the multiple regression coefficient of $\bm  {x}_{2}$.  Adding together the projections of $\bm  {y}$ on each of $\bm  {x}_{1}$ and $\bm  {z}$ gives the least squares fit $\hat  {\bm  {y}}$\relax }}{18}{figure.caption.48}\protected@file@percent }
\newlabel{fig:3.1regressionImage}{{1.12}{18}{The vector $\bm {x}_{2}$ is regressed on the vector $\bm {x}_{1}$, leaving the residual vector $\bm {z}$. The regression of $\bm {y}$ on $\bm {z}$ gives the multiple regression coefficient of $\bm {x}_{2}$.\\ Adding together the projections of $\bm {y}$ on each of $\bm {x}_{1}$ and $\bm {z}$ gives the least squares fit $\hat {\bm {y}}$\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subparagraph}{Algorithm}{18}{figure.caption.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Python code}{19}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{R code}{19}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multiple Outputs}{19}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Model fit}{19}{section*.52}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{this due to the fact that adding another variable to least square equations to fit the training data more accurately}{19}{section*.53}\protected@file@percent }
\pgfsyspdfmark {pgfid26}{19827806}{12177679}
\pgfsyspdfmark {pgfid29}{33391901}{12191786}
\pgfsyspdfmark {pgfid30}{37143836}{11966056}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces We can see that there is a pronounced non-linear relationship in data. Positive residuals tend to lie along the 45-degree line, the negative residuals tend to lie away from this line where budgets are more lopsided.\relax }}{20}{figure.caption.54}\protected@file@percent }
\newlabel{fig:fig 2.6}{{1.13}{20}{We can see that there is a pronounced non-linear relationship in data.\\Positive residuals tend to lie along the 45-degree line, the negative residuals tend to lie away from this line where budgets are more lopsided.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subparagraph}{Prediction}{20}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Assumption Checking}{20}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Non-linearity of the Data}{22}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Correlation of error terms}{22}{section*.57}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Residuals versus observation for given correlated coefficient, as we could plot for time series versus time.\relax }}{23}{figure.caption.58}\protected@file@percent }
\newlabel{fig:fig2.7}{{1.14}{23}{Residuals versus observation for given correlated coefficient, as we could plot for time series versus time.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {subparagraph}{Definition Autocorrelation}{23}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Autocorrelation Function (ACF)}{23}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Breusch-Godfrey test}{24}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Non-constant variance of error terms}{25}{section*.62}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Residuals plots. In each plot the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals and emphasis pattern. Left:The funnel shape indicates \emph  {heteroscedasticity}. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity .\relax }}{25}{figure.caption.63}\protected@file@percent }
\newlabel{fig:fig2.8}{{1.15}{25}{Residuals plots. In each plot the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals and emphasis pattern. Left:The funnel shape indicates \emph {heteroscedasticity}. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity .\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {paragraph}{Generalized Linear Regression Model and Heteroscedasticity}{26}{section*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Generalized Linear Regression Model }{26}{section*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Definition Heteroscedasticity}{26}{section*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weighted Least Squares (WLS)}{27}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generalized Least Squares (GLS)}{28}{section*.68}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Known covariance matrix}{28}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Unknown covariance matrix}{28}{section*.70}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Procedure to deal with heteroscedasticity\relax }}{29}{figure.caption.72}\protected@file@percent }
\newlabel{fig:6_heteroscedasticity}{{1.16}{29}{Procedure to deal with heteroscedasticity\relax }{figure.caption.72}{}}
\@writefile{toc}{\contentsline {paragraph}{Heteroscedasticity}{29}{figure.caption.72}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Breusch and Pagan test}{32}{section*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Outliers}{32}{section*.74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Left:The least squared regression is shown in red and the least squared regression after removal outlier is shown in dashed blue line. Center: the residual plot identifies clearly the outlier value. Right: The studentized residual plot identifies outliers too\relax }}{32}{figure.caption.75}\protected@file@percent }
\newlabel{fig:fig2.8}{{1.17}{32}{Left:The least squared regression is shown in red and the least squared regression after removal outlier is shown in dashed blue line. Center: the residual plot identifies clearly the outlier value. Right: The studentized residual plot identifies outliers too\relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {subparagraph}{High leverage points}{34}{section*.76}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces Left:Observation $41$ is a high leverage point, while 20 is not.The red line is the least squared fit to all data, and the blue line is the fit with observation $41$ removed.  Center:The red observation is not unusual in term of its $X_{1} $or its $X_{f2}$ value. Right: Observation $41$ has a high leverage and a high residual.\relax }}{35}{figure.caption.77}\protected@file@percent }
\newlabel{fig:fig2.8}{{1.18}{35}{Left:Observation $41$ is a high leverage point, while 20 is not.The red line is the least squared fit to all data, and the blue line is the fit with observation $41$ removed.\\ Center:The red observation is not unusual in term of its $X_{1} $or its $X_{f2}$ value.\\Right: Observation $41$ has a high leverage and a high residual.\relax }{figure.caption.77}{}}
\@writefile{toc}{\contentsline {subparagraph}{Collinearity}{36}{section*.78}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces Scatter plot of the observation from credit data set. \relax }}{36}{figure.caption.79}\protected@file@percent }
\newlabel{fig:fig2.8}{{1.19}{36}{Scatter plot of the observation from credit data set. \relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces In each plot the black dot represents the coefficient values corresponding to the minimum value of RSS.  Left: a contour plot of RSS for the regression of balance onto regression age and limit. Right:Because of the collinearity there are many pairs of $\beta _{Limit},\beta _{Rating}$ with a similar value of RSS.\relax }}{36}{figure.caption.80}\protected@file@percent }
\newlabel{fig:fig2.8}{{1.20}{36}{In each plot the black dot represents the coefficient values corresponding to the minimum value of RSS.\\ Left: a contour plot of RSS for the regression of balance onto regression age and limit.\\Right:Because of the collinearity there are many pairs of $\beta _{Limit},\beta _{Rating}$ with a similar value of RSS.\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces Model 1 is a multiple regression of balance onto age and limit, then Model 2 is one of balance onto rating and limit  The standard error of $\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \beta $}\mathaccent "0362{\beta }_{limit}$ increases 12-fold in the second regression, due to collinearity.\relax }}{37}{figure.caption.81}\protected@file@percent }
\newlabel{fig:fig2.8}{{1.21}{37}{Model 1 is a multiple regression of balance onto age and limit, then Model 2 is one of balance onto rating and limit \\The standard error of $\widehat {\beta }_{limit}$ increases 12-fold in the second regression, due to collinearity.\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}ANOVA \& ANCOVA}{37}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ANOVA}{37}{section*.82}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{ANOVA models}{37}{section*.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\emph  {One-way} ANOVA}{38}{section*.84}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Deviations for within-group of squares\relax }}{38}{figure.caption.85}\protected@file@percent }
\newlabel{fig:3_anovaInterGrp}{{1.22}{38}{Deviations for within-group of squares\relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces Deviations for betwwen-group sum of squares\relax }}{39}{figure.caption.86}\protected@file@percent }
\newlabel{fig:4_anovaBtw}{{1.23}{39}{Deviations for betwwen-group sum of squares\relax }{figure.caption.86}{}}
\@writefile{toc}{\contentsline {subparagraph}{Two-way-ANOVA}{40}{section*.87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ANCOVA}{41}{section*.88}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{When is ANCOVA used?}{41}{section*.89}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Relation to Repeated Measures ANOVA}{41}{section*.90}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Adjusted Means}{42}{section*.91}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Should I use ANCOVA or Regression?}{42}{section*.92}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Sum of Squares}{42}{section*.93}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Classification}{42}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Why not linear regression ?}{42}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Logistic regression}{43}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The logistic model}{43}{section*.94}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces Left:Estimated probability of default using linear regression. Some of estimated probability are negative! The orange ticks indicate the $0/1$ values coded for default (NO/YES) Right: Predicted probabilities of default using logistic regression. All probabilities lie between $0$ and $1$.\relax }}{43}{figure.caption.95}\protected@file@percent }
\newlabel{fig:fig3.2}{{1.24}{43}{Left:Estimated probability of default using linear regression. Some of estimated probability are negative! The orange ticks indicate the $0/1$ values coded for default (NO/YES)\\Right: Predicted probabilities of default using logistic regression.\\All probabilities lie between $0$ and $1$.\relax }{figure.caption.95}{}}
\@writefile{toc}{\contentsline {paragraph}{The logistic model}{43}{section*.96}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Assumptions}{43}{section*.97}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Requirement}{43}{section*.98}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Formula}{43}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estimating the Regression Coefficients}{44}{section*.100}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Marketing Plan}{44}{section*.101}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multiple Logistic Regression}{44}{section*.102}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Logistic regression for $p$>2 response classes}{44}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitting Logistic Regression Models}{44}{section*.104}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quadratic Approximations and Inference}{46}{section*.105}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Linear discriminant analysis}{46}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Using Bayes Theorem for classification}{47}{section*.106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Discriminant Analysis for p=1}{47}{section*.107}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Discriminant Analysis for p>1}{47}{section*.108}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Assumptions}{47}{section*.109}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Formulas}{48}{section*.110}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quadratic Discriminant Analysis}{48}{section*.111}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regularized Discriminant Analysis}{49}{section*.112}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computations for LDA}{49}{section*.113}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reduced-Rank Discriminant Analysis}{50}{section*.114}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}A comparison of classification methods}{50}{subsection.1.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Logistic regression, LDA, QDA and KNN}{50}{section*.115}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Logistic regression VS LDA}{51}{section*.116}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{KNN}{51}{section*.117}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{QDA}{51}{section*.118}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Separating Hyperplanes}{51}{subsection.1.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces The linear algebra of a hyperplane (affine set)\relax }}{52}{figure.caption.119}\protected@file@percent }
\newlabel{fig:1_hyperplaneAffine}{{1.25}{52}{The linear algebra of a hyperplane (affine set)\relax }{figure.caption.119}{}}
\@writefile{toc}{\contentsline {paragraph}{Rosenblatt's Perceptron Learning Algorithm}{52}{section*.120}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimal Separating Hyperplanes}{52}{section*.121}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Resampling Methods}{53}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Cross-validation}{53}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Validation Set Approach}{53}{section*.122}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Leave-One-Out Cross-Validation}{53}{section*.123}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{k-Fold Cross-Validation}{54}{section*.124}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Biais-Variance Trade-Off for k-Fold Cross-Validation}{54}{section*.125}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cross-Validation on Classification Problems}{55}{section*.126}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}The Bootstrap}{55}{subsection.1.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.26}{\ignorespaces Schematic of the bootstrap process\relax }}{55}{figure.caption.127}\protected@file@percent }
\newlabel{fig:3_bootstrap}{{1.26}{55}{Schematic of the bootstrap process\relax }{figure.caption.127}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Linear model selection and regularization}{55}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Subset selection}{55}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Best Subset Selection}{55}{section*.128}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Algorithm}{56}{section*.129}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stepwise Selection}{57}{section*.130}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Forward Stepwise Selection}{57}{section*.131}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Backward Stepwise Selection}{58}{section*.132}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Hybrid approach}{58}{section*.133}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimism of the Training Error Rate}{58}{section*.134}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The effective number of parameters}{59}{section*.135}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Choosing the Optimal Model}{59}{section*.136}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{$C_{p}$, AIC, BIC, and adjusted $R^{2}$}{59}{section*.137}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vapnik-Chervonenkis Dimension}{61}{section*.138}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Validation and Cross-Validation}{61}{section*.139}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bootstrap Methods}{61}{section*.140}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Shrinkage methods}{62}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ridge Regression}{62}{section*.141}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Definition}{62}{section*.142}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.27}{\ignorespaces The largest principal components is the direction that maximizes the variance of the projected data and the smallest principal component minimizes that variance. \leavevmode {\color {bleudefrance}Ridge regression projects $\bm  {y}$ onto these components, and then shrinks the coefficients of the low-variance components more than the high-variance components}\relax }}{63}{figure.caption.143}\protected@file@percent }
\newlabel{fig:5.4_PrincipalComponentsRidge}{{1.27}{63}{The largest principal components is the direction that maximizes the variance of the projected data and the smallest principal component minimizes that variance. \tB {Ridge regression projects $\bm {y}$ onto these components, and then shrinks the coefficients of the low-variance components more than the high-variance components}\relax }{figure.caption.143}{}}
\@writefile{toc}{\contentsline {subparagraph}{Why does Ridge Regression improve over Least Squares?}{64}{section*.144}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Lasso}{64}{section*.145}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Another formulation for Ridge Regression and the Lasso}{64}{section*.146}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The variable selection property of the Lasso}{64}{section*.147}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.28}{\ignorespaces Contours of the error and constraint functions for the lasso (left) and ridge regression (right). The solid blue areas are the constraint regressions, $|\beta _{1}|+|\beta _{2}|\leq s\text  { and }\beta _{1}^{2}+\beta _{2}^{2}\leq s$, while the red ellipses are the contours of the RSS (all of the points on a given ellipse share a common value of the RSS).\relax }}{65}{figure.caption.148}\protected@file@percent }
\newlabel{fig:5.1 RSSelipses}{{1.28}{65}{Contours of the error and constraint functions for the lasso (left) and ridge regression (right). The solid blue areas are the constraint regressions, $|\beta _{1}|+|\beta _{2}|\leq s\text { and }\beta _{1}^{2}+\beta _{2}^{2}\leq s$, while the red ellipses are the contours of the RSS (all of the points on a given ellipse share a common value of the RSS).\relax }{figure.caption.148}{}}
\@writefile{toc}{\contentsline {subparagraph}{Comparing the Lasso and Ridge Regression}{65}{section*.149}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Bayesian Interpretation}{65}{section*.150}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.29}{\ignorespaces Left: Ridge regression is the posterior mode for $\beta $ under a Gaussian prior.  Right: The lasso is the posterior mode for $\beta $ under a double-exponential prior.\relax }}{66}{figure.caption.151}\protected@file@percent }
\newlabel{fig:2bayesianPointOfView}{{1.29}{66}{Left: Ridge regression is the posterior mode for $\beta $ under a Gaussian prior.\\ Right: The lasso is the posterior mode for $\beta $ under a double-exponential prior.\relax }{figure.caption.151}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.30}{\ignorespaces Contours of constant value of ${\displaystyle \DOTSB \sum@ \slimits@ _{j}^{}}|\beta _{j}|^{q}$\relax }}{66}{figure.caption.152}\protected@file@percent }
\newlabel{fig:5.5_penaltyContours}{{1.30}{66}{Contours of constant value of $\su {j}{}|\beta _{j}|^{q}$\relax }{figure.caption.152}{}}
\@writefile{toc}{\contentsline {paragraph}{Elastic-net}{67}{section*.153}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Selecting the Tuning Parameter}{67}{section*.154}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Least Angle Regression}{67}{section*.155}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Least Angle Regression}{67}{section*.156}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Least Angle Regression: Lasso Modification}{68}{section*.157}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Dimension reduction methods}{68}{subsection.1.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Principal Component Analysis}{68}{section*.158}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Intuitive}{68}{section*.159}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Methods}{68}{section*.160}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Mathematically}{69}{section*.161}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.31}{\ignorespaces The population size and ad spending for 100 different cities are shown as purple circles. The green solid line indicates the first principal component, and the blue dashed line indicates the second principal compoment.\relax }}{70}{figure.caption.162}\protected@file@percent }
\newlabel{fig:3principalCompoment}{{1.31}{70}{The population size and ad spending for 100 different cities are shown as purple circles. The green solid line indicates the first principal component, and the blue dashed line indicates the second principal compoment.\relax }{figure.caption.162}{}}
\@writefile{toc}{\contentsline {subparagraph}{Principal Components Regression and Ridge Regression}{70}{section*.163}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Partial Least Squares}{71}{section*.165}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Partial Least Squares}{71}{section*.166}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.4}Multiple Outcome Shrinkage and Selection}{72}{subsection.1.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Reduced-rank regression}{72}{section*.168}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.5}More on the Lasso and Related Path algorithms}{73}{subsection.1.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Incremental Forward Stagewise Regression}{73}{section*.169}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Incremental Forward Stagewise Regression - $FS_{\epsilon }$}{73}{section*.170}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.32}{\ignorespaces The left pannel shows incremental \leavevmode {\color {bleudefrance}forward stagewise regression with step size $\epsilon =0.01$}. The right panel shows the infinitesimal version \leavevmode {\color {bleudefrance}$FS_{0}$ obtained letting $\epsilon \rightarrow 0$}\relax }}{73}{figure.caption.171}\protected@file@percent }
\newlabel{fig:7_coefficientsFS}{{1.32}{73}{The left pannel shows incremental \tB {forward stagewise regression with step size $\epsilon =0.01$}. The right panel shows the infinitesimal version \tB {$FS_{0}$ obtained letting $\epsilon \rightarrow 0$}\relax }{figure.caption.171}{}}
\@writefile{toc}{\contentsline {subparagraph}{Least Angle Regression $FS_{0}$}{74}{section*.172}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Piecewise-Linear Path Algorithms}{74}{section*.173}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Dantzig Selector}{74}{section*.174}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Grouped Lasso}{74}{section*.175}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.6}Consideration in high dimensions}{75}{subsection.1.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High-Dimensional Data}{75}{section*.176}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression in High Dimensions}{75}{section*.177}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Moving beyond linearity}{75}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Polynomial Regression}{75}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{75}{section*.178}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Regression splines}{75}{subsection.1.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Piecewise Polynomials}{75}{section*.179}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The spline Basis Representation}{76}{section*.180}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.33}{\ignorespaces A cubic spline and a natural cubic spline, with 3 knots, fit to a subset of the Wage data, confidence interval as dashed lines.\relax }}{76}{figure.caption.181}\protected@file@percent }
\newlabel{fig:6.1splines}{{1.33}{76}{A cubic spline and a natural cubic spline, with 3 knots, fit to a subset of the Wage data, confidence interval as dashed lines.\relax }{figure.caption.181}{}}
\@writefile{toc}{\contentsline {paragraph}{Choosing the Number and Locations of the Knots}{77}{section*.182}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison to Polynomial Regression}{78}{section*.183}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Smoothing splines}{78}{subsection.1.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{78}{section*.184}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Choosing the smoothing parameter $\lambda $}{79}{section*.185}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Degrees of freedom refer to the number of free parameters}{80}{section*.186}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Automatic Selection of the Smoothing Parameters}{81}{section*.187}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{The Bias-Variance Tradeoff}{81}{section*.188}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.4}Multidimensional Splines}{81}{subsection.1.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.34}{\ignorespaces A tensor product basis of $B-\text  {splines}$, showing some selected pairs. Each $2-$dimensionnal function is the tensor product of the corresponding one dimension marginals\relax }}{82}{figure.caption.189}\protected@file@percent }
\newlabel{fig: 2_tensorProduct.PNG}{{1.34}{82}{A tensor product basis of $B-\text {splines}$, showing some selected pairs. Each $2-$dimensionnal function is the tensor product of the corresponding one dimension marginals\relax }{figure.caption.189}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.5}Wavelet Smoothing}{82}{subsection.1.6.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.35}{\ignorespaces Some selected wavelets at different translations and dilatations for the Haar and symmlet families. The functions have been scaled to suit the display.\relax }}{83}{figure.caption.190}\protected@file@percent }
\newlabel{fig: 3_wavelet.PNG}{{1.35}{83}{Some selected wavelets at different translations and dilatations for the Haar and symmlet families. The functions have been scaled to suit the display.\relax }{figure.caption.190}{}}
\@writefile{toc}{\contentsline {paragraph}{Wavelet Bases and the Wavelet Transform}{83}{section*.191}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adaptive Wavelet Filtering}{84}{section*.192}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.6}Kernel Smoothing Methods}{84}{subsection.1.6.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.36}{\ignorespaces Left pannel is the result of a 30-nearest-neighbor running-mean smoother. The right panel is the result of a kernel-weighted average using \textit  {Epanechnikov} kernel with (half) window width $\lambda =0.2$\relax }}{84}{figure.caption.194}\protected@file@percent }
\newlabel{fig:40_localReg.PNG}{{1.36}{84}{Left pannel is the result of a 30-nearest-neighbor running-mean smoother. The right panel is the result of a kernel-weighted average using \textit {Epanechnikov} kernel with (half) window width $\lambda =0.2$\relax }{figure.caption.194}{}}
\@writefile{toc}{\contentsline {paragraph}{One-Dimensional Kernel Smoother }{84}{figure.caption.194}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local Linear Regression}{85}{section*.195}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.37}{\ignorespaces The true function is approximately linear, but most of the observations in the neighborhood have a higher mean than the target point, so despite weighting, their mean will be biased upwards. By fitting a locally weighted linear regression (right panel), this bias is removed to first order.\relax }}{85}{figure.caption.196}\protected@file@percent }
\newlabel{fig:4_localReg}{{1.37}{85}{The true function is approximately linear, but most of the observations in the neighborhood have a higher mean than the target point, so despite weighting, their mean will be biased upwards. By fitting a locally weighted linear regression (right panel), this bias is removed to first order.\relax }{figure.caption.196}{}}
\@writefile{toc}{\contentsline {paragraph}{Local Polynomial Regression}{86}{section*.197}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.38}{\ignorespaces The variances functions $\left \lVert l(x)\right \rVert ^{2}$ for local constant, linear and quadratic regression, for a metric bandwidth $(\lambda =0.2)$ tri-cube kernel\relax }}{86}{figure.caption.198}\protected@file@percent }
\newlabel{fig:5_linearPol}{{1.38}{86}{The variances functions $\norm {l(x)}^{2}$ for local constant, linear and quadratic regression, for a metric bandwidth $(\lambda =0.2)$ tri-cube kernel\relax }{figure.caption.198}{}}
\@writefile{toc}{\contentsline {paragraph}{Selecting the Width of the Kernel}{87}{section*.199}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local Regression in $\mathbb  {R}^{R}$}{87}{section*.200}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Structured Local Regression Models in $\mathbb  {R}^{p}$}{87}{section*.201}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Structured Kernels}{88}{section*.202}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Structured Regression Functions}{88}{section*.203}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.7}Kernel Density Estimation and Classification}{88}{subsection.1.6.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Kernel Density Estimation}{88}{section*.204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Kernel Density Classification}{89}{section*.205}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.8}Local regression}{89}{subsection.1.6.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Span}{89}{section*.206}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.39}{\ignorespaces Blue curve represents $f(x)$ from which the data were generated.  Orange curve corresponds to the local regression estimate $f(x)$.  Orange colored points are local to the target point $x_{0}$.  Yellow-bell-shape superimposed on the plot indicates weights assigned to each point, decreasing to zero with distance from the target point.\relax }}{89}{figure.caption.208}\protected@file@percent }
\newlabel{fig:6.1localRegression}{{1.39}{89}{Blue curve represents $f(x)$ from which the data were generated.\\ Orange curve corresponds to the local regression estimate $f(x)$.\\ Orange colored points are local to the target point $x_{0}$.\\ Yellow-bell-shape superimposed on the plot indicates weights assigned to each point, decreasing to zero with distance from the target point.\relax }{figure.caption.208}{}}
\@writefile{toc}{\contentsline {paragraph}{Algorithm: \emph  {Local Regression} at $X=x_{0}$}{89}{figure.caption.208}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.9}Generalized additive models}{90}{subsection.1.6.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Principle}{90}{section*.209}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Definition}{90}{section*.210}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fitting Additive Models}{90}{section*.211}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{The Backfitting Algorithm for Additive Models}{90}{section*.212}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Pros and Cons of GAM's}{92}{section*.214}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Naive Bayes Classifier}{92}{section*.215}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.10}Radial Basis Functions and Kernels}{93}{subsection.1.6.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.11}Model Inference and Averaging}{93}{subsection.1.6.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Maximum Likelihood Inference}{93}{section*.217}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The EM Algorithm}{94}{section*.218}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{EM Algorithm for 2-Component Gaussian Mixture}{94}{section*.219}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The EM Algorithm in General}{95}{section*.220}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{EM as a Maximization Procedure}{96}{section*.221}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.40}{\ignorespaces The contours of the (augmented) observed data log-likelihood $F(\theta ,\tilde  {P})$ The $E$ step is equivalent to maximizing the log-likelihood over the parameters of the latent data distribution. The $M$ step maximizes it over the parameters of the log-likelihood. The red curve corresponds to the observed data log-likelihood a profile obtained by maximizing $F(\theta ', \tilde  {P})$ for each value of $\theta '$\relax }}{96}{figure.caption.222}\protected@file@percent }
\newlabel{fig:3_emAlgo}{{1.40}{96}{The contours of the (augmented) observed data log-likelihood $F(\theta ,\tilde {P})$ The $E$ step is equivalent to maximizing the log-likelihood over the parameters of the latent data distribution. The $M$ step maximizes it over the parameters of the log-likelihood. The red curve corresponds to the observed data log-likelihood a profile obtained by maximizing $F(\theta ', \tilde {P})$ for each value of $\theta '$\relax }{figure.caption.222}{}}
\@writefile{toc}{\contentsline {subparagraph}{Gibbs Sampler}{96}{section*.223}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MCMC (Markov chain Monte Carlo) for Sampling from the Posterior}{96}{section*.224}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gibbs sampling for mixtures}{97}{section*.225}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bagging}{97}{section*.226}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Averaging and Stacking}{97}{section*.227}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stochastic Search: Bumping}{98}{section*.228}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Tree-based-methods}{98}{section.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}The basics of decision trees}{98}{subsection.1.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression Trees}{98}{section*.229}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Predication via Stratification of the feature space }{98}{section*.230}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Tree Pruning}{99}{section*.231}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algorithm: Building a Regression Tree}{99}{section*.232}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classification Trees}{100}{section*.233}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages and Disadvantages of Trees}{100}{section*.234}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{PRIM: Bump Hunting}{101}{section*.235}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Example}{101}{section*.236}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.41}{\ignorespaces The procedure starts with a rectangle (broken black lines) surronding all of the data, and then \leavevmode {\color {bleudefrance}peels away points along one edge by a prespecified amout in order to maximize the mean of the points remaining in the box}. The iteration number is indicated at the top of each panel.\relax }}{101}{figure.caption.237}\protected@file@percent }
\newlabel{fig:6_prim}{{1.41}{101}{The procedure starts with a rectangle (broken black lines) surronding all of the data, and then \tB {peels away points along one edge by a prespecified amout in order to maximize the mean of the points remaining in the box}. The iteration number is indicated at the top of each panel.\relax }{figure.caption.237}{}}
\@writefile{toc}{\contentsline {subparagraph}{Patient Rule Induction Method}{101}{section*.238}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MARS: Multivariate Adaptive Regression Splines}{102}{section*.239}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Example}{103}{section*.240}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Cross-validation}{103}{section*.241}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Bagging, RandomForest and Boosting}{103}{subsection.1.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bagging}{103}{section*.242}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Bootstrap aggregation (bagging) definition}{103}{section*.243}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Out-of-Bag Error Estimation}{104}{section*.244}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Random Forest}{104}{section*.245}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Random Forest for Regression or Classification}{104}{section*.246}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Boosting}{104}{section*.248}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Boosting for Regression Trees}{105}{section*.249}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Boosting methods}{105}{section*.250}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.42}{\ignorespaces Schematic of AdaBoost. Classifiers are trained on weighted versions of the dataset, and then combined to produce a final prediction.\relax }}{105}{figure.caption.251}\protected@file@percent }
\newlabel{fig:1_adaboost}{{1.42}{105}{Schematic of AdaBoost. Classifiers are trained on weighted versions of the dataset, and then combined to produce a final prediction.\relax }{figure.caption.251}{}}
\@writefile{toc}{\contentsline {subparagraph}{AdaBoost.M1}{106}{section*.252}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Boosting Fits an Additive Model}{106}{section*.254}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Forward Stagewise Additive Modeling}{106}{section*.255}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Stagewise Additive Modeling}{107}{section*.256}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss Functions and Robustness}{107}{section*.257}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Robust Loss Functions for Classification}{107}{section*.258}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.43}{\ignorespaces The response is $y=\pm 1$ the prediction is $f$, with class prediction $sign(f)$. The losses are misclassification: $I(sign(f)\neq  y)$; exponential: $exp(-yf)$; binomial deviance $\qopname  \relax o{log}(1+e^{-2yf})$; squared error $(y-f)^{2}$; and support vector: $(1-yf)_{+}$\relax }}{107}{figure.caption.259}\protected@file@percent }
\newlabel{fig:2_LossFunction}{{1.43}{107}{The response is $y=\pm 1$ the prediction is $f$, with class prediction $sign(f)$. The losses are misclassification: $I(sign(f)\neq y)$; exponential: $exp(-yf)$; binomial deviance $\log (1+e^{-2yf})$; squared error $(y-f)^{2}$; and support vector: $(1-yf)_{+}$\relax }{figure.caption.259}{}}
\@writefile{toc}{\contentsline {subparagraph}{Robust Loss Functions for Regression}{108}{section*.261}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.44}{\ignorespaces The Huber loss function combines the good properties of squared-error loss near zero and absolute error loss when $|y-f|$ is large\relax }}{108}{figure.caption.262}\protected@file@percent }
\newlabel{fig:3_LossFunction_reg}{{1.44}{108}{The Huber loss function combines the good properties of squared-error loss near zero and absolute error loss when $|y-f|$ is large\relax }{figure.caption.262}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.45}{\ignorespaces Some characteristics of different learning methods.Green=good, Yellow=fair, Red=poor\relax }}{109}{figure.caption.265}\protected@file@percent }
\newlabel{fig:4_compMeth}{{1.45}{109}{Some characteristics of different learning methods.Green=good, Yellow=fair, Red=poor\relax }{figure.caption.265}{}}
\@writefile{toc}{\contentsline {paragraph}{Procedures for Data Mining}{109}{figure.caption.265}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Boosting Trees}{109}{section*.266}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Numerical Optimization via Gradient Boosting}{109}{section*.267}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Steepest Descent}{110}{section*.268}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.46}{\ignorespaces Gradients for commonly used loss functions\relax }}{110}{figure.caption.269}\protected@file@percent }
\newlabel{fig:5_common_gradients}{{1.46}{110}{Gradients for commonly used loss functions\relax }{figure.caption.269}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Support vector machines}{111}{section.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}Maximal margin classifier}{111}{subsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition}{111}{section*.270}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.47}{\ignorespaces The margin is distance from the solid line to either of the dashed lines. The 2 points and the purple point that lie on the dashed lines are the support vectors\relax }}{111}{figure.caption.271}\protected@file@percent }
\newlabel{fig:8.1margineSVM}{{1.47}{111}{The margin is distance from the solid line to either of the dashed lines. The 2 points and the purple point that lie on the dashed lines are the support vectors\relax }{figure.caption.271}{}}
\@writefile{toc}{\contentsline {paragraph}{Construction of the Maximal Margin Classifier}{111}{section*.272}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.48}{\ignorespaces The left panel shows the separable case. The decision boundary is the solid line, while broken lines bound the shaded maximal margin of width $2M=\frac  {2}{\left \lVert \beta \right \rVert }$. The right panel shows the \emph  {non-separable} case. The point labeled $\xi _{j}^{*}$ are on the wrong side of their margin by an amount $\xi _{j}^{*}=M\xi _{j}$; points on the correct side have $\xi _{j}^{*}=0$. The margin is maximized subject to a total budget ${\displaystyle \DOTSB \sum@ \slimits@ _{}^{}}\xi _{i}\leq constant$. Hence ${\displaystyle \DOTSB \sum@ \slimits@ _{}^{}}\xi _{i}^{*}$ is the total distance of points on the wrong side.\relax }}{112}{figure.caption.273}\protected@file@percent }
\newlabel{fig:21_svC}{{1.48}{112}{The left panel shows the separable case. The decision boundary is the solid line, while broken lines bound the shaded maximal margin of width $2M=\frac {2}{\norm {\beta }}$. The right panel shows the \emph {non-separable} case. The point labeled $\xi _{j}^{*}$ are on the wrong side of their margin by an amount $\xi _{j}^{*}=M\xi _{j}$; points on the correct side have $\xi _{j}^{*}=0$. The margin is maximized subject to a total budget $\su {}{}\xi _{i}\leq constant$. Hence $\su {}{}\xi _{i}^{*}$ is the total distance of points on the wrong side.\relax }{figure.caption.273}{}}
\@writefile{toc}{\contentsline {subparagraph}{Computing the Support Vector Classifier}{112}{section*.274}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}Support vectors classifiers}{113}{subsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Aim}{113}{section*.275}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Details}{113}{section*.276}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.49}{\ignorespaces Left:Purple observations: 3, 4, 5 and 6 are on the correct side, 2 is on the margin and 1 is on the wrong side.  Blue observations: 7 and 10 are on the correct side, 9 on the margin and 8 on the wrong side  Right: same as left panel with two additional points, 11 and 12 which are on the wrong side\relax }}{114}{figure.caption.277}\protected@file@percent }
\newlabel{fig:8.1 supportVectorClassifier}{{1.49}{114}{Left:Purple observations: 3, 4, 5 and 6 are on the correct side, 2 is on the margin and 1 is on the wrong side.\\ Blue observations: 7 and 10 are on the correct side, 9 on the margin and 8 on the wrong side\\ Right: same as left panel with two additional points, 11 and 12 which are on the wrong side\relax }{figure.caption.277}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.3}Support vectors machines}{114}{subsection.1.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classification with non-linear decision boundaries}{114}{section*.278}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Support Vector Machine}{115}{section*.279}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Linear Kernel}{115}{section*.280}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Polynomial Kernel}{115}{section*.281}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Radial Kernel}{115}{section*.282}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.50}{\ignorespaces Left: a SVM with a polynomial kernel of degree 3 is applied to non-linear data.  Right: a SVM with a radial kernel is applied.\relax }}{116}{figure.caption.283}\protected@file@percent }
\newlabel{fig:8.3polynomialRadialKernel}{{1.50}{116}{Left: a SVM with a polynomial kernel of degree 3 is applied to non-linear data.\\ Right: a SVM with a radial kernel is applied.\relax }{figure.caption.283}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.4}SVMs with more than 2 classes}{116}{subsection.1.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One-Versus-One Classification}{116}{section*.285}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{One-Versus-All Classification}{116}{section*.286}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.5}Relationship to Ridge Regression}{116}{subsection.1.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression and Kernels}{117}{section*.287}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.6}Linear Discriminant Analysis}{118}{subsection.1.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flexible Discriminant Analysis}{118}{section*.289}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Penalized Discriminant Analysis}{119}{section*.290}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Unsupervised learning}{119}{section.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.1}The challenge of unsupervised learning}{119}{subsection.1.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.2}Principal compoments analysis}{119}{subsection.1.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition of principal components}{120}{section*.291}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Another interpretation of principal components}{120}{section*.292}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{More on PCA}{120}{section*.293}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Scaling the variables}{120}{section*.294}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Uniqueness of the principal component}{120}{section*.295}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{The proportion of variance explained}{121}{section*.296}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Deciding how many principal component to use}{121}{section*.297}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.3}Clustering methods}{121}{subsection.1.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$K$-Means Clustering}{121}{section*.299}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Definition}{121}{section*.300}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Algorithm K-Means Clustering}{122}{section*.301}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.51}{\ignorespaces Step1: Each observation is randomly assigned  Step(2a): The clusters are computed  Step(2b): Each observation is assigned to the nearest centroid.\relax }}{122}{figure.caption.302}\protected@file@percent }
\newlabel{fig:9.1kMeans}{{1.51}{122}{Step1: Each observation is randomly assigned\\ Step(2a): The clusters are computed\\ Step(2b): Each observation is assigned to the nearest centroid.\relax }{figure.caption.302}{}}
\@writefile{toc}{\contentsline {subparagraph}{Learning Vector Quantization}{123}{section*.304}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Clustering}{123}{section*.305}\protected@file@percent }
\newlabel{fig:9.2a_hierarchicalCluster}{{1.52a}{123}{Original Data Set\relax }{figure.caption.307}{}}
\newlabel{sub@fig:9.2a_hierarchicalCluster}{{a}{123}{Original Data Set\relax }{figure.caption.307}{}}
\newlabel{fig:9.2b_hierarchicalCluster}{{1.52b}{123}{Dendrogram\relax }{figure.caption.307}{}}
\newlabel{sub@fig:9.2b_hierarchicalCluster}{{b}{123}{Dendrogram\relax }{figure.caption.307}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.52}{\ignorespaces  There are in reality 3 classes, but we will ignore them and seek to cluster the observations.  In the right figure:  Left: Dendogram obtained from hierarchically clustering the data of of the left figure.  Center: The Dendrogram from the left-hand panel cut at a heigh of 9. This cut results in 2 distinct clusters.  Right: The Dendrogram from the left-hand panel cut at a heigh of 5. This cut results in 3 distincts clusters. .\relax }}{123}{figure.caption.307}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Interpreting a Dendogram}{124}{figure.caption.307}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Hierarchical Clustering}{124}{section*.308}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adaptive Nearest-Neighbor Methods}{124}{section*.310}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Neural Networks}{125}{section.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.1}Projection Pursuit Regression}{125}{subsection.1.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.53}{\ignorespaces  Perspective plots of 2 ridge functions.  Left: $g(V)=\genfrac  {}{}{}0{1}{1+e^{-5(V-0.5)}}$, where $V=\frac  {X_{1}+X_{2}}{\sqrt  (2)}$  Right: $g(V)=(V+0.1)\qopname  \relax o{sin}\left (\genfrac  {}{}{}0{1}{ \frac  {1}{3}+0.1}\right )$, where $V=X_{1}$\relax }}{125}{figure.caption.311}\protected@file@percent }
\newlabel{fig:1_ridge_fct}{{1.53}{125}{Perspective plots of 2 ridge functions.\\ Left: $g(V)=\dfrac {1}{1+e^{-5(V-0.5)}}$, where $V=\frac {X_{1}+X_{2}}{\sqrt (2)}$\\ Right: $g(V)=(V+0.1)\sin \left (\dfrac {1}{ \frac {1}{3}+0.1}\right )$, where $V=X_{1}$\relax }{figure.caption.311}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.2}Neural Networks}{126}{subsection.1.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.54}{\ignorespaces Schematic of a single hidden layer, feed-forward neural network\relax }}{126}{figure.caption.312}\protected@file@percent }
\newlabel{fig:99_2_networkDiag}{{1.54}{126}{Schematic of a single hidden layer, feed-forward neural network\relax }{figure.caption.312}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.3}Fitting Neural Networks}{127}{subsection.1.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Learning}{129}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Forward Networks}{129}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Gradient based learning}{129}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cost functions}{129}{section*.314}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Learning conditional distributions with Maximum Likelihood}{129}{section*.315}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Regularization for Deep Learning}{129}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Optimization for training Deep Learning}{129}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Convolution Networks}{129}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Recurrent and Recursive Nets}{129}{section.2.5}\protected@file@percent }
