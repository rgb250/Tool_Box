Why do we need another method that logistic regression in the case of
several response classes?
\begin{itemize}
	\item When classes are well-separated, the \sB{parameter 
		estimates for the logistic regression model are
		unstable}
	\item If \sB{$n$ is small and the distribution of the 
		predictors is approximately normal in each classe}, the
		\emph{linear discriminant model} is \tB{more stable 
		than the logistic.}
	\item Linear discriminant analysis is popular when we have 
		\sB{more than $2$ response classes}.
 \end{itemize}
\paragraph{Using Bayes Theorem for classification}
Let \tB{$\pi_{k}$ represents the prior probability} that a randomly
chosen observation comes from the $k^{th}$ class. Let $f_{k}\text{ such
that }\tB{f_{k}(x) \equiv\ProbC{X=x}{Y=k}}$ the density function of $X$
for an observation that comes from $k^{th}$ class:\\
\enc{$\ProbC{X=x}{Y=k}=\dfrac{\pi_{k}f_{k}(x)}{\su{{j=1}}{K}\pi_{j}f_{j}(x)}$}
\paragraph{Linear Discriminant Analysis for p=1}
Aims:
\begin{enumerate}
	\item \tB{To obtain an estimate for $f_{k}(x)$} that we can plug 
		into\\ $\ProbC{X=x}{Y=k}=\dfrac{\pi_{k}f_{k}(x)}{\su{{j=1}}{K}\pi_{j}f_{j}(x)}$
	\item \tB{To classify an observation to the class for which $p_{k}(x)$} is greatest
\end{enumerate}
Assumption:
$f_{k}(x)$ is \emph{Gaussian} that is $f_{k}(x)=\dfrac{1}{\sqrt{2\pi\sigma_{k}}}e^{-\frac{1}{2\sigma_{k}^{2}}(x-\mu_{k})^{2}}$\\
Knowing that $\sigma_{1}^{2}= \cdots = \sigma_{K}^{2}$ and taking the log of $p_{k}(x)=\dfrac{\pi_{k}\dfrac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2\sigma^{2}}(x-\mu_{k})^{2}}}{\su{{j=1}}{K}\pi_{j}\dfrac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2\sigma^{2}}(x-\mu_{j})^{2}}}$\\
It is equivalent to assigning the observation to the classer for which:
\\
\enc{$\delta_{k}(x)=x\frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2\sigma^{2}}+ln(\pi_{k})$} is the largest.\\
The \emph{linear discriminant analysis} (LDA) method approximates the
Bayes classifier by plugging estimates for $\pi_{k},\mu_{k}\text{ and }\sigma^{2}$:\\
\encB{
$
\begin{cases}
	\hat{\pi}_{k} = \frac{n_{k}}{n}\\
	\hat{\mu}_{k} = \dfrac{1}{n_{k}}\su{{i:y_{i}=k}}{}x_{i}\\
	\hat{\sigma}^{2} = \dfrac{1}{n-K}\su{{k=1}}{K}\su{{i:y_{i}=k}}{}\left(x_{i}-\hat{\mu}_{k}\right)^{2}
\end{cases}$}

\paragraph{Linear Discriminant Analysis for p>1}
\subparagraph{Assumptions}
\begin{enumerate}
	\item Multivariate normality: independent variables are normal for each level of the grouping
		variable.
	\item Homoscedasticity: variances among group variables are the same accross levels of 
		predictors.
	\item Non-colinearity
	\item Observation are independent 
\end{enumerate}
\subparagraph{Formulas}
Now we assume that $X=\prth{X}{i}{n}\hookrightarrow\mathcal{N}(\mu,\Sigma)$
Here $\E{X}=\mu = 
\begin{pmatrix}
	\mu_{1}\\
	.\\
	.\\
	.\\
	\mu_{p}
\end{pmatrix}
\text{ and } \Sigma = 
\begin{pmatrix}
	Cov\left( X_{1},X_{1} \right) & \cdots & Cov\left( X_{1},X_{j}\right) &\cdots & Cov\left( X_{1},X_{n} \right)\\
	&\cdots& & &\\
	&\cdots& & &\\
	&\cdots& & &\\
	Cov\left( X_{i},X_{1} \right) & \cdots & Cov\left( X_{i},X_{j}\right) &\cdots & Cov\left( X_{i},X_{n} \right)\\
	&\cdots& & &\\
	&\cdots& & &\\
	&\cdots& & &\\
	Cov\left( X_{n},X_{1} \right) & \cdots & Cov\left( X_{n},X_{j}\right) &\cdots & Cov\left( X_{n},X_{n} \right)\\
\end{pmatrix}\\
\text{ and }
\tB{f(x)=\dfrac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}exp\left( \dfrac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu) \right)}
	$\\
\text{ then }
$$
\tR{\delta_{k}(x) = x^{T}\Sigma^{-1}\mu_{k}-\dfrac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+ln(\pi_{k})}
$$
\emph{Python code}
\begin{python}
import sklearn
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

clf = LinearDiscriminantAnalysis()
clf.fit(X, y)
print(clf.score(X, y))
\end{python}

\emph{R code}
\begin{rcode}[deletekeywords={model, lda, data, df}]
library(MASS)

model.lda <- lda(Direction ~ ., data=df)
model.lda
\end{rcode}

\paragraph{Quadratic Discriminant Analysis}
unlike LDA, \tB{QDA assumes that each class has its own covariance matrix}.
That is for an observation from the $k^{th}$ class 
\tR{$X\hookrightarrow N(\mu_{k},\Sigma^{k})$}
\begin{align*}
	\delta_{k}(x) &= -\dfrac{1}{2}(x-\mu_{k})^{T}\Sigma_{k}^{-1}(x-\mu_{k})-\dfrac{1}{2}\ln|\Sigma_{k}|+\ln(\pi_{k})\\
	&= -\dfrac{1}{2}x^{T}\Sigma_{k}^{-1}x+x^{T}\Sigma_{k}^{-1}\mu_{k}-\dfrac{1}{2}\mu_{k}^{T}\Sigma_{k}^{-1}\mu_{k}-\dfrac{1}{2}\ln|\Sigma_{k}|+\ln(\pi_{k})
\end{align*}
\emph{Python code}
\begin{python}
import sklearn
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

clf = QuadrasticDiscriminantAnalysis()
clf.fit(X, y)
print(clf.score(X, y))
\end{python}


\emph{R code}
\begin{rcode}[deletekeywords={model, lda, data, df}]
library(MASS)

model.lda <- lda(y ~ ., data=df)
model.lda
\end{rcode}
\paragraph{Regularized Discriminant Analysis}
The regularize covariance matrices have the form:
\begin{center}
\enc{$ \hat{\bm{\Sigma}}_{k}(\lambda) = \lambda\hat{\bm{\Sigma}}_{k} + (1-\lambda)\hat{\bm{\Sigma}}$}
\end{center}
where $\hat{\bm{\Sigma}}$ is the pooled covariance matrix as used in LDA.
Here \sB{$\lambda\in[0, 1]$ allows a continuum of models between LDA and QDA, $\lambda$ can be 
choosed by cross-validation}.\\
Similar modifications allow $\hat{\bm{\Sigma}}$ itself to be shrunk toward the scalar covariance:
$$ \hat{\bm{\Sigma}}(\gamma) = \gamma\hat{\bm{\Sigma}} + (1-\gamma)\hat{\sigma}^{2}\bm{I}$$
for $\gamma\in [0, 1]$
\emph{R code}
\begin{rcode}
library(klaR)
model.rda <- rda(y ~ ., data=df, gamma=0.05, lambda=0.2)
\end{rcode}
With rda we have :
$\hat{\Sigma}_{k}(\lambda, \gamma)=(1-\gamma)\hat{\Sigma}_{k}(\lambda)+\gamma\dfrac{1}{p}
tr\left(\hat{\Sigma}_{k}(\lambda)\right)I$
\begin{itemize}
	\item[$(\gamma=0,\lambda=0)$]: QDA - individual covariance for each group
	\item[$(\gamma=0,\lambda=1)$]: LDA - a common covariance matrix
	\item[$(\gamma=1,\lambda=0)$]: Conditional independent variables - similar to Naive Bayes,
		but variable variance whithin group (main diagonal elements) are all equal.
	\item[$(\gamma=1,\lambda=1)$]: Classification using euclidian distance - as in previous case,
		but variances are the same for all groups. Objects are assigned to group with nearest
		mean.
\end{itemize}

\paragraph{Computations for LDA}
The \sB{computations of LDA and QDA are simplified by diagonalizing $\hat{\bm{\Sigma}}$ or 
$\hat{\bm{\Sigma}}_{k}$ for the latter}, suppose we compute the eigendecomposistion for each
$\hat{\bm{\Sigma}}_{k} = \bm{U}_{k}\bm{D}_{k}\bm{U}_{k}^{T}\text{ where } \bm{U}_{k}\text{ is }
p\times p$ orthonormal and $\bm{D}_{k}$ a diagonal matrix of positive eigenvalues $d_{kl}$.
\begin{itemize}
	\item $(x-\hat{\mu}_{k})^{T}\hat{\bm{\Sigma}}^{-1}_{k}(x-\hat{\mu}_{k}) =
		\left[ \bm{U}_{k}^{T}(x-\hat{\mu}_{k}) \right]^{T}\bm{D}_{k}^{-1}
		\left[ \bm{U}_{k}^{T}(x-\hat{\mu}_{k}) \right]$
	\item $\log\left( \hat{\bm{\Sigma}}_{k} \right) = \su{l}{}\log(d_{kl})$
\end{itemize}
LDA classifier can be implemented by the following pair of steps:
\begin{itemize}
	\item \sB{\textbf{Sphere} the data with respect to the common covariance estimate $\hat{\bm{\Sigma}}$}:\\ $X^{*} \leftarrow \bm{D}^{\frac{1}{2}}\bm{U}^{T}X$ where $\hat{\bm{\Sigma}} = \bm{U}\bm{D}\bm{U}^{T}$. \sV{The common covariance estimate of $X^{*}$ will now be the identity.}
	\item \tB{Classify to the closet class centroid in the transformed space}, modulo the effect
		of the class prior probabilities $\pi_{k}$.
\end{itemize}

\paragraph{Reduced-Rank Discriminant Analysis}
The $K$ centroids in $p-\text{dimensional}$ input space lie in an affine subspace of dimension
$\leq K-1$, and \sB{if $p$ is much larger than $K$ this will be a considerable drop in dimension}.\\
Moreover in locating the closet centroid \sB{we can ignore distances orthogonal to this subspace, 
since they will contribute equally to each class. Thus we might just as well project the $X^{*}$
onto centroid-spanning subspace $H_{K-1}$} and make distance comparisons there.

Finding the sequences of optimal subspaces for LDA involves the following steps:
\begin{itemize}
	\item compute the \sB{$K\times p$ matrix of class centroids $\bm{M}$} and the common
		\sB{covariance matrix $\bm{W}$}
	\item compute \sB{$\bm{M}^{*} = \bm{M}\bm{W}^{-\frac{1}{2}}$} using the eigen-decomposition 
		of $\bm{W}$
	\item compute \sB{$\bm{B}^{*}$, the covariance matrix of $\bm{M}^{*}$} and its 
		eigen-decomposition \tB{$\bm{B}^{*} = \bm{V}^{*}\bm{D}_{B}(\bm{V}^{*})^{T}$}.
		The columns $v_{l}^{*}$ of $\bm{V}^{*}$ in sequence from first to last \sB{define
		the coordinates of the optimal subspaces}.
\end{itemize}
Combining all these operations \sB{the $l^{th}$ discriminant variable} is given by \tB{$Z_{l}=v_{l}^{
T}X$} with $v_{l}=\bm{W}^{-\frac{1}{2}}v_{l}^{*}$

Fisher arrived at this decomposition via a different route, without referring to Gaussian 
distribution at all.
\begin{center}
	Find the linear combination $Z=a^{T}X$ such that the between class variance is maximized
	relative to the within-class variance.
\end{center}

The \sB{between-class variance of $Z$ is a $a^{T}\bm{B}a$} and the \sB{within-class variance $a^{T}
\bm{W}a$}, where $\bm{W}$ is defined earlier, and $\bm{B}$ is the covariance matrix of the class centroid matrix
$\bm{M}$.\\
Fisher's problem therefore amounts to maximizing the Rayleigh quotient:
\begin{center}
\enc{$ \max\limits_{a}\dfrac{a^{T}\bm{B}a}{a^{T}\bm{W}a}$} 
\end{center}
or equivalently:
$$ \max\limits_{a}a^{T}\bm{B}a \text{ subject to } a^{T}\bm{W}a=1$$
This is a generalized eigenvalue problem, with $a$ given by the largest eigenvalue of $\bm{W}^{-1}
\bm{B}$. 
\begin{itemize}
	\item Gaussian classification with common covariances leads to linear decision boundaries
	\item One can confine the data to the subspace spanned by the centroids in the sphered
		space.
	\item This subspace can be further decomposed into successively optimal subspaces in 
		term of centroid separation. This decomposition is identical to the decomposition
		due to Fisher.
\end{itemize}

