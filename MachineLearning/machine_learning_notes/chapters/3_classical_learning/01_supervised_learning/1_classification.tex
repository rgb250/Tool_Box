\subsection{Naive Bayes classifiers}
\paragraph{Purpose}
\tR{Classifying vectors of discrete-valuated features $x\in\left\{i
\right\}_{1\leq i \leq K}^{D}$}, where $K$ is the number of values for
each feature, and $D$ the number of features.

\paragraph{Assumptions}
\begin{itemize}
    \item \tB{Features are conditionally independent given the class 
        label}
\end{itemize}

\paragraph{Theory}
As a \emph{generative} model, meaning of the form:
$\prob{y=c|\bm{x}, \bm{\theta}} \propto \prob{\bm{x}|y=c,\bm{\theta}}
\prob{y=c|\bm{\theta}}$. The key of such models is the possibility
to \tR{specify a suitable form for the class-conditional density 
$\prob{\bm{x}|y=c, \bm{\theta}}$ which defines what kind of data we 
expect to see in each class}. And with the independence assumption we 
have:
\begin{center}
    \tB{$\prob{\bm{x}|y=c, \bm{\theta}} = \prd{j=1}{D}\prob{x_{j}|y=c,\bm{\theta}_{jc}}$}
\end{center}
with all \tB{$\prob{x_{j}|y=c,\bm{\theta}_{jc}}$ being able to 
follow a \textit{normal}, \textit{bernoulli} or \textit{multinoulli} 
distribution}.\\
\uB{Training a NBC consists in computing the MLE or the MAP estimate for 
the parameters.}\\
For a single observation
$\prob{x_{i}, y_{i}|\bm{\theta}} = \prob{y_{i}|\bm{\pi}}\prd{j}{}\prob{x_{ij}|\bm{\theta}_{j}} = 
\prd{c}{}\pi_{c}^{\mathbbm{1}(y_{i}=c)}\prd{j}{}\prd{c}{}\prob{x_{ij}|
\theta_{jc}}^{\mathbbm{1}(y_{i}=c)}
$\\ 
Hence the \tB{\emph{log-likelihooh}:
$\log\left(\mathcal{D}|\theta\right) = \su{c=1}{C}N_{c}\log(\pi_{c}) + 
\su{j=1}{D}\su{c=1}{C}\su{i:y_{i}=c}{}\log\left(\prob{x_{ij}|
\bm{\theta}_{jc}}\right)$}\\
By optimizing the above equation we are able to find the $\left(\theta_{jc}\right)_{1\leq j \leq D,~ 1\leq c\leq C}$ and we can then use them to
predict the output of an observation $\bm{x}$ as: $\prob{y=c|\bm{x},\mathcal{D}} \propto \prob{y=c|\mathcal{D}}\prd{j=1}{D}\prob{x_{j}|y=c,\mathcal{D}}$

\paragraph{Strengths}
\begin{itemize}
    \item Simple model, for $C$ classes and $D$ features, and hence \tB{relatively immune to 
        overfitting}
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item \uB{Unaccuracy} because of the strong independence assumption
\end{itemize}

\paragraph{Relationships with other methods}
\tB{Logistic Regression}: for discrete inputs 
\emph{Naive Bayesian Classifiers} form a 
generative-discriminant pair with \emph{Multinomial Logistic
Regression}: \uB{each NBC can be considered a way of fitting a
probability model that optimizes the joint likelihood 
$\prob{C, \bm{x}}$, while Multinomial Logistic Regression fits the same 
probability to optimize the conditional $\prob{C|\bm{x}}$}

\paragraph{Examples of application}
\begin{itemize}
    \item Classifying documents using bag of words
    \item Determining the gender of a person, based on measured features 
\end{itemize}


\subsection{Linear/Quadratic Discriminant Analysis}
\paragraph{Purpose}
\uB{It consists in defining the class conditional densities in a 
generative classifier}: $\prob{\bm{x}|y=c,\bm{\theta}} = \mathcal{N}
\left(\bm{x}, \bm{\mu}_{c},\bm{\Sigma}_{c}\right)$\\
As for a generative classifier we have the following equation: 
\begin{center}
    $\prob{y=c|\bm{x},\bm{\theta}} = \dfrac{\overbrace{\prob{\bm{x}|y=c,
        \bm{\theta}}}^{\text{\emph{class-conditional density}}}
        \overbrace{\prob{y=c|\bm{\theta}}}^{\text{\emph{class prior}}}}{
    \su{c'}{}\prob{y=c'|\bm{\theta}} \prob{\bm{x}|y=c',\bm{\theta}}}$
\end{center}

\paragraph{Assumptions}
\begin{itemize}
    \item Independent \tB{variables are normal for each level} of the
        grouping variable.
    \item \tB{Homoscedasticity} for LDA: variances \uB{among group} 
        variables are the same across levels of predictors.
    \item \uB{Independence of the observations}.
\end{itemize}


\paragraph{Theory of Quadratic Discriminant Analysis}
\begin{center}
    $\prob{y=c|\bm{x},\bm{\theta}} = \dfrac{|2\pi\Sigma_{c}|^{
    -\frac{1}{2}}\exp\left(-\frac{1}{2}[\bm{x}-\bm{\mu}_{c}]^{T}\Sigma_{
    c}^{-1}[\bm{x}-\bm{\mu}_{c}]\right)\pi_{c}}{\su{c'}{}|2\pi\Sigma_{
    c'}|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}[\bm{x}-\bm{\mu}_{c'}]^{T}
    \Sigma_{c'}^{-1}[\bm{x}-\bm{\mu}_{c'}]\right)\pi_{c'}}$
\end{center}
\tB{The threshold of this results will be a quadratic function of 
$\bm{x}$.}

\paragraph{Theory of Linear Discriminant Analysis}
\subparagraph{LDA}
Same equation than above but this time, \tB{$\forall c\in \inter{1}{C} 
\Sigma_{c} = \Sigma$}, \tB{then quadratic term $\bm{x}^{T}\Sigma^{-1}
\bm{x}$ will cancel out from numerator and denominator}.
Then by considering the above cancellation and the fact that 
evidence is considered as a constant, we have:
\tB{
\begin{align*}
    \prob{y=c|\bm{x},\bm{\theta}} & \propto \exp\left(\log(\pi_{c})+\bm{
        \mu}_{c}^{T}\Sigma^{-1}\bm{x}\bm{\mu}_{c}\right)\\
    &= \exp\left(\bm{\beta}_{c}^{T}\bm{x} + \gamma_{c}\right)
    \end{align*}
}
Note also that we have exactly: \tR{$\prob{y=c|\bm{x},\bm{\theta}} =
\dfrac{e^{\bm{\beta}_{c}^{T}\bm{x} + \gamma_{c}}}{\su{c'}{}e^{\bm{\beta
}_{c'}^{T}\bm{x} + \gamma_{c'}}} = S(\bm{\eta})_{c}$}. With $\eta=\left(
\bm{\beta}^{T}_{c}\bm{x} +\gamma_{c}\right)_{1\leq c\leq C}$
We recognize the \emph{softmax} function.
\subparagraph{RDA}
Regularized Discriminant Analysisa, assuming the covariance matrix is shared across the classes as in
LDA, we perform MAP estimation of $\bm{\Sigma}\hookrightarrow\text{ \emph{InverseWishart}}\left(
\text{\emph{diag}}\left(\hat{\bm{\Sigma}}_{mle}\right), \nu_{0}\right)$ then:
\begin{center}
    \fr{$\hat{\bm{\Sigma}} = \lambda\text{\emph{diag}}\left(\hat{\bm{\Sigma}}_{mle}
        \right) + (1-\lambda) \hat{\bm{\Sigma}}_{mle}$}
\end{center}
where $\lambda$ controls the amount of regularization which is related to the strength of the prior
$\nu_{0}$


\paragraph{Strengths}
\begin{itemize}
    \item more common MVN(Multivariate Normal) model which is the most widely used joint probability
        density function for continuous variables.
    \item good simplicity-efficiently trade-off 
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item Multicollinearity: predictive power can decrease with an 
        increased correlation between predictor variables.
\end{itemize}

\paragraph{Relationships with other methods}
\begin{itemize}
    \item ANOVA
    \item Logistic Regression
    \item PCA: look for linear combination of variables which best explain the model
    \item Nearest Shrunken Centroids
\end{itemize}

\paragraph{Examples of application}



\subsection{Nearest shrunken centroids classifier}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Logistic Regression}
\paragraph{Purpose}
With the generative approach we create a joint model of the form $\prob{y,\bm{x}}$, and
then to condition on $\bm{x}$, thereby deriving $\prob{y|bm{x}}$, it is the \emph{
generative} approach.\\
Alternatively, fitting directly a model of the form $\prob{y|\bm{x}}$ is a \emph{
discriminative} approach.
\paragraph{Assumptions}
\begin{itemize}
    \item Independence
\end{itemize}

\paragraph{Theory}
The data distribution is modelled by : 
\fr{$\prob{y|\bm{x}} = \text{\emph{Bernoulli}} \left(y|\sigma\left(\bm{w}^{T}\bm{x}
\right)\right)$}\\
With $\sigma$ being the \emph{sigmoid} function, such that 
$\sigma = \begin{cases}
    \mathbb{R} \longrightarrow [0, 1]\\ 
    x \mapsto \dfrac{e^{x}}{1 + e^{x}}
\end{cases}
$
\subparagraph{Maximum Likelihood Estimator}
\begin{align*}
    \text{\emph{NLL}}(\bm{w})
    &= -\su{i=1}{N}\log\left(\hat{y}_{i}^{\mathbbm{1}_{\{y_{i} = 1\}}}\left[1 -
    \hat{y}_{i}\right]^{\mathbbm{1}_{\{y_{i}=0\}}}\right)\\ 
    &= -\su{i=1}{N}y_{i}\log\left(\hat{y}_{i}\right) + \left[1 -y_{i}\right]\log
    \left(1 -\hat{y}_{i}\right)
\end{align*}
This called \textit{cross-entropy}


\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Fisher's Linear Discriminant Analysis (FLDA)}
\paragraph{Purpose}
An alternative way to the above discriminant analysis is to reduce the dimensionality
of the features $\bm{x}\in\mathbb{R}^{D}$ and then fit a Multivariate Normal to the 
resulting low-dimensional features $\bm{z}\in \mathbb{R}^{L}$.\\
\emph{PCA} would not be a good idea because it is unsupervised and the low-dimensional
resulting features would not be optimal for the classification problem.\\
A better option would be to \uB{find a matrix $\bm{W}$ such that the low-dimensional 
data can be classified as well as possible using a Gaussian class-conditional density 
model}, this is the \emph{FLDA}.
\paragraph{Assumptions}
\begin{itemize}
    \item Gaussian class-conditional: reasonable since we are computing linear 
        combination of potentially non-Gaussian features.
\end{itemize}

\paragraph{Theory}
For $2$ classes
Let us define 
$\begin{cases}
    \mu_{1} = \frac{1}{N_{1}}\su{i:y_{i}=1}{}\bm{x}_{i}\\
    \mu_{2} = \frac{1}{N_{2}}\su{i:y_{i}=2}{}\bm{x}_{i}
\end{cases}$,
and $m_{k} = \bm{w}^{T}\bm{\mu}_{k}$ being the projection of each mean onto the line
$\bm{w}$. Also let $z_{i}=\bm{w}^{T}\bm{x}_{i}$ be the projection of the data onto the 
line $\bm{w}$.\\
The goal is to find $\bm{w}$ such that we maximize the distance between the means,
$m_{1} - m_{2}$, while also ensuring the projected clusters are "tight":
\begin{center}
    $\bm{J}(\bm{w}) = \dfrac{(m_{f} - m_{1})^{2}}{s_{1}^{2} + s_{2}^{2}}
    = \dfrac{\bm{w}^{T}\bm{S}_{B}\bm{w}}{\bm{w}^{T}\bm{S}_{W}\bm{w}}$
\end{center}
with the \emph{between-class scatter matrix}: $\bm{S}_{B} = \left(\mu_{2} - \mu_{1}
\right)\left(\mu_{2} - \mu_{1}\right)^{T}$ and \emph{within-class scatter matrix}:
$
\su{i:y_{i}=1}{}\left(\bm{x}_{i} - \bm{\mu}_{1}\right) \left(\bm{x}_{i} - 
\bm{\mu}_{1}\right)^{T} + \su{i:y_{i}=2}{}\left(\bm{x}_{i} - \bm{\mu}_{2}\right)\left(
\bm{x}_{i} - \bm{\mu}_{2}\right)^{T}
$


\paragraph{Strengths}
\begin{itemize}
    \item Classification in taking in account the response label.
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item FLDA is restricted to using $L\leq C-1$ dimensions, regardless of $D$.
\end{itemize}

\paragraph{Relationships with other methods}
\begin{itemize}
    \item Linear Discriminant Analysis
    \item PCA
\end{itemize}

\paragraph{Examples of application}
\begin{itemize}
    \item Speech recognition
\end{itemize}


\subsection{Nearest Shrunken Centroids Classifier}
\paragraph{Purpose}
The basic idea is to perform \tB{MAP estimation for diagonal \emph{LDA} with a 
sparsity-promoting (Laplace) prior} 
\paragraph{Assumptions}
\paragraph{Theory}
Define the class-specific feature mean $\mu_{cj}$ in terms of the class-independent 
feature mean $m_{j}$ and a class-specific offset $\Delta_{cj}$ Thus:
\begin{center}
    $\mu_{jc} = m_{j} + \Delta_{cj}$
\end{center}
We will then put a prior on the $\Delta_{cj}$ terms to encourage them to be strictly
zero and compute MAP estimate.

\paragraph{Strengths}
\begin{itemize}
    \item Sparsity $\rightarrow$ more interpretable model
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
