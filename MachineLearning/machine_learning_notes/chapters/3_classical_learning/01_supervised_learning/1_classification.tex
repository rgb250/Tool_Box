% Naive Bayes Classifiers
\subsection{Naive Bayes classifiers}
\paragraph{Purpose}
\tR{Classifying vectors of discrete-valuated features $x\in\left\{i
\right\}_{1\leq i \leq K}^{D}$}, where $K$ is the number of values for
each feature, and $D$ the number of features.

\paragraph{Assumptions}
\begin{itemize}
    \item \tB{Features are conditionally independent given the class 
        label}
\end{itemize}

\paragraph{Theory}
As a \emph{generative} model, meaning of the form:
$\prob{y=c|\bm{x}, \bm{\theta}} \propto \prob{\bm{x}|y=c,\bm{\theta}}
\prob{y=c|\bm{\theta}}$. The key of such models is the possibility
to specify a suitable form for the class-conditional density 
$\prob{\bm{x}|y=c, \bm{\theta}}$ which definees what kind of data we 
expect to see in each class. And with the independence assumption we 
have:
\begin{center}
    \tB{$\prob{\bm{x}|y=c, \bm{\theta}} = \prd{j=1}{D}\prob{x_{j}|y=c,\bm{\theta}_{jc}}$}
\end{center}
with all $\prob{x_{j}|y=c,\bm{\theta}_{jc}}$ being able to 
follow a \textit{normal}, \textit{bernoulli} or \textit{multinoulli} 
distribution.\\
\uB{Training a NBC consists in computing the MLE or the MAP estimate for 
the parameters.}\\
For a single observation
$\prob{x_{i}, y_{i}|\bm{\theta}} = \prob{y_{i}|\bm{\pi}}\prd{j}{}\prob{x_{ij}|\bm{\theta}_{j}} = 
\prd{c}{}\pi_{c}^{\mathbbm{1}(y_{i}=c)}\prd{j}{}\prd{c}{}\prob{x_{ij}|
\theta_{jc}}^{\mathbbm{1}(y_{i}=c)}
$\\ 
Hence the \tB{\emph{log-likelihooh}:
$\log\left(\mathcal{D}|\theta\right) = \su{c=1}{C}N_{c}\log(\pi_{c}) + 
\su{j=1}{D}\su{c=1}{C}\su{i:y_{i}=c}{}\log\left(\prob{x_{ij}|
\bm{\theta}_{jc}}\right)$}\\
By optimizing the above equation we are able to find the $\left(\theta_{jc}\right)_{1\leq j \leq D,~ 1\leq c\leq C}$ and we can then use them to
predict the output of an observation $\bm{x}$ as: $\prob{y=c|\bm{x},\mathcal{D}} \propto \prob{y=c|\mathcal{D}}\prd{j=1}{D}\prob{x_{j}|y=c,\mathcal{D}}$

\paragraph{Strengths}
\begin{itemize}
    \item Simple model, for $C$ classes and $D$ features, and hence \tB{relatively immune to 
        overfitting}
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item \uB{Unaccuracy} because of the strong independence assumption
\end{itemize}

\paragraph{Relationships with other methods}
\tB{Logistic Regression}: for discrete inputs 
\emph{Naive Bayesian Classifiers} form a 
generative-discriminant pair with \emph{Multinomial Logistic
Regression}: \uB{each NBC can be considered a way of fitting a
probability model that optimizes the joint likelihood 
$\prob{C, \bm{x}}$, while Multinomial Logistic Regression fits the same 
probability to optimize the conditional $\prob{C|\bm{x}}$}

\paragraph{Examples of application}
\begin{itemize}
    \item Classifying documents using bag of words
    \item Determining the gender of a person, based on measured features 
\end{itemize}


% Linear/Quadratic Discriminant Analysis
\subsection{Linear/Quadratic Discriminant Analysis}
\uB{It consists in defining the class conditional densities in a 
generative classifier}: $\prob{\bm{x}|y=c,\bm{\theta}} = \mathcal{N}
\left(\bm{x}, \bm{\mu}_{c},\bm{\Sigma}_{c}\right)$\\
As for a generative classifier we have the following equation: 
\begin{center}
    $\prob{y=c|\bm{x},\bm{\theta}} = \dfrac{\overbrace{\prob{\bm{x}|y=c,
        \bm{\theta}}}^{\text{\emph{class-conditional density}}}
        \overbrace{\prob{y=c|\bm{\theta}}}^{\text{\emph{class prior}}}}{
    \su{c'}{}\prob{y=c'|\bm{\theta}} \prob{\bm{x}|y=c',\bm{\theta}}}$
\end{center}
\paragraph{Purpose of Quadratic Discriminant Analysis}
\begin{center}
    $\prob{y=c|\bm{x},\bm{\theta}} = \dfrac{|2\pi\Sigma_{c}|^{
    -\frac{1}{2}}\exp\left(-\frac{1}{2}[\bm{x}-\bm{\mu}_{c}]^{T}\Sigma_{
    c}^{-1}[\bm{x}-\bm{\mu}_{c}]\right)\pi_{c}}{\su{c'}{}|2\pi\Sigma_{
    c'}|^{-\frac{1}{2}}\exp\left(-\frac{1}{2}[\bm{x}-\bm{\mu}_{c'}]^{T}
    \Sigma_{c'}^{-1}[\bm{x}-\bm{\mu}_{c'}]\right)\pi_{c'}}$
\end{center}
\tB{The threshold of this results will be a quadratic function of 
$\bm{x}$.}

\paragraph{Purpose of Linear Discriminant Analysis}
Same equation than above but this time, \tB{$\forall c\in \inter{1}{C} 
\Sigma_{c} = \Sigma$}, \tB{then quadratic term $\bm{x}^{T}\Sigma^{-1}
\bm{x}$ will cancel out from numerator and denominator}.
Then by considering the above cancellation and the fact that 
evidence is considered as a constant, we have:
\tB{
\begin{align*}
    \prob{y=c|\bm{x},\bm{\theta}} & \propto \exp\left(\log(\pi_{c})+\bm{
        \mu}_{c}^{T}\Sigma^{-1}\bm{x}\bm{\mu}_{c}\right)\\
    &= \exp\left(\bm{\beta}_{c}^{T}\bm{x} + \gamma_{c}\right)
    \end{align*}
}
Note also that we have exactly: \tR{$\prob{y=c|\bm{x},\bm{\theta}} =
\dfrac{e^{\bm{\beta}_{c}^{T}\bm{x} + \gamma_{c}}}{\su{c'}{}e^{\bm{\beta
}_{c'}^{T}\bm{x} + \gamma_{c'}}} = S(\bm{\eta})_{c}$}. With $\eta=\left(
\bm{\beta}_{c}\bm{x} +\gamma_{c}\right)_{1\leq c\leq C}$
We recognize the \emph{softmax} function.

\paragraph{Assumptions}
\begin{itemize}
    \item Independent \tB{variables are normal for each level} of the
        grouping variable.
    \item \tB{Homoscedasticity} for LDA: variances \uB{among group} 
        variables are the same across levels of predictors.
    \item \uB{Independence of the observations}.
\end{itemize}


\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\begin{itemize}
    \item Multicollinearity: predictive power can decrease with an 
        increased correlation between predictor variables.
\end{itemize}

\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Logistic Regression}
\paragraph{Purpose}
With the generative approach we create a joint model of the form $\prob{y,\bm{x}}$, and
then to condition on $\bm{x}$, thereby deriving $\prob{y|bm{x}}$, it is the \emph{
generative} approach.\\
Alternatively, fitting directly a model of the form $\prob{y|\bm{x}}$ is a \emph{
discriminative} approach.
\paragraph{Assumptions}
\begin{itemize}
    \item Independence
\end{itemize}

\paragraph{Theory}
The data distribution is modelled by : 
\fr{$\prob{y|\bm{x}} = \text{\emph{Bernoulli}} \left(y|\sigma\left(\bm{w}^{T}\bm{x}
\right)\right)$}\\
With $\sigma$ being the \emph{sigmoid} function, such that 
$\sigma = \begin{cases}
    \mathbb{R} \longrightarrow [0, 1]\\ 
    x \mapsto \dfrac{e^{x}}{1 + e^{x}}
\end{cases}
$
\subparagraph{Maximum Likelihood Estimator}
\begin{align*}
    \text{\emph{NLL}}(\bm{w})
    &= -\su{i=1}{N}\log\left(\hat{y}_{i}^{\mathbbm{1}_{\{y_{i} = 1\}}}\left[1 -
    \hat{y}_{i}^{\mathbbm{1}_{\{y_{i}=0\}}}\right]\right)\\ 
    &= -\su{i=1}{N}y_{i}\log\left(\hat{y}_{i}\right) + \left[1 -y_{i}\right]\log
    \left(1 -\hat{y}_{i}\right)
\end{align*}
This called \textit{cross-entropy}


\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Logistic Regression}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Logistic Regression}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Logistic Regression}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

