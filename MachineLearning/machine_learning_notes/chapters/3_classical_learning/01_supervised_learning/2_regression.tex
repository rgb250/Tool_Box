\subsection{Linear Regression}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{General}
It is a model for which the data distribution (likelihood) is described 
by:
\tR{\begin{center}
    $\prob{y|\bm{x},\bm{\theta}} = \mathcal{N}\left(y|\bm{w}^{T}\phi(
    \bm{x}), \sigma^{2}\right)$
\end{center}}
with $\phi$ that can be a non-linear function, in this case we talk about
\emph{basis function expansion}.\\
To estimate the parameters we can use the \emph{maximum likelihood 
estimation}: \tB{$\hat{\theta} \triangleq \argmax_{\theta} \log\left(
\prob{\mathcal{D}|\bm{\theta}}\right)$}.\\
For computational purpose it is better to consider the minimization of 
the \textit{Negative Log Likelihood} (NLL):
\begin{align*}
    \text{\emph{NLL}}(\theta) &\triangleq -\log\left(p\left(\mathcal{D}|
    \theta\right)\right)\\
                              &=-\su{i=1}{n}\log\left(\prob{y_{i}|
                                      \bm{x}_{i},\bm{\theta}}\right)\\
                              &=-\su{i=1}{n}\log\left(\left[\dfrac{1}{
                              2\pi\sigma^{2}}\right]^{\frac{1}{2}}\exp
                              \left(-\dfrac{1}{2\sigma^{2}}\left[y_{i}
                                      - \bm{w}^{T} \bm{x}_{i}\right]^{2}
                              \right)\right)\\
                              &= \dfrac{n}{2}\log\left(2\pi\sigma^{2}
                              \right) + \dfrac{1}{2\sigma^{2}}\su{i=1}{
                              n}\left(y_{i} - \bm{w}^{T}\bm{x}_{i}
                              \right)^{2}\\
                              &= \dfrac{n}{2}\log\left(2\pi\sigma^{2}
                              \right) + \dfrac{1}{2\sigma^{2}}\text{
                              \emph{RSS}}(\bm{w})\\
                              &= \dfrac{n}{2}\log\left(2\pi\sigma^{2}
                          \right) + \dfrac{1}{2\sigma^{2}}\norm{\epsilon
                          }_{2}^{2}
\end{align*}
As the \emph{MLE} for $\bm{w}$ is the one minimizing the \emph{RSS} then
this method is known as \emph{least square}.

\subparagraph{Derivation of the MLE}
it is better to use a matrix-vector representation.\\
$\text{\emph{NLL}}(\bm{w}) = \dfrac{1}{2}\left(y-\bm{X}\bm{w}\right)^{T}\left(y-\bm{X}
\bm{w}\right) = \dfrac{1}{2}\bm{w}^{T}\left(\bm{X}^{T}\bm{X}\right)\bm{w} - \bm{w}^{T}
\left(\bm{X}^{T}y\right)$ Note that $\bm{X}^{T}\bm{X}$ is the \emph{sum of squares 
matrix}. Then \textbf{gradient}, $g(\bm{w}) = \bm{X}^{T}\bm{X}\bm{w} - \bm{X}^{T}\bm{y}$
that we have to equate to zero to get $\bm{X}^{T}\bm{X}\bm{w} = \bm{X}^{T}\bm{y}$ to 
conclude that:
\begin{center}
    \fr{$\hat{\bm{w}}_{OLS} = \left(\bm{X}^{T}\bm{X}\right)^{-1}\bm{X}^{T}\bm{y}$}
\end{center}

\subparagraph{Robust Linear Regression}
It is very common to model the noise in regression models using a \emph{Gaussian 
distribution}, meaning \tR{$\epsilon_{i} = y_{i} - \bm{w}^{T}\bm{x}_{i} \hookrightarrow 
\mathcal{0, \sigma^{2}}$}. One way to achieve \emph{robustness} against \emph{outliers}
is to replace the Gaussian distribution for the response variable with a distribution 
having \textbf{heavy tails}.

\begin{center}
    \begin{tabular}{|*{3}{c|}}
    \hline
    \textbf{Likelihood} & \textbf{Prior} & \textbf{Name}\\
    \hline
    Gaussian & Uniform & \emph{Least Squares}\\
    \hline
    Gaussian & Gaussian & \emph{Ridge}\\
    \hline
    Gaussian & Laplace & \emph{Lasso}\\
    \hline
    \end{tabular}
\end{center}

\subparagraph{Ridge}
encourages parameters to be small by using a zero-mean Gaussian prior: $\prob{\bm{w}} = 
\prd{j=1}{D} \mathcal{N}\left(\omega_{j}|0, \tau^{2}\right)$, where $\frac{1}{\tau^{2}}$
controls the strength of the prior.\\
The corresponding \tB{\emph{MAP}} estimation problem becomes:
\tB{$\argmax_{\bm{w}}\su{i=1}{n}\log\left(\mathcal{N}\left(y_{i}|\omega_{0} + \bm{w}^{T}
\bm{x}_{i}, \sigma^{2}\right)\right) + \su{j=1}{D}\log\left(\mathcal{N}\left(\omega_{j}
|0,\tau^{2} \right)\right)$}. After some calculus and with where $\lambda \triangleq 
\dfrac{\sigma^{2}}{\tau^{2}}$ we deduce that:
\begin{center}
    \fr{$
        \hat{\bm{w}}_{\text{\emph{Ridge}}} = \left(\lambda\bm{I}_{D} + \bm{X}^{T}\bm{X}
        \right)^{-1}\bm{X}\bm{y}
    $}
\end{center}
Advantages of Ridge regression on OLS regression:
\begin{itemize}
    \item $\left(\lambda\bm{I}_{D}+\bm{X}^{T}\bm{X}\right)$ is much better conditioned, 
        and hence more likely to be invertible, than $\bm{X}^{T}\bm{X}$ at least for 
        suitable large $\lambda$
    \item if we follow a \emph{Singular Value Decomposition} $\bm{X} = \bm{U}\bm{S}
        \bm{V}^{T}$ we find 
        that \tB{$\hat{\bm{y}} = \bm{X}\hat{\bm{w}}_{\text{\emph{Ridge}}} = \su{j=1}{D}
        \bm{u}_{j}\dfrac{\sigma_{j}^{2}}{\sigma_{j}^{2} + \lambda}\bm{u}_{j}^{T}\bm{y}$}
        with \tB{$\left(\sigma_{j}\right)_{1\leq j \leq D}$ the singular value of $\bm{X}$}
        whereas for OLS we have \tB{$\hat{\bm{y}} = \bm{X}\hat{\bm{w}}_{\text{\emph{OLS}}} 
        = \su{j=1}{D} \bm{u}_{j}\bm{u}_{j}^{T}\bm{y}$}. Meaning that with \uB{Ridge if 
        $\sigma_{j}^{2}$ is small compared to $\lambda$ then direction $\bm{u}_{j}$
        will not have much effect on the prediction}. In term of predictive accuracy
        \emph{Ridge} regression is more interesting than \emph{PCA} regression.
\end{itemize}
\subparagraph{Lasso (Least Absolute Shrinkage and Abosolute Selection Operator)}
\tB{$\hat{\bm{w}}_{Lasso} = sign(\hat{\bm{w}}_{OLS})\left[|\hat{\bm{w}}_{OLS}| -\dfrac{
\lambda}{2}\right]$}

\subparagraph{Elastic-Net}
In practice Elastic-Net often performs best, since it provides a good combination of
sparsity and regularization.
\paragraph{Strengths}
\begin{itemize}
    \item Simple
    \item Customizable to achieve robustness
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item Not very powerful for non-linear data

\end{itemize}

\paragraph{Relationships with other methods}
\begin{itemize}
    \item Ridge Regression has similitude with PCA
\end{itemize}

\paragraph{Examples of application}


\subsection{Least Angle Regression Shrinkage}
\paragraph{Purpose}
Instead of updating only one variable at a time, inducing slow converging, we can use \uB{\emph{
Active set} methods that update many variables at a time}. They add or remove a few variables at
a time so they can take a long time if they started far from the solution.\\
Suppose $\mathcal{A}_{k}$ is the active set of variables at the 
beginning of the $k^{th}$ step and let $\beta_{\mathcal{A}_{k}}$ be the
coefficient vector for these variables at this step, there will be 
$k-1$ nonzero values and the one just entered will be zero.\\
If \uB{$\bm{r}_{k}=\bm{y}-\bm{X}_{\mathcal{A}_{k}}\beta_{\mathcal{A}_{k}}$}
is the current residual, then the direction for this step is:
\begin{center}
	$\delta_{k}=(\bm{X}_{\mathcal{A}_{k}^{T}}\bm{X}_{\mathcal{A}_{k}^{T}})^{-1}\bm{X}_{\mathcal{A}_{k}^{T}}^{T}\bm{r}_{k}$
\end{center}
The name ``least angle'' arises from a geometrical interpretation of 
this process; $\bm{u}_{k}$ makes the smallest (and equal) angle with
each of the predictors in $\mathcal{A}_{k}$
\paragraph{Assumptions}
\paragraph{Theory}
Active methods exploit the fact that one can quickly compute $\hat{\bm{w}}(\lambda_{k})$ from 
$\hat{\lambda}(\lambda_{k-1})$ if $\lambda_{k}\approx\lambda_{k-1}$ this is known as \textbf{warm
starting}.

\subparagraph{Least Angle Regression}
\begin{enumerate}
	\item \uB{Standardize the predictors} to have mean zero an unit 
		norm.\\ \uB{Start with the residual $\bm{r}=\bm{y}-
		\overline{\bm{y}}$}, and for all $j\in\inter{1}{p},~\beta_{j}=0$
    \item Find the predictor $x_{j}$ most correlated with $\bm{r}$
    \item For $j\in\inter{1}{p}$
    \begin{itemize}
        \item Move $\beta_{j}$ from 0 to its least-squares coefficient
            $\dfrac{\sP{\bm{x}_{j}}{\bm{r}}}{\sP{\bm{x}_{j}}{\bm{x}_{j}}}$, then recompute 
            $\bm{r}$. Find the predictor $x_{k}$ most correlated with the new $\bm{r}$
        \item Move $\beta_{k}$ from 0 to its least-squares coefficient
            $\dfrac{\sP{\bm{x}_{k}}{\bm{r}}}{\sP{\bm{x}_{k}}{\bm{x}_{k}}}$, then recompute 
            $\bm{r}$. Find the predictor $x_{l}$ most correlated with the new $\bm{r}$\dots
        \item Move $\beta_{j}$ and $\beta_{k}$ in the direction defined
            by their joint least squares coefficient of the current
            residual on $(x_{j},x_{k})$, until some other 
            competitor $x_{l}$ has as much correlation with the
            current residual.
    \end{itemize}
	\item After $\min(N-1, p)$ steps, we arrive at the
		full least-squares solution.
\end{enumerate}

\subparagraph{Least Angle Regression: Lasso Modification}
\begin{itemize}
	\item[4a] If a non-zero coefficient hits zero, drop its variable from the
		active set of varibalbes and recompute the current joint least squares
		direction
\end{itemize}


\paragraph{Strengths}
\begin{itemize}
	\item Numerically efficient when $p\gg n$
	\item As fast as forward selection and has the same order of complexity as OLS
	\item Produces a full piecewise linear solution path
	\item Stable
	\item Esealy modified to produce for other estimators like the Lasso
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item Because LARS is based upon iterative refitting of the residuals, it would
		appears to be especially sensitive to the effect noise.
\end{itemize}

\paragraph{Relationships with other methods}
\begin{itemize}
    \item Least Squares
\end{itemize}

\paragraph{Examples of application}


\subsection{Generalized Linear Models (GLMs)}
Models in which the \tR{output density is in the exponential family} and in which \tR{the mean
parameters are a linear combination of the inputs}, passed through a possibly nonlinear
function such as the logistic function.\\
Exponential family:
\tB{$\prob{\bm{x}|\bm{\theta}} = \dfrac{1}{Z(\bm{\theta})}h(x)e^{\bm{\theta}^{T}\phi(
\bm{x})} = h(\bm{x})e^{\bm{\theta}^{T}\phi(\bm{x}) - A(\bm{\theta})}$}
where
$\begin{cases}
    Z(\bm{\theta}) = \su{\mathcal{X}^{m}}{}h(\bm{x})e^{\bm{\theta}^{T}\phi(\bm{x})}
    d\bm{x} \\
    A(\bm{\theta}) = \log\left(Z(\bm{\theta})\right)
\end{cases}$
with the \emph{natural parameters} $\bm{\theta}$, the vector of  \emph{sufficient 
statistics} $\phi(\bm{x})$, the \emph{partition function} $Z(\bm{\theta})$, the 
\emph{log partition function} or \emph{cumulant function} $A(\bm{\theta})$and the 
\emph{scaling constant} $h(\bm{x})$.\\
The name \emph{cumulant function} comes from the property of the exponential family 
being that derivatives of the log partition function can be used to generate cumulant
of the sufficient statistics (the first and second cumulants of a distribution being
its mean and variance).
\paragraph{Purpose}
We have the following data distribution:
\tR{$\prob{y_{i}|\bm{\theta},\sigma^{2}} = \exp\left(\dfrac{y_{i}\bm{\theta} - A\left(
\bm{\theta}\right)}{\sigma^{2}} + c(y_{i}, \sigma^{2})
\right)$}
with $c$ a normalization constant.\\
Let's consider an invertible mapping \tB{$\Phi$ such that $\bm{\theta} = \Phi(\mu)$} with
\uB{$\mu$ being the mean parameter and $\bm{\theta}$ the natural parameter}.\\
We have as well \tB{$\mu = \Phi^{-1}(\bm{\theta}) = A'(\bm{\theta})$}.\\
We are free to chose any link function as long as the inverse have an appropriate 
range. 
% A simple form of link function is to use $g=\Phi$, $g$ is then called \emph{canonical function}.

\begin{center}
    \begin{tabular}{|*{4}{l|}}
    \hline
    \textbf{Distribution} & \textbf{Link function}  & 
    \textbf{Natural parameter} $\theta = \Phi(\mu)$ & \textbf{Mean parameter} $\mu = 
    \Phi^{-1}(\theta) = \mathbb{E}(y)$ \\
    \hline
    $\mathcal{N}(\mu, \sigma^{2})$ & \emph{identity} & $\bm{\theta} = \bm{\mu}$ & 
    $\bm{\mu} = \bm{\theta}$\\
    \hline
    $\mathcal{B}(n,\mu)$ & \emph{logit} & $\bm{\theta} = \log\left(\frac{\mu}{1-\mu}
    \right)$ & $\mu = \text{\emph{sigm}}(\bm{\theta})$\\
    \hline
    \emph{Poisson}$(\mu)$ & \emph{log} & $\bm{\theta} = \log\left(\mu\right)$ & 
    $\mu=e^{\theta}$\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\begin{itemize}
    \item GLMs can be fit using methods like gradient descent.
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Learning to rank}
\paragraph{Purpose}
Modeling a function being able to rank a set of items.
Suppose we have a query $q$ and a set of documents $(d_{i})_{1\leq i\leq n}$ and we would like to 
sort these documents in decreasing order of relevance. 

\paragraph{Assumptions}
\paragraph{Theory}
Let us consider a \tB{document $d$} and a \tB{query $q$}, a standard way to measure the relevance
between the both is to use \tB{$\text{\emph{similitude}}(q,d) \triangleq \prob{q|d} = \prd{i=1}{n}
\prob{q_{i}|d}$} with $q_{i}$ being the $i^{\text{\emph{th}}}$ word or term of $q$.
\subparagraph{Pointwise approach}
for binary relevance labels, we can follow a standard binary classification scheme to
estimate $\prob{y=1|\bm{x}(q,d)}$, in the case of ordered relevancy labels we can use
an \tB{\emph{ordinal regression} to predict the rating $\prob{y=r|\bm{x}(q,d)}$}.\\
However this method does not take into account the location of each document in the 
list.
\subparagraph{Pairwise approach}
to \tB{check the relative relevance of two items rather than absolute relevance}. We 
can model this kind of data using a binary classifier of the form \tB{$\prob{y_{jk}|
\bm{x}_{j},\bm{x}_{k}} = \sigma\left(f(\bm{x}_{j}) - f(\bm{x}_{k})\right)$} 
where \uB{$f$ is a scoring function}, often taken to be linear: $f(\bm{x}) = \bm{w}^{T}
\bm{x}$.

\subparagraph{Listwise approach}
we now consider methods looking at the entire list of items at the same time. We can 
define a total order on a list by specifying a \tB{permutation of its indices: $\bm{\pi}$}.
To model the uncertainty about $\bm{\pi}$ we can use the \emph{Plackett-Luce} 
distribution. \tB{$\prob{\bm{\pi}|\bm{s}} = \prd{j=1}{m}\dfrac{s_{j}}{\su{u=j}{m}s_{u}}$} where
$s_{j}=s\left(\pi^{-1}(j)\right)$, \uB{the score of the document ranked at the $j^{th}$ position}.
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}
\begin{itemize}
    \item \emph{information retrieval} return a list of the top $k$ more relevant 
        documents, depending on a given query 
\end{itemize}

\subsection{Supervised PCA}
\paragraph{Purpose}
Also called \emph{Bayesian factor regression} this model take in account $y_{i}$ when
learning the low dimension embedding.
\paragraph{Assumptions}
\paragraph{Theory}
$\begin{cases}
    \prob{\bm{z}_{i}} = \mathcal{N}\left(\bm{0}, \bm{I}_{L}\right) \\
    \prob{y_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{w}_{y}^{T}\bm{z}_{i}
    +\mu_{y},\sigma_{y}^{2}\right) \\
    \prob{\bm{x}_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{W}_{x}^{T}\bm{z}_{i}
    +\bm{\mu}_{x},\sigma_{x}^{2}\bm{I}_{D}\right)
\end{cases}$\\
The \uB{basic idea compressing $\bm{x}_{i}$ to predict $y_{i}$ can be formulated using
information theory}, in particular we might want to find an encoding distribution
\tB{$\prob{\bm{z}|\bm{x}}$ such that we minimize $\mathbb{I}(X;Z) - \beta\mathbb{I}(X;Y)$}.
Where $\beta\geq 0$, the \emph{information bottleneck} is some parameter controlling 
the trade-off between compression and predictive accuracy.
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\begin{itemize}
    \item PCA
\end{itemize}

\paragraph{Examples of application}
\begin{itemize}
    \item predict the movies that you would like knowing who your friends are as well 
        as the rating from other users
\end{itemize}


\subsection{Partial Least Squares}
\paragraph{Purpose}
The key idea is to \tB{allow some of the (co)variance in the input features to be explained
by its own subspace $\bm{z}_{i}^{x}$} and to let the remaining of subspace $\bm{z}_{i}^{
s}$ be shared between input and output.
\paragraph{Assumptions}
\paragraph{Theory}
$\begin{cases}
    \prob{\bm{z}_{i}} = \mathcal{N}\left(\bm{z}^{s}_{i}|\bm{0},\bm{I}_{L_{s}}\right)
    \mathcal{N}\left(\bm{z}^{x}_{i}|\bm{0},\bm{I}_{L_{x}}\right)\\
    \prob{\bm{y}_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{W}_{y}\bm{z}^{s}_{i} + 
    \bm{\mu}_{y},\sigma^{2}\bm{I}_{D_{y}}
    \right)\\
        \prob{\bm{x}_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{W}_{x}\bm{z}^{s}_{i} + \bm{B}_{x}
        \bm{z}^{x}_{i} + \bm{\mu}_{x},\sigma^{2}\bm{I}_{D_{x}}\right)
\end{cases}$\\
\uB{We should choose $L$ large enough so that the shared subspace does not capture covariate specific
variations}.
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
