\subsection{Linear Regression}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{General}
It is a model for which the data distribution (likelihood) is described 
by:
\tR{\begin{center}
    $\prob{y|\bm{x},\bm{\theta}} = \mathcal{N}\left(y|\bm{w}^{T}\phi(
    \bm{x}), \sigma^{2}\right)$
\end{center}}
with $\phi$ that can be a non-linear function, in this case we talk about
\emph{basis function expansion}.\\
To estimate the parameters we can use the \emph{maximum likelihood 
estimation}: \tB{$\hat{\theta} \triangleq \argmax_{\theta} \log\left(
\prob{\mathcal{D}|\bm{\theta}}\right)$}.\\
For computational purpose it is better to consider the minimization of 
the \textit{Negative Log Likelihood} (NLL):
\begin{align*}
    \text{\emph{NLL}}(\theta) &\triangleq -\log\left(p\left(\mathcal{D}|
    \theta\right)\right)\\
                              &=-\su{i=1}{n}\log\left(\prob{y_{i}|
                                      \bm{x}_{i},\bm{\theta}}\right)\\
                              &=-\su{i=1}{n}\log\left(\left[\dfrac{1}{
                              2\pi\sigma^{2}}\right]^{\frac{1}{2}}\exp
                              \left(-\dfrac{1}{2\sigma^{2}}\left[y_{i}
                                      - \bm{w}^{T} \bm{x}_{i}\right]^{2}
                              \right)\right)\\
                              &= \dfrac{n}{2}\log\left(2\pi\sigma^{2}
                              \right) + \dfrac{1}{2\sigma^{2}}\su{i=1}{
                              n}\left(y_{i} - \bm{w}^{T}\bm{x}_{i}
                              \right)^{2}\\
                              &= \dfrac{n}{2}\log\left(2\pi\sigma^{2}
                              \right) + \dfrac{1}{2\sigma^{2}}\text{
                              \emph{RSS}}(\bm{w})\\
                              &= \dfrac{n}{2}\log\left(2\pi\sigma^{2}
                          \right) + \dfrac{1}{2\sigma^{2}}\norm{\epsilon
                          }_{2}^{2}
\end{align*}
As the \emph{MLE} for $\bm{w}$ is the one minimizing the \emph{RSS} then
this method is known as \emph{least square}.

\subparagraph{Derivation of the MLE}
it is better to use a matrix-vector representation.\\
$\text{\emph{NLL}}(\bm{w}) = \dfrac{1}{2}\left(y-\bm{X}\bm{w}\right)^{T}\left(y-\bm{X}
\bm{w}\right) = \dfrac{1}{2}\bm{w}^{T}\left(\bm{X}^{T}\bm{X}\right)\bm{w} - \bm{w}^{T}
\left(\bm{X}^{T}y\right)$ Note that $\bm{X}^{T}\bm{X}$ is the \emph{sum of squares 
matrix}. Then \textbf{gradient}, $g(\bm{w}) = \bm{X}^{T}\bm{X}\bm{w} - \bm{X}^{T}\bm{y}$
that we have to equate to zero to get $\bm{X}^{T}\bm{X}\bm{w} = \bm{X}^{T}\bm{y}$ to 
conclude that:
\begin{center}
    \fr{$\hat{\bm{w}}_{OLS} = \left(\bm{X}^{T}\bm{X}\right)^{-1}\bm{X}^{T}\bm{y}$}
\end{center}

\subparagraph{Robust Linear Regression}
It is very common to model the noise in regression models using a \emph{Gaussian 
distribution}, meaning \tR{$\epsilon_{i} = y_{i} - \bm{w}^{T}\bm{x}_{i} \hookrightarrow 
\mathcal{0, \sigma^{2}}$}. One way to achieve \emph{robustness} against \emph{outliers}
is to replace the Gaussian distribution for the response variable with a distribution 
having \textbf{heavy tails}.

\begin{center}
    \begin{tabular}{|*{3}{c|}}
    \hline
    \textbf{Likelihood} & \textbf{Prior} & \textbf{Name}\\
    \hline
    Gaussian & Uniform & \emph{Least Squares}\\
    \hline
    Gaussian & Gaussian & \emph{Ridge}\\
    \hline
    Gaussian & Laplace & \emph{Lasso}\\
    \hline
    \end{tabular}
\end{center}

\subparagraph{Ridge}
encourages parameters to be small by using a zero-mean Gaussian prior: $\prob{\bm{w}} = 
\prd{j=1}{D} \mathcal{N}\left(\omega_{j}|0, \tau^{2}\right)$, where $\frac{1}{\tau^{2}}$
controls the strength of the prior.\\
The corresponding \tB{\emph{MAP}} estimation problem becomes:
\tB{$\argmax_{\bm{w}}\su{i=1}{n}\log\left(\mathcal{N}\left(y_{i}|\omega_{0} + \bm{w}^{T}
\bm{x}_{i}, \sigma^{2}\right)\right) + \su{j=1}{D}\log\left(\mathcal{N}\left(\omega_{j}
|0,\tau^{2} \right)\right)$}. After some calculus and with where $\lambda \triangleq 
\dfrac{\sigma^{2}}{\tau^{2}}$ we deduce that:
\begin{center}
    \fr{$
        \hat{\bm{w}}_{\text{\emph{Ridge}}} = \left(\lambda\bm{I}_{D} + \bm{X}^{T}\bm{X}
        \right)^{-1}\bm{X}\bm{y}
    $}
\end{center}
Advantages of Ridge regression on OLS regression:
\begin{itemize}
    \item $\left(\lambda\bm{I}_{D}+\bm{X}^{T}\bm{X}\right)$ is much better conditioned, 
        and hence more likely to be invertible, than $\bm{X}^{T}\bm{X}$ at least for 
        suitable large $\lambda$
    \item if we follow a \emph{Singular Value Decomposition} $\bm{X} = \bm{U}\bm{S}
        \bm{V}^{T}$ we find 
        that $\hat{\bm{y}} = \bm{X}\hat{\bm{w}}_{\text{\emph{Ridge}}} = \su{j=1}{D}
        \bm{u}_{j}\dfrac{\sigma_{j}^{2}}{\sigma_{j}^{2} + \lambda}\bm{u}_{j}^{T}\bm{y}$
        with $\left(\sigma_{j}\right)_{1\leq j \leq D}$ the singular value of $\bm{X}$
        whereas for OLS we have $\hat{\bm{y}} = \bm{X}\hat{\bm{w}}_{\text{\emph{OLS}}} 
        = \su{j=1}{D} \bm{u}_{j}\bm{u}_{j}^{T}\bm{y}$. Meaning that with Ridge if 
        $\sigma_{j}^{2}$ is small compared to $\lambda$ then direction $\bm{u}_{j}$
        will not have much effect on the prediction. In term of predictive accuracy
        \emph{Ridge} regression is more interesting than \emph{PCA} regression.
\end{itemize}
\subparagraph{Lasso (Least Absolute Shrinkage and Abosolute Selection Operator)}
$\hat{\bm{w}}_{Lasso} = sign(\hat{\bm{w}}_{OLS})\left[|\hat{\bm{w}}_{OLS}| -\dfrac{
\lambda}{2}\right] $

\subparagraph{Elastic-Net}
In practice Elastic-Net often performs best, since it provides a good combination of
sparsity and regularization.
\paragraph{Strengths}
\begin{itemize}
    \item Simple
    \item Customizable to achieve robustness
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item Not very powerful for non-linear data

\end{itemize}

\paragraph{Relationships with other methods}
\begin{itemize}
    \item Ridge Regression has similitude with PCA
\end{itemize}

\paragraph{Examples of application}


\subsection{Generalized Linear Models (GLMs)}
Models in which the output density is in the exponential family and in which the mean
parameters are a linear combination of the inputs, passed through a possibly nonlinear
function such as the logistic function.\\
Exponential family:
\tR{$\prob{\bm{x}|\bm{\theta}} = \dfrac{1}{Z(\bm{\theta})}h(x)e^{\bm{\theta}^{T}\phi(
\bm{x})} = h(\bm{x})e^{\bm{\theta}^{T}\phi(\bm{x}) - A(\bm{\theta})}$}
where
$\begin{cases}
    Z(\bm{\theta}) = \su{\mathcal{X}^{m}}{}h(\bm{x})e^{\bm{\theta}^{T}\phi(\bm{x})}
    d\bm{x} \\
    A(\bm{\theta}) = \log\left(Z(\bm{\theta})\right)
\end{cases}$
with the \emph{natural parameters} $\bm{\theta}$, the vector of  \emph{sufficient 
statistics} $\phi(\bm{x})$, the \emph{partition function} $Z(\bm{\theta})$, the 
\emph{log partition function} or \emph{cumulant function} $A(\bm{\theta})$and the 
\emph{scaling constant} $h(\bm{x})$.\\
The name \emph{cumulant function} comes from the property of the exponential family 
being that derivatives of the log partition function can be used to generate cumulant
of the sufficient statistics (the first and second cumulants of a distribution being
its mean and variance).
\paragraph{Purpose}
We have the following data distribution:
$\prob{y_{i}|\bm{\theta},\sigma^{2}} = \exp\left(\dfrac{y_{i}\bm{\theta} - A\left(
\bm{\theta}\right)}{\sigma^{2}} + c(y_{i}, \sigma^{2})
\right)$
with $c$ a normalization constant.\\
Let's consider an invertible mapping $\Phi$ such that $\bm{\theta} = \Phi(\mu)$ with
$\mu$ being the mean parameter and $\bm{\theta}$ the natural parameter.\\
We have as well \tB{$\mu = \Phi^{-1}(\bm{\theta}) = A'(\bm{\theta})$}.\\
We are free to chose any link function as long as the inverse have an appropriate 
range. A simple form of link function is to use $g=\Phi$, $g$ is then called 
\emph{canonical function}.

\begin{center}
    \begin{tabular}{|*{4}{l|}}
    \hline
    \textbf{Distribution} & \textbf{Link function}  & 
    \textbf{Natural parameter} $\theta = \Phi(\mu)$ & \textbf{Mean parameter} $\mu = 
    \Phi^{-1}(\theta) = \mathbb{E}(y)$ \\
    \hline
    $\mathcal{N}(\mu, \sigma^{2})$ & \emph{identity} & $\bm{\theta} = \bm{\mu}$ & 
    $\bm{\mu} = \bm{\theta}$\\
    \hline
    $\mathcal{B}(n,\mu)$ & \emph{logit} & $\bm{\theta} = \log\left(\frac{\mu}{1-\mu}
    \right)$ & $\mu = \text{\emph{sigm}}(\bm{\theta})$\\
    \hline
    \emph{Poisson}$(\mu)$ & \emph{log} & $\bm{\theta} = \log\left(\mu\right)$ & 
    $\mu=e^{\theta}$\\
    \hline
    \end{tabular}
\end{center}

\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\begin{itemize}
    \item GLMs can be fit using methods like gradient descent.
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Learning to rank}
\paragraph{Purpose}
Modeling a function being able to rank a set of items.



\paragraph{Assumptions}
\paragraph{Theory}
Let us consider a document $d$ and a query $q$, a standard way to measure the relevance
between the both is to use $\text{\emph{sim}}(q,d) \triangleq \prob{q|d} = \prd{i=1}{n}
\prob{q_{i}}{d}$ with $q_{i}$ being the $i^{\text{\emph{th}}}$ word or term of $q$.
\subparagraph{Pointwise approach}
for binary relevance labels, we can follow a standard binary classification scheme to
estimate $\prob{y=1|\bm{x}(q,d)}$, in the case of ordered relevancy labels we can use
an \tB{\emph{ordinal regression} to predict the rating $\prob{y=r|\bm{x}(q,d)}$}.\\
However this method does not take into account the location of each document in the 
list.
\subparagraph{Pairwise approach}
to \tB{check the relative relevance of two items rather than absolute relevance}. We 
can model this kind of data using a binary classifier of the form \tB{$\prob{y_{jk}|
\bm{x}(q,d_{j})\bm{x}(q,d_{k})} = \sigma\left(f(\bm{x}_{j}) - f(\bm{x}_{k})\right)$} 
where $f$ is a scoring function, often taken to be linear: $f(\bm{x}) = \bm{w}^{T}
\bm{x}$.

\subparagraph{Listwise approach}
we now consider methods looking at the entire list of items at the same time. We can 
define a total order on a list by specifying a permutation of its indices: $\bm{\pi}$.
To model the uncertainty about $\bm{\pi}$ we can use the \emph{Plackett-Luce} 
distribution. $\prob{p(\bm{\pi}|\bm{s})} = \prd{j=1}{m}\dfrac{s_{j}}{\su{u=j}{m}s_{u}}$
Where $s_{j}=s\left(\pi^{-1}(j)\right)$
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}
\begin{itemize}
    \item \emph{information retrieval} return a list of the top $k$ more relevant 
        documents, depending on a given query 
\end{itemize}

\subsection{Supervised PCA}
\paragraph{Purpose}
Also called \emph{Bayesian factor regression} this model take in account $y_{i}$ when
learning the low dimension embedding.
\paragraph{Assumptions}
\paragraph{Theory}
$\begin{cases}
    \prob{\bm{z}_{i}} = \mathcal{N}\left(\bm{0}, \bm{I}_{L}\right) \\
    \prob{y_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{w}_{y}^{T}\bm{z}_{i},
    +\mu_{y},\sigma_{y}^{2}\right) \\
    \prob{x_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{W}_{x}^{T}\bm{z}_{i},
    +\bm{\mu}_{x},\sigma_{x}^{2}\bm{I}_{D}\right)
\end{cases}$
The basic idea compressing $\bm{x}_{i}$ to predict $y_{i}$ can be formulated using
information theory, in particular we might want to find an encoding distribution
$\prob{\bm{z}|\bm{x}}$ such that we minimize $\mathbb{I}(X;Z) - \beta\mathbb{I}(X;Y)$.
Where $\beta\geq 0$, the \emph{information bottleneck} is some parameter controlling 
the trade-off between compression and predictive accuracy.
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\begin{itemize}
    \item PCA
\end{itemize}

\paragraph{Examples of application}
\begin{itemize}
    \item predict the movies that you would like knowing who your friends are as well 
        as the rating from other users
\end{itemize}


\subsection{Partial Least Squares}
\paragraph{Purpose}
The key idea is to allow some of the (co)variance in the input features to be explained
by its own subspace $\bm{z}_{i}^{x}$ and to let the remaining of subspace $\bm{z}_{i}^{
s}$ be shared between input and output.
\paragraph{Assumptions}
\paragraph{Theory}
$\begin{cases}
    \prob{\bm{z}_{i}} = \mathcal{N}\left(\bm{z}^{s}_{i}|\bm{0},\bm{I}_{L_{s}}\right)
    \mathcal{N}\left(\bm{z}^{x}_{i}|\bm{0},\bm{I}_{L_{x}}\right)\\
    \prob{\bm{y}_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{W}_{y}\bm{z}^{s}_{i} + 
    \bm{\mu}_{y},\sigma^{2}\bm{I}_{D_{y}}
    \right)\\
        \prob{\bm{x}_{i}|\bm{z}_{i}} = \mathcal{N}\left(\bm{W}_{x}\bm{z}^{s}_{i} + \bm{B}_{x}
        \bm{z}^{x}_{i} + \bm{\mu}_{x},\sigma^{2}\bm{I}_{D_{x}}\right)
\end{cases}$
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
