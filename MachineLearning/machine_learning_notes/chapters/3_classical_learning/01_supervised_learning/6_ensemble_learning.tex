The overall purpose is to learn a weighted combination of base models of the form:
\begin{center}
    $f(y|\bm{x},\bm{\pi}) = \su{m\in\mathcal{M}}{}w_{m}f_{m}(y|\bm{x})$
\end{center}
where $w_{m}$ are tunable parameters

\subsection{Boosting}
\paragraph{Purpose}
It aims to \tB{build a model with reduced bias and variance}.\\
It is a greedy algorithm for fitting adaptive basis-function models \tB{$f(\bm{x}) = 
w_{0} + \su{m=1}{M}w_{m}\phi_{m}(\bm{x})$}, where $\phi_{m}$ are generated by an
algorithm called \textbf{weak learner}. \uB{This algorithm works by applying the weak
learner sequentially to weighted versions of the data, where more weight is given to
examples that were misclassified by earlier rounds.}\\
This weak learner can be any classification or regression algorithm, but \tB{it common
to use CART model}.\\
\tB{Boosting is very resistant to overfitting.}\\
The goal of boosting is to solve the following optimization problem:
\begin{center}
    $\displaystyle\min_{f}\su{i=1}{n}L(y_{i},f(\bm{x}_{i}))$
\end{center}

\paragraph{Assumptions}
A \uB{family of \emph{weak learners}} (working slightly better that random guess) 
\uB{can be transformed in a family of \emph{strong learners}}

\paragraph{Theory}
\subparagraph{Forward stagewise additive modeling}
For squared error loss the optimal estimate is given by: $f^{*}(\bm{x}) = \displaystyle
\argmin_{f(\bm{x})}=\mathbb{E}_{y|\bm{x}}\left(\left[y-f(\bm{X}) \right]^{2}\right) = \E{y|\bm{x}}$\\
For binary classification we can use the logloss which is a convex upper bound on 0-1 loss. One can
show that $f^{*}(\bm{x}) = \dfrac{1}{2}\log\left(\dfrac{\prob{\hat{y}=1|\bm{x}}}{\prob{\hat{y}=-1|
\bm{x}}}\right)$

\begin{enumerate}
    \item initialise by defining $f_{0}(\bm{x}) = \displaystyle\argmin_{\gamma}\su{i=1}{n}L\left(
        y_{i},f(\bm{x};\gamma)\right)$, for example if we use squared error we can
        set $f_{0}(\bm{x})=\overline{y}$ and if we use log-loss or exponential loss we can set       
        $f_{0}=\frac{1}{2}\log\left(\frac{\hat{\pi}}{1-\hat{\pi}}\right)$ where $\hat{\pi} = 
        \frac{1}{n}\su{i=1}{n}\mathbbm{1}_{\{y_{i}=1\}}$
    \item at iteration $m$ we compute $(\beta_{m},\gamma_{m}) = \displaystyle\argmin_{\beta,\gamma}
        \su{i=1}{n}L\left(y_{i},f_{m-1}(\bm{x}_{i})+\beta\phi(\bm{x}_{i};\gamma) 
        \right)$ where $\phi$ is generated by the \emph{weak learner} algorithm.
    \item $f_{m}(\bm{x}) = f_{m-1}(\bm{x}) + \nu\beta_{m}\phi(\bm{x}_{i};\gamma_{m})$ where $\nu\in
        [0,1]$ is a step-size parameter.
\end{enumerate}

\begin{center}
    \begin{tabular}{|*{5}{c|}}
    \hline
    \textbf{Name} & \textbf{Loss}& \textbf{Derivate}& \textbf{$f{*}$} & \textbf{Algorithm}\\
    \hline
    \emph{Squared error} & $\frac{1}{2}\left(y_{i}-f(\bm{x}_{i})\right)^{2}$ & 
    $y_{i}-f(\bm{x}_{i})$ & $\E{y|\bm{x}_{i}}$ & \emph{L2Boosting}\\
    \hline
    \emph{Absolute error} & $|y_{i}-f(\bm{x}_{i}|$ & 
    $sgn(y_{i}-f(\bm{x}_{i}))$ & \emph{median}$(y|\bm{x}_{i})$ & \emph{Gradient 
    Boosting}\\
    \hline
    \emph{Exponential loss} & $\exp\left(-y_{i},f(\bm{x}_{i})\right)$ & $-y_{i}\exp(
    -y_{i}f(\bm{x}_{i}))$ & $\frac{1}{2}\log\left(\frac{\pi_{i}}{1 - \pi_{i}}\right)$&
    \emph{AdaBoost}\\
    \hline
    \emph{Logloss} & $\log\left(1+e^{-y_{i}f(\bm{x}_{i})}\right)$ & $-y_{i}-\pi_{i}$ &
    $\frac{1}{2}\log\left(\frac{\pi_{i}}{1 - \pi_{i}}\right)$ & \emph{LogitBoost}\\
    \hline
    \end{tabular}
\end{center}
where $\pi=\sigma\left(2f(\bm{x})\right)$


\paragraph{Strengths}
- robust against overfitting
- reduce bias and variance
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Bayes Model Averaging (that is not an Ensemble Method)}
\paragraph{Purpose}
Instead of picking the best model and then using this to make predictions we make
a weighted average of the predictions made by each model:
\paragraph{Assumptions}
\paragraph{Theory}
Let's compute:
\begin{center}
    $\Prob{y|\bm{x},\bm{\mathcal{D}}} = \su{m\in\mathcal{M}}{}
    \Prob{y|\bm{x}, m, \bm{\mathcal{D}}}\Prob{m|\bm{\mathcal{D}}}$
\end{center}
Obviously averaging over all the models is computationally infeasible, then a simple
approximation 
\paragraph{Strengths}
\begin{itemize}
    \item Better results than using any single model
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Stacking}
\paragraph{Purpose}
Refers to learning a weighted combination of base models of the form:
$f(y|\bm{x},\bm{\pi}) = \su{m\in\mathcal{M}}{}w_{m}f_{m}(y|\bm{x})$ where $w_{m}$ are
the tunable parameters.
\paragraph{Assumptions}
\paragraph{Theory}
To estimate the weights we can use : 
\begin{center}
    $\hat{\bm{w}} = \displaystyle\argmin_{\bm{
w}}\su{i=1}{n}L\left(y_{i},\su{m=1}{M}w_{m}\hat{f}_{m}^{-i}(\bm{x})\right)$
\end{center}
with 
$\hat{f}_{m}^{-i}(\bm{x})$ being the predictor obtained by training on data excluding
$(\bm{x}_{i},y_{i})$. It would allow to avoid overfitting that would produce in using
simply $f_{m}$. This know as \textbf{stacking} or \textbf{stacked generalization}.
\paragraph{Strengths}
\begin{itemize}
    \item Robustness when the "true" model is not in the model class.
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}
\begin{itemize}
    \item analogy with neural networks: $f_{m}$ representing the $m^{th}$ hidden unit 
        and $w_{m}$ are the outputs layer weights
    \item analogy with boosting: where the weights on the base models are determined 
        sequentially
\end{itemize}



\subsection{Boostrap Aggregating (Bagging)}
\paragraph{Purpose}
It is designed to \tB{improve the stability and accuracy} of machine learning 
algorithms.
\paragraph{Assumptions}
\paragraph{Theory}
Let's consider \uB{$M$ different models $\left(f_{m}\right)_{1\leq m\leq M}$} that we 
will train on random different subsets of data (\emph{bootstrap step}).\\
We then compute the ensemble:
\begin{center}
    $f(\bm{x}) = \su{m=1}{M}\dfrac{1}{M}f_{m}(\bm{x})$
\end{center}

\paragraph{Strengths}
\begin{itemize}
    \item Better results than using any single model
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item Variance reduction not very efficient because of rerunning same learning
        algorithm on different subsets of the data can result in highly correlated
\end{itemize}


\paragraph{Relationships with other methods}
\begin{itemize}
    \item Random forest: use bagging in randomly cutting the input space vertically 
        (subsets of predictors) and horizontally (subset of observations)
\end{itemize}

\paragraph{Examples of application}
