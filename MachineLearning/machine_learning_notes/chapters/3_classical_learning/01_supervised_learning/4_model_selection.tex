\subsection{Bayesian Variable Selection}
\paragraph{Purpose}
Let \tB{$\gamma_{j}: 
\begin{cases}
    \gamma_{j} = 1 \Leftarrow \text{feature } j \text{ is relevant}\\
    \gamma_{j} = 0 \Leftarrow \text{otherwise}\\
\end{cases}$} our goal is to compute the posterior over models:
\begin{center}
    $\prob{\gamma|\mathcal{D}} = \dfrac{e^{-f(\gamma)}}{\su{\gamma'}{}e^{-f(\gamma')}}$
\end{center}
where the cost function is defined by $f(\gamma) \triangleq -\left[\log\left(\prob{
\mathcal{D}|\gamma}\right) + \log\left(\prob{\gamma}\right)\right]$

\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{Spike and slab model}
remember that posterior is given by $\prob{\bm{\gamma}|\mathcal{D}}\propto \prob{
\mathcal{D}|\bm{\gamma}}\prob{\bm{\gamma}}$.\\
It is common to use the following prior $\prob{\bm{\gamma}} = \prd{j=1}{D}\text{
\emph{Ber}}(\gamma_{j}|\pi_{0}) = \pi_{0}^{\norm{\gamma}_{0}}(1-\pi_{0})^{D-\norm{
\gamma}_{0}}$, where $\pi_{0}$ is the probability that a feature is relevant.\\
The likelihood is defined as follows: $\prob{\mathcal{D}|\bm{X},\bm{\gamma}} = 
\Su{}{}\Su{}{}\prob{\bm{y}|\bm{X},\bm{w},\bm{\gamma}}\prob{\bm{w}|\bm{\gamma},
\sigma^{2}}\prob{\sigma^{2}}d\bm{w}d\sigma^{2}$\\
Consider the prior $\prob{\bm{w}|\bm{\gamma},\sigma^{2}}$ in standardizing the inputs, 
a reasonable prior is $\mathcal{N}(0, \sigma^{2}\sigma_{w}^{2})$, where 
$\sigma_{w}^{2}$ controls how big we expect the coefficients associated with the 
relevant variables to be, which is scaled by the overall noise. We can summarize this 
prior as follows: $\prob{\bm{w}_{j}|\sigma^{2},\gamma_{j}}
\begin{cases}
    \delta_{0}(w_{j}) \Leftarrow \gamma_{j} = 0\\
    \mathcal{N}(w_{j}|0, \sigma^{2}\sigma_{w}^{2}) \Leftarrow \gamma_{j} = 1
\end{cases}$
\uB{the first term is a "spike" at the origin, as $\sigma_{w}^{2}\rightarrow +\infty$ 
the distribution $\prob{w_{j}|\gamma_{j} = 1}$ approaches a uniform distribution which 
can be thought of as a "slab"}.

\subparagraph{Bernoulli-Gaussian model}
we have 
$\begin{cases}
    \prob{y_{i}|\bm{x}_{i},\bm{w}, \bm{\gamma},\sigma^{2}} = \mathcal{N}\left(\su{j}{}
    \gamma_{j}w_{j}x_{ij}, \sigma^{2}\right) \\
    \prob{\gamma_{j}} = \text{\emph{Ber}}(\pi_{0})\\
    \prob{w_{j}} = \mathcal{N}(0, \sigma_{w}^{2})
\end{cases}$
we can think of the \tB{$\gamma_{j}$ as a masking out the weights $w_{j}$}. Unlike the
spike and slab model we do not integrate the irrelevant coefficients, they always 
exists.\\
\uB{One interesting aspect of this model is that it can be used to derive objective 
function that is widely used in the non-Bayesian subset selection literature.}
\subparagraph{Algorithms}
assuming that we want to find the MAP model.
\begin{itemize}
    \item Single best replacement: at each step, we define a \uB{neighborhood of the 
        current model to be all models than can be reached by flipping a single bit
        of $\gamma$}
    \item Orthogonal least squares: we start from an empty set of variables and we 
        add the best feature \tB{$j^{*} = \displaystyle\argmin_{j\notin\bm{\gamma}_{t}}\displaystyle
            \min_{\bm{w}}\norm{\bm{y}-\bm{X}_{\bm{\gamma}_{t}\cup j}\bm{w}}^{2}$} 
    \item Orthogonal matching pursuits: as Orthogonal least squares is somewhat 
        expensive, a simplification is to freeze the current weight and then pick the 
        next feature to add by solving \tB{$j^{*} = \displaystyle\argmin_{j\notin\bm{\gamma}_{t}}
        \displaystyle\min_{\beta} \norm{\bm{y}-\bm{X}\bm{w}_{t} - \beta\bm{x}_{\cdot j}}^{2}$}. And
        $\beta = \dfrac{\bm{x}_{\cdot j}^{T}\bm{r}_{t}}{\norm{\bm{x}_{\cdot j}}^{2}}$, where
        $\bm{r} = \bm{y}-\bm{X}\bm{w}_{t}$
    \item Backward selection: starts with all variables in the model and deletes the
    \item Bayesian matching pursuits: similar to OMP except it uses a Bayesian marginal
        likelihood scoring criterion instead of a least square objective.
\end{itemize}

\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Least Angle Regression Shrinkage}
\paragraph{Purpose}
Instead of updating only one variable at a time, inducing slow converging, we can use \uB{\emph{
Active set} methods that update many variables at a time}. They add or remove a few variables at
a time so they can take a long time if they started far from the solution.\\
Suppose $\mathcal{A}_{k}$ is the active set of variables at the 
beginning of the $k^{th}$ step and let $\beta_{\mathcal{A}_{k}}$ be the
coefficient vector for these variables at this step, there will be 
$k-1$ nonzero values and the one just entered will be zero.\\
If \uB{$\bm{r}_{k}=\bm{y}-\bm{X}_{\mathcal{A}_{k}}\beta_{\mathcal{A}_{k}}$}
is the current residual, then the direction for this step is:
\begin{center}
    $\delta_{k}=(\bm{X}_{\mathcal{A}_{k}}^{T}\bm{X}_{\mathcal{A}_{k}})^{-1}\bm{X}_{\mathcal{A}_{k}^{T}}^{T}\bm{r}_{k}$
\end{center}
The name ``least angle'' arises from a geometrical interpretation of 
this process; $\bm{u}_{k}$ makes the smallest (and equal) angle with
each of the predictors in $\mathcal{A}_{k}$
\paragraph{Assumptions}
\paragraph{Theory}
Active methods exploit the fact that one can quickly compute $\hat{\bm{w}}(\lambda_{k})$ from 
$\hat{\lambda}(\lambda_{k-1})$ if $\lambda_{k}\approx\lambda_{k-1}$ this is known as \textbf{warm
starting}.

\subparagraph{Least Angle Regression}
\begin{enumerate}
	\item \uB{Standardize the predictors} to have mean zero an unit 
		norm.\\ \uB{Start with the residual $\bm{r}=\bm{y}-
		\overline{\bm{y}}$}, and for all $j\in\inter{1}{p},~\beta_{j}=0$
    \item Find the predictor $x_{j}$ most correlated with $\bm{r}$
    \item For $j\in\inter{1}{p}$
    \begin{itemize}
        \item Move $\beta_{j}$ from 0 to its least-squares coefficient
            $\dfrac{\sP{\bm{x}_{j}}{\bm{r}}}{\sP{\bm{x}_{j}}{\bm{x}_{j}}}$, then recompute 
            $\bm{r}$. Find the predictor $x_{k}$ most correlated with the new $\bm{r}$
        \item Move $\beta_{k}$ from 0 to its least-squares coefficient
            $\dfrac{\sP{\bm{x}_{k}}{\bm{r}}}{\sP{\bm{x}_{k}}{\bm{x}_{k}}}$, then recompute 
            $\bm{r}$. Find the predictor $x_{l}$ most correlated with the new $\bm{r}$\dots
        \item Move $\beta_{j}$ and $\beta_{k}$ in the direction defined
            by their joint least squares coefficient of the current
            residual on $(x_{j},x_{k})$, until some other 
            competitor $x_{l}$ has as much correlation with the
            current residual.
    \end{itemize}
	\item After $\min(N-1, p)$ steps, we arrive at the
		full least-squares solution.
\end{enumerate}

\subparagraph{Least Angle Regression: Lasso Modification}
\begin{itemize}
	\item[4a] If a non-zero coefficient hits zero, drop its variable from the
		active set of varibalbes and recompute the current joint least squares
		direction
\end{itemize}


\paragraph{Strengths}
\begin{itemize}
	\item Numerically efficient when $p\gg n$
	\item As fast as forward selection and has the same order of complexity as OLS
	\item Produces a full piecewise linear solution path
	\item Stable
	\item Esealy modified to produce for other estimators like the Lasso
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item Because LARS is based upon iterative refitting of the residuals, it would
		appears to be especially sensitive to the effect noise.
\end{itemize}

\paragraph{Relationships with other methods}
\begin{itemize}
    \item Least Squares
\end{itemize}

\paragraph{Examples of application}


%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
