\subsection{$l_{2}$ regularization}
\paragraph{Purpose}
\begin{itemize}
    \item Prevent overfitting
    \item Improve condition number 

\end{itemize}

\paragraph{Assumptions}
We assume \tB{$\prob{\bm{w}} = \prd{j=1}{D}\mathcal{N}(w_{j} |0,\tau^{2})$}, where
$\frac{1}{\tau^{2}}$ controls the strength of the prior! 

\paragraph{Theory}
The corresponding MAP estimation problem is :

\begin{center}
    \frB{$\argmax{\bm{w}}\su{i=1}{n}\log\left(\mathcal{N}(y_{i}|w_{0} +
    \bm{w}^{T}\bm{x}_{i}, \sigma^{2})\right) + \su{i=1}{D}\log\left(\mathcal{N}(w_{j}
|0, \tau^{2})\right)$}
\end{center}
We can rewrite this as 
\begin{center}
    $\dfrac{1}{n}\norm{\bm{y} - \bm{X}\bm{w}}^{2} + \lambda\norm{\bm{w}}^{2}_{2}$
\end{center}
The optimal solution is 
\begin{center}
    $\bm{w}_{ridge} = \left(\lambda\bm{I}_{D} + \bm{X}^{T}\bm{X}\right)^{-1}\bm{X}^{T}
    \bm{y}$
\end{center}
\paragraph{Relationships with other methods}

\subparagraph{Connection with PCA}
Let $\bm{X} = \bm{USV}^{T}$ be the \emph{SVD} of $\bm{X}$.
$\hat{\bm{w}}_{ridge} = \bm{V}\left(\bm{S}^{²} +\lambda\bm{Ij}\right)^{-1}\bm{SU}^{T}\bm{y}$
Hence 
\begin{align*}
    \hat{\bm{y}} &= \bm{X}\hat{\bm{w}}_{ridge}\\
                 &= \bm{USV}^{T}\bm{V}\left(\bm{S}^{²} +\lambda\bm{Ij}\right)^{-1}\bm{SU}^{T}\bm{y}\\
                 &= \bm{U\tilde{S}U}^{T}\bm{y}\\
                 &= \su{j=1}{p}\bm{u}_{j}\bm{\tilde{S}}_{jj}\bm{u}_{j}^{T}\bm{y}
\end{align*}
where $\bm{\tilde{S}}_{jj}
\triangleq \left[\bm{S}\left(\bm{S}^{2} + \lambda\bm{I}\right)^{-1}\bm{S}\right]_{jj} = 
\dfrac{\sigma_{j}^{2}}{\sigma_{j}^{2} + \lambda}$ and $\sigma_{j}$ are the singular values of 
$\bm{X}$.
Therefore 
\begin{center}
    \fr{$\hat{\bm{y}} = \bm{X}\hat{\bm{w}}_{ridge} = \su{j=1}{p}\bm{u}_{j}\dfrac{\sigma_{j}^{2}}{
    \sigma_{j}^{2} + \lambda}\bm{u}_{j}^{T}\bm{y}$}
\end{center}
Whereas in ordinary least squares we have:
$\hat{\bm{y}} = \bm{X}\hat{\bm{w}}_{ols} = \left(\bm{USV}^{T}\right)\left(\bm{VS}^{-1}\bm{U}^{T}
\bm{y}\right) = \bm{UU}^{T}\bm{y}=\su{j=1}{p}\bm{u}_{j}\bm{u}_{j}^{T}\bm{y}$


\subsection{$l_{1}$ regularization}
\paragraph{Purpose}

\paragraph{Assumptions}
We assume \tB{$\prob{\bm{w}|\lambda} = \prd{j=1}{D}\text{\emph{Lap}}(w_{j}|0,
\frac{1}{\lambda}) \propto \prd{j=1}{D}e^{-\lambda|w_{j}|}$}
\paragraph{Theory}
For penalized negative log likelihood has the form: $f(\bm{w}) = -\log\left(\prob{
\mathcal{D}|\bm{w}}\right) - \log\left(\prob{\bm{w}|\lambda}\right) = \text{\emph{NLL}}
(\bm{w}) + \lambda\norm{\bm{w}}_{1}$.\\
Geometrically we understand that as we relax the constraint we grow $l_{1}$ ball until
it meets the objective, \tB{the corners of the ball are more likely to intersect the
ellipse than one of the sides} especially in high dimensions because the corners "stick
out". The corners correspond to sparse solutions which lie on the coordinate axes. By
contrast when we grow the $l_{2}$ ball it can intersect the objective at any point, 
there are no "corners" so there is no preference for sparsity. 
\paragraph{Strengths}

\paragraph{Weaknesses}
\begin{itemize}
    \item can give quite different results if the data is slightly perturbed
\end{itemize}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Noise Robustness}
\paragraph{Purpose}
It can be \tB{much more powerful than shrinking the parameters} 
\paragraph{Assumptions} 
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Regularization path}
\paragraph{Purpose}
As we increase $\lambda$, the solution vector $\hat{\bm{w}}(\lambda)$ will tend to get
sparser, although not necessary monotonically. For each feature $j$ we can plot the 
values $\hat{w}_{j}(\lambda)$ vs $\lambda$ which is known as \emph{regularization path}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Coordinate Descent}
\paragraph{Purpose}
These algorithms exploit the fact that one can quickly compute $\hat{\bm{w}}(
\lambda_{k})$ from $\hat{\bm{w}}(\lambda_{k-1})$ if $\lambda_{k} \approx \lambda_{k-1}$
this is known as \emph{warm starting}.\\
Useful when it is hard to optimize all the variables simultaneously 
\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{Aim of Coordinate descent}
We solve for the $j^{th}$ coefficient with all the others held fixed: 
\begin{center}
    \fr{$w^{*}_{j} = \displaystyle\argmin_{z}f(\bm{w} + z\bm{e}_{j}) - f(\bm{w})$} with $\bm{e}_{j}$
    is the \emph{j}'th unit vector. 
\end{center}


\subparagraph{Algorithm for Lasso}
\begin{enumerate}
    \item Initialise $\bm{w} = \left(\bm{X}^{T}\bm{X} + \lambda\bm{I}\right)^{-1}\bm{X}^{T}y$
    \item Repeat until convergence:\\for $j\in\inter{1}{D}$
        \begin{itemize}
            \item $\alpha_{j} = 2\su{i=1}{n}x_{ij}^{2}$
            \item $c_{j} = 2\su{i=1}{n}x_{ij}(y_{i} - \bm{w}^{T}\bm{x}_{i} + w_{j}x_{ij})$
            \item $w_{j} = soft\left(\dfrac{c_{j}}{\alpha_{j}}, \dfrac{\lambda}{\alpha_{j}}\right)$
        \end{itemize}
\end{enumerate}


\paragraph{Strengths}
\begin{itemize}
    \item if each one-dimensional optimization problem can be solved analitically
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
