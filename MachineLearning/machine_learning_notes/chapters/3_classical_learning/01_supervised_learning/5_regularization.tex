\subsection{$l_{1}$ regularization}
\paragraph{Purpose}

\paragraph{Assumptions}
We assume $\prob{\bm{w}|\lambda} = \prd{j=1}{D}\text{\emph{Lap}}(w_{j}|0,
\frac{1}{\lambda}) \propto \prd{j=1}{D}e^{-\lambda|w_{j}|}$
\paragraph{Theory}
For penalized negative log likelihood has the form: $f(\bm{w}) = -\log\left(\prob{
\mathcal{D}|\bm{w}}\right) - \log\left(\prob{\bm{w}|\lambda}\right) = \text{\emph{NLL}}
(\bm{w}) + \lambda\norm{\bm{w}}_{1}$.\\
Geometrically we understand that as we relax the constraint we grow $l_{1}$ ball until
it meets the objective, \tB{the corners of the ball are more likely to intersect the
ellipse than one of the sides} especially in high dimensions because the corners "stick
out". The corners correspond to sparse solutions which lie on the coordinate axes. By
contrast when we grow the $l_{2}$ ball it can intersect the objective at any point, 
there are no "corners" so there is no preference for sparsity. 
\paragraph{Strengths}

\paragraph{Weaknesses}
\begin{itemize}
    \item can give quite different results if the data is slightly perturbed
\end{itemize}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Regularization path}
\paragraph{Purpose}
As we increase $\lambda$, the solution vector $\hat{\bm{w}}(\lambda)$ will tend to get
sparser, although not necessary monotonically. For each feature $j$ we can plot the 
values $\hat{w}_{j}(\lambda)$ vs $\lambda$ which is known as \emph{regularization path}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Coordinate Descent}
\paragraph{Purpose}
These algorithms exploit the fact that one can quickly compute $\hat{\bm{w}}(
\lambda_{k})$ from $\hat{\bm{w}}(\lambda_{k-1})$ if $\lambda_{k} \approx \lambda_{k-1}$
this is known as \emph{warm starting}.\\
Useful when it is hard to optimize all the variables simultaneously 
\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{Aim of Coordinate descent}
We solve for the $j^{th}$ coefficient with all the others held fixed: 
\begin{center}
    \fr{$w^{*}_{j} = \displaystyle\argmin_{z}f(\bm{w} + z\bm{e}_{j}) - f(\bm{w})$} with $\bm{e}_{j}$
    is the \emph{j}'th unit vector. 
\end{center}


\subparagraph{Algorithm for Lasso}
\begin{enumerate}
    \item Initialise $\bm{w} = \left(\bm{X}^{T}\bm{X} + \lambda\bm{I}\right)^{-1}\bm{X}^{T}y$
    \item Repeat until convergence:\\for $j\in\inter{1}{D}$
        \begin{itemize}
            \item $\alpha_{j} = 2\su{i=1}{n}x_{ij}^{2}$
            \item $c_{j} = 2\su{i=1}{n}x_{ij}(y_{i} - \bm{w}^{T}\bm{x}_{i} + w_{j}x_{ij})$
            \item $w_{j} = soft\left(\dfrac{c_{j}}{\alpha_{j}}, \dfrac{\lambda}{\alpha_{j}}\right)$
        \end{itemize}
\end{enumerate}


\paragraph{Strengths}
\begin{itemize}
    \item if each one-dimensional optimization problem can be solved analitically
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
