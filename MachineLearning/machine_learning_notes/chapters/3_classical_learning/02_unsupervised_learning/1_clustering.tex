\subsection{\emph{K-means} algorithm}
\paragraph{Purpose}
Consider a \emph{GMM} in which  we make the following assumptions $\bm{\Sigma}_{k}=
\sigma^{2}\bm{I}_{D}$ is fixed and $\pi_{k} = \frac{1}{K}$, then only the cluster 
centers $\mu_{k}\in\mathbb{R}^{D}$ have to be estimated.\\
Now consider $\prob{z_{i} = k|\bm{x}_{i},\bm{\theta}} \approx \mathbbm{1}_{\{k=z_{i}^{
*}\}}$, where $z_{i}^{*} = \argmax_{k}\prob{z_{i} = k}|\bm{x}_{i},\bm{\theta}$.
As we are making a hard assignment of points to clusters, this is sometimes called 
\emph{hard EM}. Since we assumed an equal spherical covariance matrix for each cluster 
we have $z_{i}^{*} = \argmin_{k}\norm{\bm{x}_{i} -\bm{\mu}_{k}}_{2}^{2}$\\
The \emph{M step} updates each cluster center by computing the mean of all points 
assigned to it:
$\bm{\mu}_{k} = \dfrac{1}{n_{k}}\su{i:z_{i}=k}{}\bm{x}_{i}$.
Since K-means is not a proper EM algorithm it is not maximizing likelihood, instead 
\uB{it can be interpreted as a greedy algorithm minimizing a loss function related to 
data compression.}
 

\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Factor Analysis}
\paragraph{Purpose}
To have a representation power through latent variables.
\paragraph{Assumptions}
\paragraph{Theory}
Let's consider 
$\begin{cases}
    \prob{\bm{z}_{i}} = \mathcal{N}\left(\bm{z}_{i}|\bm{\mu}_{0},\bm{\Sigma}_{0}
    \right) \\
    \prob{\bm{x}_{i}|\bm{z}_{i},\bm{\theta}} = \mathcal{N}\left(\bm{W}\bm{z}_{i}+
        \bm{\mu},\bm{\Phi}\right)
\end{cases}$
where $\bm{D}$ is a $D\times L$ matrix knows as \emph{factor loading matrix} and $\Phi$
is a $D\times D$ covariance matrix, which is diagonal as we aim to force $\bm{z}_{i}$
to explain correlation.
This model is then called \textbf{Factor Analysis}.
\subsection{Model selection}
for FA and PPCA ideally we can compute $L = \argmax_{L} \prob{L|\mathcal{D}}$ but
in practice we will use approximation like BIC a better approach is to perform 
exhaustive search over all candidates of $L$ while using a relevancy determination 
technique.
\paragraph{Strengths}
\begin{itemize}
    \item Use of real-valued latent variable $\bm{z}_{i}\in\mathbb{R}^{L}$
\end{itemize}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Principal Components Analysis}
\paragraph{Purpose}
Consider the \emph{FA} model where we constrain $\Phi = \sigma^{2}\bm{I}$ and $\bm{W}$
to be orthonormal. It can be shown that when $\sigma^{2}$ tends to 0, this model 
reduces to classical PCA, otherwise it will be \emph{Probabilistic PCA}.
\paragraph{Assumptions}
\paragraph{Theory}
Suppose we want to find an orthogonal set of $L$ linear basis vectors $\bm{w}_{j}\in
\mathbb{R}^{D}$ and the corresponding scores $\bm{z}_{i}\in\mathbb{R}^{L}$ such that we
minimize the average reconstruction error:
\begin{center}
    $\bm{J}(\bm{W}, \bm{Z}) = \dfrac{1}{n}\su{i=1}{n}\norm{\bm{x}_{i} - 
    \bm{\hat{x}}_{i}}^{2}$
\end{center}
where $\bm{\hat{x}}_{i} = \bm{W}\bm{z}_{i}$ subject to the constraint that $\bm{W}$
is orthonormal. 
\subsection{Model selection}

We can plot the \emph{scree plot} : $E(\mathcal{D}_{train}, L)$ vs $L$, with $E$ being
the error reconstruction. 
A related quantity is the fraction of variance explained defined as $F\left(
\mathcal{D}_{train}, L\right) = \dfrac{\su{j=1}{L}\lambda_{j}}{\su{j'=1}{L_{max}}
\lambda_{j'}}$

\paragraph{Strengths}
\begin{itemize}
    \item \emph{PCA} is the best low rank approximation to the data
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item It is not a proper generative model of the data, in providing more latent 
        dimensions it will be able to better approximate the test data
\end{itemize}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{PCA for categorical data}
\paragraph{Purpose}
Useful when data is categorical rather than real-valued
\paragraph{Assumptions}
\paragraph{Theory}
Let $\bm{X}\in\mathcal{M}_{np}(\{c\}_{1\leq c\leq C})$ and we assume that each $x_{ij}$
is generated from a latent variable $\bm{z}_{i}\in\mathbb{R}^{L}$.
$\begin{cases}
    \prob{\bm{z}_{i}} = \mathcal{N}(\bm{0}, \bm{I}) \\
    \prob{\bm{x}_{i}|\bm{z}_{i},\bm{\theta}} = \prod{j=1}{p}\text{\emph{Cat}}\left(
        x_{ij}|\mathcal{S}\left(\bm{W}_{j}^{T}\bm{z}_{i} + \bm{w}_{0j}\right)
    \right)
\end{cases}$
where $\bm{W}\in\mathbb{R}^{L\times M}$ is the factor loading matrix for response $j$ 
and $\bm{w}_{0j}\in\mathbb{R}^{M}$ is the offset term for response $j$.
To fit the model in using a modified version of EM: infer a Gaussian approximation to 
the posterior $\prob{\bm{z}_{i}|\bm{x}_{i},\bm{\theta}}$ in the E step and then to 
maximize $\bm{\theta} = \left(\bm{W}_{j},\bm{w}_{0j}\right)_{1\leq j\leq p}$ in M step.

\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Canonical Correlation Analysis}
\paragraph{Purpose}
It is a symmetric unsupervised version of PLS, it allows each view to have its own 
"private" subspace, but there is also a shared space.
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Independent Component Analysis (ICA)}
\paragraph{Purpose}
When we want to deconvolve mixed signals into their constituent parts.\\
\tB{Unlike PCA, we relax the Gaussian assumption about latent variable, now the 
distribution can be any non-Gaussian distribution.}
The reason the Gaussian distribution is disallowed as a source prior in ICA is that 
it does not permit unique recovery of the sources.

\paragraph{Assumptions}
\paragraph{Theory}
$\prob{\bm{z}_{t}} = \prd{j=1}{L}\mathbb{P}_{j}\left(z_{tj}\right)$
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
