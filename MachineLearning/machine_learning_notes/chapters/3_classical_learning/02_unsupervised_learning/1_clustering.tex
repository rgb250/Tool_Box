\subsection{Clustering in general}

\paragraph{Purpose}
2 kinds of inputs we might use, \emph{similarity-based clustering} the input to the algorithm is an
$N\times N$ dissimilarity matrix. In \emph{feature-based clustering} the input to the algorithm
is an $N\times D$ feature matrix or design matrix $\bm{X}$.


\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{Measuring dissimilarity}
$\Delta(\bm{x}_{i}, \bm{x}_{i'}) = \su{j=1}{D}\Delta_{j}(x_{ij},x_{i'j})$
\begin{itemize}
    \item \emph{Squared distance} $\Delta_{j}(x_{ij},x_{i'j}) = (x_{ij}-x_{i'j})^{2}$
    \item \emph{correlation} (useful for time-series of real-valued data) 
        $\Delta_{j}(x_{ij},x_{i'j}) = Cor(\bm{x}_{i},\bm{x}_{i'})$
    \item \emph{l1 distance} $\Delta_{j}(x_{ij},x_{i'j}) = |x_{ij}-x_{i'j}|$
    \item \emph{ordinal variables} any dissimilarity functions for quantitative function
     \item \emph{categorical distance} $\Delta_{j}(x_{ij},x_{i'j}) = \su{j=1}{D}\mathbbm{1}_{\{x_{ij}
         \neq x_{i'j}\}}$
\end{itemize}

\paragraph{Measuring dis(similarity)}
\paragraph{Evaluating the output of clustering methods}
The validation of clustering structures is the most difficult and frustrating part of cluster
analysis
\subparagraph{Purity}
$purity\triangleq \su{i}{}\dfrac{N_{i}}{N}p_{i}$
\subparagraph{Rand index}
\subparagraph{Mutual information}
\subparagraph{Varation of information}

\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}



\subsection{\emph{K-means} algorithm}
\paragraph{Purpose}
Consider a \emph{Gaussian Mixture Models} in which  we make the following assumptions \tB{$\bm{\Sigma
}_{k}= \sigma^{2}\bm{I}_{D}$} is fixed and \tB{$\pi_{k} = \frac{1}{K}$}, then only the cluster 
centers $\mu_{k}\in\mathbb{R}^{D}$ have to be estimated.\\
 
\paragraph{Assumptions}
\paragraph{Theory}
Now consider \tB{$\prob{z_{i} = k|\bm{x}_{i},\bm{\theta}} \approx \mathbbm{1}_{\{k=z_{i}^{
*}\}}$}, where \tB{$z_{i}^{*} = \argmax_{k}\prob{z_{i} = k|\bm{x}_{i},\bm{\theta}}$}.\\
As we are making a hard assignment of points to clusters, this is sometimes called 
\emph{hard EM}. \tR{Since we assumed an equal spherical covariance matrix for each cluster 
we have $z_{i}^{*} = \argmin_{k}\norm{\bm{x}_{i} -\bm{\mu}_{k}}_{2}^{2}$}\\
The \uB{\emph{M step} updates each cluster center} by computing the mean of all points 
assigned to it: \tB{$\bm{\mu}_{k} = \dfrac{1}{n_{k}}\su{i:z_{i}=k}{}\bm{x}_{i}$}.\\
Since K-means is not a proper EM algorithm it is not maximizing likelihood, instead 
\uB{it can be interpreted as a greedy algorithm minimizing a loss function related to 
data compression.}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\begin{itemize}
    \item Gaussian Mixture
\end{itemize}

\paragraph{Examples of application}


\subsection{Vector Quantization}
\paragraph{Purpose}
\uB{Suppose performing lossy compression of some real-valued vectors} 
$\bm{x}_{i}\in\mathbb{R}^{D}$.\\
The basic idea is to replace each real-valued vector $\bm{x}_{i}\in\mathbb{R}^{D}$ with a discrete
symbol $z_{i}\in\inter{1}{K}$ being an index into a \emph{codebook} of $K$ prototypes $\bm{\mu}_{k}
\in\mathbb{R}^{D}$
\paragraph{Theory}
Each data vector is encoded by using the index of the most similar prototype where similarity is
measured in terms of Euclidean distance:
\begin{center}
    $\text{\emph{encode}}(\bm{x}_{i}) = \displaystyle\argmin_{k}\norm{\bm{x}_{i}-\bm{\mu}_{k}}^{2}$
\end{center}
Here a cost function measuring the quality of the codebook by computing the \emph{reconstruction
error}:
\begin{center}
    \fr{$J(\bm{\mu},\bm{z}|K,\bm{X}) \triangleq \dfrac{1}{N}\su{i=1}{N}\su{i=1}{N}\norm{\bm{x}_{i}-
    decode(encode(\bm{x}_{i}))}^{2} = \dfrac{1}{N}\su{i=1}{N}\norm{\bm{x}_{i}-\bm{\mu}_{z_{i}}}^{2}$}
\end{center}

\paragraph{Assumptions}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Factor Analysis}
\paragraph{Purpose}
Getting a \tB{bigger representation power through latent variables than Mixture models in using
real-valued latent variable $\bm{z}_{i}\in\mathbb{R}^{L}$}.\\
It can be thought of as a \uB{way of specifying a joint density model on $\bm{x}$ using
a small number of parameters}.
\paragraph{Assumptions}
\paragraph{Theory}
Let's consider 
\tB{$\begin{cases}
    \prob{\bm{z}_{i}} = \mathcal{N}\left(\bm{z}_{i}|\bm{\mu}_{0},\bm{\Sigma}_{0}
    \right) \\
    \prob{\bm{x}_{i}|\bm{z}_{i},\bm{\theta}} = \mathcal{N}\left(\bm{W}\bm{z}_{i}+
        \bm{\mu},\bm{\Phi}\right)
\end{cases}$}
where $\bm{W}$ is a $D\times L$ matrix knows as \tR{\emph{factor loading matrix}} and \uB{$\Phi$
is a $D\times D$ covariance matrix, which is diagonal as we aim to force $\bm{z}_{i}$ to explain
correlation}.


\subsection{Mixture of Factor Analyzers}
\paragraph{Purpose}
While Factor Analysis assumes that data lives on a low dimensional \emph{linear} manifold, in reality
most data is better modeled by some form of low dimensional \emph{curved} manifold.\\
We can \tB{approximate a curved manifold by a piecewise linear manifold}
\paragraph{Assumptions}
\paragraph{Theory}
Let the $k^{th}$ linear subspace of dimensionality $L_{k}$ be represented by $\bm{W}_{k}$.
Suppose we have a latent indicator $q_{i}\in\inter{1}{k}$ specifying which subspace we should use 
to generate the data. Here the model:
\begin{center}
    \fr{$\begin{cases}
            \prob{\bm{x}_{i}|\bm{z}_{i},q_{i}=k, \bm{\theta}} &= \mathcal{N}\left(\bm{x}_{i}|\bm{W}_{
                k}\bm{z}_{i}, \Psi\right)\\
                    \prob{\bm{z}_{i}|\bm{\theta}} &= \mathcal{N}\left(\bm{z}_{i}|\bm{0}, \bm{I}
                    \right)\\
                        \prob{q_{i}|\bm{\theta}} &= \text{\emph{Cat}}\left(q_{i}|\bm{\pi}\right)
    \end{cases}$}
\end{center}

\paragraph{Strengths}
\begin{itemize}
    \item it is a good density model for high-dimensional real-valued data
\end{itemize}

\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}



\subsection{Canonical Correlation Analysis}
\paragraph{Purpose}
It is a symmetric unsupervised version of PLS, it allows each view to have its own 
"private" subspace, but there is also a shared space.
\paragraph{Assumptions}
\paragraph{Theory}
\tB{If we have 2 observed variables $\bm{x}_{i}$ and $\bm{y}_{i}$ then we have 3 latents variables:
$\bm{z}^{s}_{i}\in\mathbb{R}^{L_{0}}$ which is shared, $\bm{z}^{x}_{i}\in\mathbb{R}^{L_{x}}$ and 
$\bm{z}^{x}_{i}\in\mathbb{R}^{L_{x}}$ which are private} :
\begin{center}
    \fr{$\begin{cases}
        \prob{\bm{z}_{i}} &= \mathcal{N}\left(\bm{z}^{s}_{i}|\bm{0},\bm{I}_{L_{s}}\right) 
        \mathcal{N}\left(\bm{z}^{x}_{i}|\bm{0},\bm{I}_{L_{x}}\right) 
        \mathcal{N}\left(\bm{z}^{y}_{i}|\bm{0},\bm{I}_{L_{y}}\right)\\ 
        \prob{\bm{x}_{i}|\bm{z}_{i}} &= \mathcal{N}\left(\bm{x}_{i}|\bm{B}_{x}\bm{z}_{i} +
        \bm{W}_{x}\bm{z}^{s}_{i} + \bm{\mu}_{x}, \sigma^{2}\bm{I}_{D_{x}}\right)\\
        \prob{\bm{y}_{i}|\bm{z}_{i}} &= \mathcal{N}\left(\bm{y}_{i}|\bm{B}_{y}\bm{z}_{i} +
        \bm{W}_{y}\bm{z}^{s}_{i} + \bm{\mu}_{y}, \sigma^{2}\bm{I}_{D_{y}}\right)
    \end{cases}$}
\end{center}

\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Independent Component Analysis (ICA)}
\paragraph{Purpose}
When we want to deconvolve mixed signals into their constituent parts.\\
\tB{Unlike PCA, we relax the Gaussian assumption about latent variable, now the 
distribution can be any non-Gaussian distribution.}
\uB{The reason the Gaussian distribution is disallowed as a source prior in ICA is that 
it does not permit unique recovery of the sources.}

\paragraph{Assumptions}
\paragraph{Theory}
$\prob{\bm{z}_{t}} = \prd{j=1}{L}\mathbb{P}_{j}\left(z_{tj}\right)$
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}



\subsection{DBSCAN: Density-Based Spatial Clustering of Applications with Noise}
\paragraph{Purpose}
Given a set of points in some space $(p_{i})_{1\leq i\leq n}\in\mathcal{S}^{n}$, it \uB{groups
together points that are closely packed together}, making then as \tB{outliers points that lie alone
in low density regions}.
\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{Definition}
Let $\epsilon\in\mathbb{R}$ the \uB{radius of a neighborhood} with respect to some points, 
$\rho_{min}$ a threshold of point count. Consider 2 points $p$ and $q$.
\begin{itemize}
    \item $\#\left\{m:~\norm{m-p}<\epsilon\right\} > \rho_{min} \Rightarrow p\text{ is \tB{\textbf{
                    core point}}}$
    \item 
        $\begin{cases}
            p\text{ is a core point} \\
            \norm{q-p} < \epsilon
        \end{cases} \Rightarrow~q\text{ is \tB{\textbf{directly reachable from}} } p$
    \item Let $n\in\mathbb{N}$ .
$\exists (m_{i})_{1\leq i\leq n}\in\mathcal{S}^{n},
        \begin{cases}
            (m_{1} = p) \wedge (m_{n} = q)\\
            \forall i\in\inter{1}{n-1}~m_{i+1}\text{ is directly reachable from }m_{i}
        \end{cases}\Rightarrow q\text{ \tB{\emph{is reachable from}} } p$
    \item Points that are \uB{not reachable from a core point are considered as \emph{outliers}}
    \item If there is a point $m$ from which 2 points $p$ and $q$ are reachable from $m$ then $p$ and
        $q$ are \tB{density-connected}
\end{itemize}
\subparagraph{Algorithm}
\begin{enumerate}
    \item For each point $m$, count the number of points at a distance lower than $\epsilon$ from $m$
    \item Identify the core points, the ones having a neighborhood count greater than $\rho_{min}$
    \item Delimit clusters with the reachable property of the different core points.
    \item For each non-core points assign it to the closer cluster being at a distance lower than
        $\epsilon$, if no cluster is found consider this point as noise.
\end{enumerate}


\paragraph{Strengths}
\begin{itemize}
    \item Number of cluster does not need to be specified
    \item Clusters are \uB{arbitrary-shaped}
    \item Robust to outliers
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item For a given border point that is reachable by multiple clusters, the cluster with which 
        it will be associated with depends on the database order.
    \item Due to euclidean distance to measure distance between 2 points, DBSCAN can suffer from 
        "curse of dimensionality"
    \item Cannot cluster well data sets with large differences in densities, since $\rho_{min}$ and
        $\epsilon$ would not be set appropriately for each cluster.
    \item If the data is not well understood, choosing meaningful $\rho_{min}$ and $\epsilon$
        is difficult.
\end{itemize}

\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Affinity Propagation}
\paragraph{Purpose}
The idea is that each point must choose another data point as its exemplar or centroid
some data points will choose themselves as centroids and this will automatically determine
the number of clusters 
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Dirichlet Process}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Spectral Clustering}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Agglomerative Clustering}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

\subsection{Bayesian Hierarchical Clustering}
\paragraph{Purpose}
\paragraph{Assumptions}
\paragraph{Theory}
\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}

%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
