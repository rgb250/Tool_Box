\subsection{Directed graphical models}
\paragraph{Purpose}
Relevant to compactly represent the joint distribution $\prob{\bm{x}|\bm{\theta}}$ and 
then \textit{infer} one set of variables given another, and how \textit{learn} the
parameters of this distribution.
\paragraph{Assumptions}
\paragraph{Theory}
\subparagraph{Graph terminology}
let's consider a \emph{graph} $G=(\mathcal{V}, \mathcal{E})$ consisting of a set of 
\emph{vertices} or nodes $\mathcal{V} = {v}_{1\leq v\leq V}$ and \emph{edges}, 
$\mathcal{E} = \left\{(s,t): s,t\in\mathcal{V}^{2}\right\}$
\begin{itemize}
    \item \emph{Cycle}: series a cycle to be a series of nodes such that we can get 
        back to where we started by following edges.
    \item \emph{DAG}: Directed Acyclic Graph is a drected graph without cycles.
    \item \emph{Tree}: Undirected graph without cycles.
    \item \emph{Forest}: set of trees
\end{itemize}
A \textbf{Directed Graphical Model (DGM)} are more commonly known as \emph{Bayesian 
Networks}.\\
In partitionning the data into \emph{visible variables} $\bm{x}_{v}$ and \emph{hdden
variables} $\bm{x}_{h}$, inference refers to computing the posterior distribution of 
the unknows given the knows:
\begin{center}
    $\prob{\bm{x}_{h}|\bm{x}_{v},\bm{\theta}} = \dfrac{p(\bm{x}_{h}, \bm{x}_{v}|
    \bm{\theta})}{p(\bm{x}_{v}|\bm{\theta})} = \dfrac{p(\bm{x}_{h}, \bm{x}_{v}|
\bm{\theta})}{\su{\bm{x'}_{h}}{}p(\bm{x'}_{h}, \bm{x}_{v}|\bm{\theta})}$
\end{center}


\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}


\subsection{Kernels}
\paragraph{Purpose}
Measuring the similarity between 2 objects that does not require preprocessing them 
into feature vector format.

\paragraph{Assumptions}
\paragraph{Theory}
\begin{itemize}
    \item \emph{Radial Basis Function (RBF)}, or Gaussian kernel:  $k(\bm{x}, \bm{x}')
        = e^{\frac{1}{2}(\bm{x} - \bm{x}')^{T}\bm{\Sigma^{-1}}(\bm{x} - \bm{x}')}$
    \item \emph{Mercer} kernel: kernel for which the \emph{Gram matrix} 
        $K=\begin{pmatrix}
            k(\bm{x}_{1}, \bm{x}_{1}) & \cdots & k(\bm{x}_{1}, \bm{x}_{n}) \\
                                      & \vdots &\\
            k(\bm{x}_{n}, \bm{x}_{1}) & \cdots & k(\bm{x}_{n}, \bm{x}_{n}) 
        \end{pmatrix}$ \uB{is positive definite}
    \item \emph{Linear} kernel: $k(\bm{x},\bm{x}') = \bm{x}^{T}\bm{x}'$, useful if 
        original data is already high dimensional and features individually informative
    \item \emph{Matern} kernels: $k(r) = \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{
        \sqrt{2\nu r}}{l}\right)^{\nu}\bm{K}_{\nu}\left(\frac{\sqrt{2\nu r}}{l}\right)$
        where $r=\norm{\bm{x} - \bm{x}'}$, $\nu >0$, $l>0$ and $\bm{K}_{\nu}$ a 
        modified Bessel function. 
    \item \emph{Probability product} kernels: $k(\bm{x}_{i},\bm{x}_{j}) = 
        \Su{}{}\prob{\bm{x}|\bm{x}_{i}}^{\rho}\prob{\bm{x}|\bm{x}_{j}}^{\rho}d\bm{x}$
        where $\rho > 0$, relevant for a probabilistic generative model.
    \item \emph{Fisher} kernels: $k(\bm{x},\bm{x}') = \bm{g}(\bm{x})^{T}\bm{F}^{-1}
        \bm{g}(\bm{x}')^{T}$ where $g$ is the gradient of the log-likelihood and 
        $\bm{F}$ is the Fisher information matrix. 
\end{itemize}
\subparagraph{Kernel machines}
is a GLM where the input feature vector has the form $\phi(\bm{x}) = \left(
k(\bm{x}, \bm{\mu}_{k})\right)_{1\leq k\leq K}$ where $\bm{\mu}_{k}$ being the kth 
centroid.
We can use any of the sparsity-promoting priors for $\bm{w}$ to efficiently select a 
subset of the training exemplars, it is \uB{sparse vector machine}.
We can get even greater sparsity by using \emph{ARD/SBL} resulting in a method called
the Relevance Vector Machine (RVM).
\subparagraph{Kernel trick}
Rather than defining our feature vector in terms of kernels, $\phi(\bm{x}) = \left(
k(\bm{x},\bm{x}_{i})\right)_{1\leq i\leq n}$, \tR{we can instead work with the 
original feature vectors $\bm{x}$ but modify algorithm so that we replace all inner 
product $\sP{\bm{x}}{\bm{x}'}$ with a call to the kernel function 
$k(\bm{x}, \bm{x}')$}.

\subparagraph{Kernalized model}
\begin{itemize}
    \item \emph{Nearest Neighbor (Classification)}: in using $\norm{\bm{x}_{i}
        -\bm{x}_{i'}}^{2}_{2} = \sP{\bm{x}_{i}}{\bm{x}_{i}}  +
        \sP{\bm{x}_{i'}}{\bm{x}_{i'}} -2\sP{\bm{x}_{i}}{\bm{x}_{i'}}$
    \item \emph{K-medoids (Clustering)}: similar to K-means, but instead of
        representing each cluster's centroid by the mean of all data vectors
        assigned to this cluster, we make each centroid be one of the data 
        vectors themselves. \uB{When we update the centroids, we look at each 
        object that belong to the cluster and measure the sum of its 
        distance to all others in the same cluster} with: $m_{k} = 
        \argmin_{i:z_{i}=k}\su{i':z_{i'}=k}{}
        \norm{\bm{x}_{i}-\bm{x}_{i'}}^{2}_{2}$ where $z_{i} = \argmin_{k} 
        d(i,m_{k})$
    \item \emph{Ridge Regression}: using the matrix inversion lemme, $\bm{w}_{
        Ridge} = \bm{X}_{T}\left(\bm{X}\bm{X}^{T}+\lambda\bm{I}_{n}\right)^{
        -1}\bm{y}$, we can partially kernalize this by replacing $\bm{X}
        \bm{X}^{T}$ with the \emph{Gram matrix}. Let's define \uB{\textbf{dual
        variables} $\bm{\alpha} \triangleq \left(\bm{K}+\lambda\bm{I}_{n}
        \right)\bm{y}$} then we rewrite the \uB{\textit{primal variables} 
        $\bm{w}= \bm{X}^{T}\alpha$}$=\su{i=1}{n}\alpha_{i}\bm{x}_{i}^{T}\bm{x}
        _{i} = \su{i=1}{n}\alpha_{i}k(\bm{x}_{i},\bm{x})$
    \item \emph{PCA}: \uB{from the eigenvectors of the inner product matrix 
        $\bm{X}\bm{X}^{T}$, allowing to produce a nonlinear embedding using
        kernel trick}. The mathematical demonstration is long and not 
        important.
\end{itemize}


\paragraph{Strengths}
\paragraph{Weaknesses}
\paragraph{Relationships with other methods}
\paragraph{Examples of application}
\begin{itemize}
    \item comparing documents
\end{itemize}


%\subsection{Logistic Regression}
%\paragraph{Purpose}
%\paragraph{Assumptions}
%\paragraph{Theory}
%\paragraph{Strengths}
%\paragraph{Weaknesses}
%\paragraph{Relationships with other methods}
%\paragraph{Examples of application}
