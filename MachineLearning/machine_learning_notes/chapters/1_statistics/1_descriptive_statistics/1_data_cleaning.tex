\paragraph{Data Quality} 
\subparagraph{Validity}
\begin{itemize}
    \item \textbf{Data-Type Constraints}: for a given column a fixed data-type must be associated with.
    \item \textbf{Range Constraints}: only a range of values should be taken.
    \item \textbf{Mandatory Constraints}: some columns cannot be empty.
    \item \textbf{Unique Constraints}: across a given dataset a field or a combination of
    \item \textbf{Foreign-key constraints}: a foreign key column cannot have a value that
        does not exist in the primary key.
    \item \textbf{Regular expression patterns}: text fields that have to follow a given 
        alphanumerical pattern.
    \item \textbf{Cross-field validation}: consistency of values, for example considering
        a given man, his birth date have to be older than his death date.
\end{itemize}

\subparagraph{Accuracy}
The degree to which the data is close to the true value.
\cite{omar_elgabry_the_ultimate_guide_to_data_cleaning}

\subparagraph{Completeness}
The degree to which the all the required data is known.

\subparagraph{Consistency}
The degree to which the data is consistent, within the same data set or across multiple 
data sets.

\subparagraph{Uniformity}
The degree to which the data is specified using the same unit of measure.


\paragraph{The workflow}
\subparagraph{Inspection} 
Detect unexpected behavior in the data.
\begin{itemize}
    \item \textbf{Data profiling}: summary statistics about the data, see \href{https://github.com/ydataai/ydata-profiling}{ydata-profiling} in Python.
    \item \textbf{Visualizations}: visualize the data using statistical metrics, see
        \href{https://plotly.com/python/}{plotly}
    \item \textbf{Software packages}: to note and check the constraints regarding the data
        see \href{https://pypi.org/project/pydeequ/}{pydeequ}
\end{itemize}

\subparagraph{Cleaning} 
Fix or remove anomalies discovered in the above phase.
\begin{itemize}
    \item \textbf{Irrelevant Data}: ask to the expert what can be the unnecessary columns,
        check them and remove them if they are not useful.
    \item \textbf{Duplicates}
    \item \textbf{Type conversion}: make sure the appropriate data type is associated with
        a given column.
    \item \textbf{Syntax errors}: white spaces, pad strings \dots
    \item \textbf{Standardize}: same unit across the dataset, same pattern for text.
    \item \textbf{Scaling/Transformation}: in order to compare different scores for 
        example.
    \item \textbf{Normalization}: useful for some statistical methods.
    \item \textbf{Missing values}: 
        \begin{itemize}
            \item Drop: only if the missing values in a column rarely and randomly occur.
            \item Impute: many methods, \textit{mean} is relevant when data is not skewed 
                otherwise we should use \textit{median}. A linear regression or a hot-deck
                (copying of values) approach can be taken as well, and more interestingly 
                a \textit{k-nearest} method approach.
            \item Flag: let the missing value as it is.
        \end{itemize}
    \item Outliers: Remove outliers only if they are harmful for the chosen model.
    \item In-record \& cross-datasets errors: fix non-consistent situations like married 
        kids, quantity being different of the one when we compute using other columns.
\end{itemize}

\subparagraph{Verifying}
Check correctness of the cleaning phase.
\subparagraph{Reporting} 
Report about changes made, using one of the software summarising the data quality for 
example.



