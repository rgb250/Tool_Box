Originally developed \tB{to study sending messages from discrete alphabets over a noisy channel}
(like communication via radio transmission).
The basic intuition behind information theory is that learning that an unlikely event has occurred
is more informative than learning that a likely event has occurred.
\subsection{Shannon entropy}
\paragraph{Purpose}
It is a \tB{measure of the uncertainty in a random variable}.
\paragraph{Theory}
We have self-information
\begin{center}
    $I(x) = -\log(\prob{x})$
\end{center}
Depending of the basis of the used logarithm, the unit can be expressed in \emph{nats} if the base is
\emph{e} or \emph{bits} if the basis is 2.\\
One \emph{nat} is the amount of information gained by observing an event of probability $\frac{1}{e}$


\begin{center}
    \fr{$\mathbb{H}(X) \triangleq -\mathbb{E}_{X\hookrightarrow P}(I(x))= -\su{k=1}{K}\prob{X=k}\log\left(\prob{X=k}\right)$}
\end{center}
It gives a lower bound on the number of bits needed on average to encode symbols drawn from a 
distribution $P$.
The discrete distribution with \uB{maximum entropy is the uniform distribution},
conversely the one with \uB{minimum entropy is any delta-function} that puts all its
mass on one state.


\subsection{Kullback-Leibler (KL) divergence}
\paragraph{Purpose}
It is a \tB{measure of dissimilarity of 2 probability distribution $p$ and $q$}.
\paragraph{Theory}
\tB{\begin{align*}
    \mathbb{KL}(p||q) &= \su{k=1}{K}p_{k}\log\left(\dfrac{p_{k}}{q_{k}}\right)\\
                      &= \su{k=1}{K}p_{k}\log(p_{k}) - \su{k=1}{K}p_{k}\log(q_{k})\\
                      &= -\mathbb{H}(p) + \mathbb{H}(p,q)
\end{align*}}
\tB{It gives the extra amount of information needed to send a message containing symbols drawn from
probability distribution $P$ when we use a code that was designed to minimize the length of messages
drawn from probability distribution $Q$}
Where $\mathbb{H}(p,q)$ is called \emph{cross entropy}, \tB{being the average number of
bits needed to encode data coming from a source with distribution $p$ when we use model
$q$ to define our codebook}.

\subsection{Mutual information}
\paragraph{Purpose}
Correlation coefficient is quite restrictive, a more general approach is to determine
\tB{how similar the joint distribution $p(X,Y)$ is to the factorized distribution $p(X)
p(Y)$}

\paragraph{Theory}
\subparagraph{Discrete}
\begin{center}
    \fr{$\mathbb{I}(X,Y) \triangleq \mathbb{KL}\left(p(X,Y)||p(X)p(Y)\right) =
    \mathbb{H}(X)- \mathbb{H}(X|Y) = \mathbb{H}(Y)-\mathbb{H}(Y|X)$}
\end{center}
where $\mathbb{H}(Y|X)$ is the \tB{\emph{conditional entropy}} defined as 
\tB{$\mathbb{H}(Y|X) =\Sigma_{x}p(x)\mathbb{H}(Y|X=x)$}
It is \uB{the extra amount of information needed to send a message containing symbols drawn from
probability distribution $P$ when use a code that was designed to minimize the length of messages
drawn from probability distribution $Q$}.
Thus we can interpret the \tR{\emph{Mutual Information} between $X$ and $Y$ as the 
reduction in uncertainty about $X$ after observing Y, or, by symmetry about $Y$ after
observing $X$}.

\subparagraph{Continuous}
Quantizing can have significant impact on the results, we could then try to estimate
many different bin sizes and locations to finally compute the maximum MI achieved 
called \emph{maximal information coefficient}:
\begin{center}
    $m(x,y) = \dfrac{\max_{G\in\mathcal{G}(x,y)}\mathbb{I}\left(X(G),Y(G)\right)}{
    \log\left(min(x,y)\right)}$
\end{center}
where $\mathcal{G}(x,y)$ is the set of 2d grids of size $x\times y$ and $X(G), Y(G)$
represents a discretization of the variable onto the grid.
\tB{$MIC\triangleq\max_{x,y:xy<B}m(x,y)$}


