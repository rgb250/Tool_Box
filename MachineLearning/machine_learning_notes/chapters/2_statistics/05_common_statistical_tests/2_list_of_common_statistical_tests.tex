\paragraph{\hyperref[binomial_test]{Binomial}}
To check if the deviations from a theoretically expected distribution of observations into
2 categories.\\

\subparagraph{Assumptions}
\begin{itemize}
    \item \tB{Sample items are independent.}
    \item Items are dichotomous and nominal.
    \item The sample size is significantly less than the population size
    \item The sample is a fiar representation of the population
\end{itemize}


\subparagraph{Frequentist}
Let define a user-defined probability $p_{0}$, with $H_{0}: p = p_{0}$ and
$\begin{cases}
    H_{1}: p \neq p_{0}\text{: two-tailed test} \\
    H_{1}: p < p_{0}\text{: left-tailed test} \\
    H_{1}: p > p_{0}\text{: right-tailed test} \\
     
\end{cases}$

\subparagraph{Bayesian}
Define the prior distribution with a \emph{Beta}($a,b$) distribution\\

\textit{\hyperref[statistical_method_table]{Return to the table.}}


\paragraph{$\bm{\chi^{2}}$ test}
Either used to test \emph{goodness-of-fit} and \emph{independence} between 2 variables.
It checks either if there is a significant difference between the expected and observed 
frequencies.
\begin{itemize}
    \item \emph{goodness-of-fit}: expected frequencies are computed with a theoretical 
        relationship between observed frequencies
    \item \emph{independence}: expected frequencies are computed with observed frequencies
        from the other sample
\end{itemize}
\subparagraph{Assumptions}
\begin{itemize}
    \item simple random sample
    \item sample with a sufficiently large size is assumed, for small sample size see Cash test
    \item expected cell count has to be adequate, a rule of thumb is at least 5 for 2-by-2
        table and 5 or more in 80\% of cells in larger tables.
    \item Independence of the observations
\end{itemize}

\subparagraph{Frequentist}
$\chi^{2} = \su{i=1}{n}\dfrac{\left(\frac{O_{i}}{N} - p_{i} \right)^{2}}{p_{i}}
\begin{cases}
    O_{i}\text{: number of observations of type }i \\
    N\text{: total number of observations}\\
    n\text{: number of cells in the table.}
    p_{i}\text{: expected proportions of the fraction of type}i\text{ in the population.}
\end{cases}
$

\subparagraph{Bayesian}
Does not exist, see \emph{contingency table}\\

\textit{\hyperref[statistical_method_table]{Return to the table.}}


\paragraph{Exact test of goodness-of-fit}
Unlike the conventional statistical tests, there is no \emph{test statistic}, we directly
compute the \emph{p-value} under the null hypothesis.
The most common use are for dichotomous nominal variables or multinomial variables.

\subparagraph{Assumptions}
\begin{itemize}
    \item \tB{Observations are independent.}
    \item Small sample size $\lessapprox 1000$
\end{itemize}

\subparagraph{Frequentist}
Let us define the list of, respectively, expected counts for each modality 
$i,~(E_{i})_{1\leq i\leq m}$, and observed counts $(O_{i})_{1\leq i\leq m}$.
Then
$\begin{cases}
    \bm{H_{0}: \forall i\in \inter{1}{m},~O_{i} = E_{i}}\\
    H_{1}: \exists i\in \inter{1}{m},~O_{i} \neq E_{i}\text{: two-tailed test} \\
\end{cases}$




\paragraph{Fisher's exact test}
To check the significance of the contingency between 2 kinds of classification of a given
object, \uB{initially Fisher used this test to distinguish drink in which the tea has been
put before the milk or vice-versa}. For large sample use \emph{G-test}

\subparagraph{Assumptions}
\begin{itemize}
    \item In practice, small sample size $\lessapprox 1000$
\end{itemize}

\subparagraph{Frequentist}
For example let's divide a population into male and female and for each persons indicating
if this person is currently studying or not. We want to test if the proportion of studying
students is higher among the women than among the men.
\begin{center}
    \begin{tabular}{|*{4}{c|}}
    \hline
    & \textbf{Men} & \textbf{Women} & \textbf{Row Total}\\
    \hline
    \textbf{Studying} & $a$ & $b$ & $a+b$ \\
    \hline
    \textbf{Non-Studying} & $c$ & $d$ & $c+d$ \\
    \hline
    \textbf{Column Total} & $a+c$ & $b+d$ & $a + b + c + d = n$ \\
    \hline
    \end{tabular}
\end{center}
The conditional on the margins of the table is distributed as $\text{\emph{Hypergeometric}}(a+c, a+b, c+d)$ meaning $a + c$ draws from a population with $a + b$ success and $c+d$ 
failures.
The probability of obtaining such set of values is given by
\begin{center}
    $p = \dfrac{{{a+b}\choose{a}}\times{{c+d}\choose{c}}}{{{n}\choose{a+c}}}
    = \dfrac{{{a+b}\choose{b}}\times{{c+d}\choose{d}}}{{{n}\choose{b+d}}}$
\end{center}

\subparagraph{Bayesian}
Does not exist, see \emph{contingency table} \\

\textit{\hyperref[statistical_method_table]{Return to the table.}}


\paragraph{G-test}

It's a likelihood-ratio or a maximum likelihood statistical significance test. 
Either used to test \emph{goodness-of-fit} and \emph{independence} between 2 variables.
It checks either if there is a significant difference between the expected and observed 
frequencies. This test tends to replace $\chi^{2}$\emph{-test}
\begin{itemize}
    \item \emph{goodness-of-fit}: expected frequencies are computed with a theoretical 
        relationship between observed frequencies
    \item \emph{independence}: expected frequencies are computed with observed frequencies
        from the other sample
    \item \emph{repeated tests}: first variable is analysed with a goodness-of-fit and the
        second one represents the repetition of the experiments multiple times. Thus it
        allows to assess the goodness-of-fit on a large sample instead of multiple lower
        samples. Expected frequencies is a theoretical relationship between the observed
        frequencies segmented in groups by the modalities of the second variable.
\end{itemize}


\subparagraph{Assumptions}
\begin{itemize}
    \item 
    \item Expected count must not be small in any modality.
\end{itemize}

\subparagraph{Strengths}
\begin{itemize}
    \item Approximation to the theoretical $chi^{2}$ distribution is better attained with
        \emph{G-test} than $\chi^{2}$ \emph{test}.
    \item Cases where $O_{i} > 2\times E_{i}$, \emph{G-test} is always better than
        $\chi^{2}$ \emph{test}.
\end{itemize}

\subparagraph{Weaknesses}
\begin{itemize}
    \item in test of independence, for a small sample size use rather Fisher's extract 
        test.
\end{itemize}


\subparagraph{Frequentist}
We compare the observed counts in each modality with their expected counts.
Let us define the list of, respectively, expected counts for each modality 
$i,~(E_{i})_{1\leq i\leq m}$, and observed counts $(O_{i})_{1\leq i\leq m}$.
Then
$\begin{cases}
    \bm{H_{0}: \forall i\in \inter{1}{m},~O_{i} = E_{i}}\\
    H_{1}: \exists i\in \inter{1}{m},~O_{i} \neq E_{i}\text{: two-tailed test} \\
\end{cases}$

\begin{center}
    $G = 2\su{i=1}{m}O_{i}\times\ln\left(\dfrac{O_{i}}{E_{i}}\right)$
\end{center}

$$
\ln\left(\dfrac{L(\tilde{\theta}|x)}{L(\hat{\theta}|x)}\right)
= \ln\left(\dfrac{\prd{i=1}{m}\tilde{\theta}^{x_{i}}}{\prd{i=1}{m}\hat{\theta}^{x_{i}}}\right)
= \ln\left(\dfrac{\prd{i=1}{m}\left(\frac{x_{i}}{n}\right)^{x_{i}}}{\prd{i=1}{m}\left(\frac{e_{i}}{n}\right)^{x_{i}}}\right)
= \ln\left(\prd{i=1}{m}\left(\dfrac{x_{i}}{e_{i}}\right)^{x_{i}}\right)
= \su{i=1}{m}x_{i}\ln\left(\dfrac{x_{i}}{e_{i}}\right)
$$

Then we multiply by $-2$ to get \emph{G-test} that is asymptotically equivalent to the 
\emph{Pearson's} $\chi^{2}$ formula.


\paragraph{Cochran's Q test}
It checks if $k$ treatments have identical effect, the response can take only 2 possible 
outcomes and a second variable segments the treatments.

\begin{center}
    \begin{tabularx}{.65\textwidth}{|*{5}{c|}}
    \hline
     & \textbf{Treatment 1} & \textbf{Treatment 2} & $\cdots$  & \textbf{Treatment k}\\
    \hline
    \emph{Block 1} & $x_{11}$ & $x_{12}$ & $\cdots$  & $x_{1k}$\\
    \hline
    \emph{Block 2} & $x_{21}$ & $x_{22}$ & $\cdots$  & $x_{2k}$\\
    \hline
    \emph{Block 3} & $x_{31}$ & $x_{32}$ & $\cdots$  & $x_{3k}$\\
    \hline
    $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
    \hline
    \emph{Block b} & $x_{b1}$ & $x_{b2}$ & $\cdots$  & $x_{bk}$\\
    \hline
    \end{tabularx}
\end{center}
And $\forall (i,j)\in\inter{1}{b}\times\inter{1}{k}, x_{ij}\in\left\{0, 1\right\}$

\subparagraph{Assumptions}
\begin{itemize}
    \item The blocks are randomly selected from the population of all possible blocks.
    \item Outcome of the treatments are dichotomous, and should be coded in a standard way
\end{itemize}

\subparagraph{Frequentist}
For example if $b$ respondents in a survey had each been asked $k$ \emph{Yes/No} questions
the \emph{Q-test} could be use to test the null hypothesis that all questions were equally
likely to elicit the answer "Yes".\\
We have
$\begin{cases}
    H_{0}\text{: the treatments are equally effective} \\
    H_{a}\text{: the treatments are \emph{not} equally effective} 
\end{cases}$

\begin{center}
    $
    T = k(k-1)\dfrac{\su{j=1}{k}\left(x_{\cdot j} - \frac{N}{k}\right)^{2}}{
    \su{i=1}{b}x_{i\cdot}\left(k-x_{i\cdot}\right)}
    \begin{cases}
        k\text{: number of treatments} \\
        x_{\cdot j}\text{: column total for the }j\text{th treatment} \\
        b\text{: number of blocks} \\
        X_{i\cdot}\text{: row total for the }i\text{th block} \\
        N\text{: grand total}
         
    \end{cases}$
\end{center}

For significance level $\alpha$, the asymptotic critical region is 
$T > \chi^{2}_{1-\alpha, k-1}$ which is the ($a-\alpha$) quantile of the $\chi^{2}$ 
distribution with $K-1$ degrees of freedom.

\subparagraph{Bayesian}
Does not exist, see \emph{contingency table} \\



\paragraph{Sign test}
It is a statistical method to test for consistent differences between pairs of 
observations, such as the weight of subjects before and after treatment.
For comparisons of paired observations $(x, y)$ the \emph{sign-test} is most useful if 
comparison can only be expressed as $x>y,~x=y,\text{ or } x<y$.
If instead the differences can be expressed in numeric quantities it is worthy to use 
\emph{t-test} or \emph{Wilcoxon signed-rank test} will usually have greater power than
the sign test to detect consistent differences.

\subparagraph{Frequentist}
Let $p=\Prob{X>Y}$, then
$\begin{cases}
    H_{0}:~p=0.5\text{ meaning that given a random pair of measurements }(x_{i}, y_{i})
    \text{ then }x_{i}\text{ and }y_{i}\text{are equally likely to be larger than the
    other}\\
    H_{a}:~p\neq 0.5
\end{cases}$
Pairs are omitted for which there is no differences so that there is a potential reduced
sample of $m$ pairs.\\
The statistics $W$ is defined as follow:
\begin{center}
    $W = \mathbf{1}_{\left\{x_{i} > y_{i}\right\}} \hookrightarrow \mathcal{B}(m, 0.5)$
\end{center}

\subparagraph{Assumptions}
Let $\forall i\in\inter{1}{n},~Z_{i} = X_{i} - Y_{i}$

\begin{itemize}
    \item $Z_{i}$ are assumed independent.
    \item Each $Z_{i}$ comes from the same continuous population.
    \item The values $X_{i}$ and $Y_{i}$ are ordered.
\end{itemize}

\subparagraph{Strengths}
\begin{itemize}
    \item A fewer assumptions need to be made than for parametrical test
\end{itemize}

\subparagraph{Weaknesses}
\begin{itemize}
    \item The power of test is lower than for a parametrical test
\end{itemize}



\paragraph{Contingency coefficients: Cramér's V}
To quantify associations between 2 paired samples in a contingency table, it is based
on $\chi^{2}$ and varies from 0 (no association) to 1 (complete association).
\subparagraph{Frequentist}
Let a sample of size $n$ of the simultaneously distributed variable $A$ and $B$.
$\forall (i,j)\in\inter{1}{r}\times\inter{1}{c},~n_{ij} = \text{\emph{Card}}\left(
\left\{A_{i},B_{j}\right\}\right)$.
Then $\chi^{2} = \su{(i,j)\in\inter{1}{r}\times\inter{1}{c}}{}\dfrac{\left(n_{ij} - \frac{
n_{i\cdot}\times n_{\cdot j}}{n}\right)^{2}}{\frac{n_{i\cdot}\times n_{\cdot j}}{n}}$\\
Finally the Cramér's V with bias correction is: 
\begin{center}
    $V = \sqrt{\dfrac{\max\left(0, \frac{\chi^{2}}{n} - \frac{(r-1)(c-1)}{n}\right)}{\min\left(r-\frac{(r-1)^{2}}{n-1}-1, c-\frac{(c-1)^{2}}{n-1}-1\right)}}$
\end{center}

\subparagraph{Assumptions}
\begin{itemize}
    \item The both variables have to be nominal.
\end{itemize}

\subparagraph{Strengths}
\begin{itemize}
    \item Good analog of the $R^{2}$ for categorical variables.
\end{itemize}

\subparagraph{Weaknesses}
\begin{itemize}
    \item Can tend to overestimate the strength of association.
\end{itemize}



\paragraph{Contingency table from a Bayesian perspective}
To test the independence hypothesis between 2 variables.
\subparagraph{Bayesian}
Let's consider 4 sampling plans, depending on which sampling plan is chosen the Bayes
factor formula will change.
\begin{itemize}
    \item \emph{Poisson} sampling scheme: Each cell count is considered as random and so
        is the grand total, the cells are Poisson distributed. This design often occurs
        in purely observational work.
    \item \emph{Joint multinomial} sampling scheme: same as above except that now, the 
        grand total is fixed. 
    \item \emph{Independent multinomial} sampling scheme: either all row margins or all 
        column margins are fixed, this scheme is frequently used in psychological studies.
    \item \emph{Hypergeometric} sampling scheme: here both row margins and column margins are fixed. Practical use of this scheme is rare!  
\end{itemize}
Bayes factors are often difficult to compute, as they are obtained by integrating out over
the entire parameter space, a process that is non-trivial when the integrals are 
high-dimensional and intractable. \\
Then we will use the 4 Bayes Factor developed by \emph{Gunnel and Dickey in 1974}, because
they only require computation of common functions such as gamma functions, for which 
numerical approximation are already available.\\
Here the logic: the Bayes Factor $BF^{i+1}_{01}$ computed at the observation $i+1$, 
contains the information up to the step $i$ with the extra information of the step $i+1$. 
We can then see $BF^{i+1}_{01}$ as the Bayes factor of the observation $i+1$ conditioned
on the observation $i$.\\
Finally thanks to the successive conditionalization the Bayes Factor are easy to compute.

\subparagraph{Assumptions}
\begin{itemize}
    \item We need to be consider data providing from one of the following sampling scheme: \emph{Poisson}, \emph{Joint multinomial}, \emph{Independent multinomial} or 
        \emph{Hypergeometric}
\end{itemize}

\subparagraph{Strengths}
\begin{itemize}
    \item Bayesian approach, then no issue to assess the significance
    \item Implemented in R
\end{itemize}

\subparagraph{Weaknesses}
\begin{itemize}
    \item Restricted to the above sampling scheme today. 
\end{itemize}


\paragraph{Wilconox test}
Non parametric test, used to test the location of a population based on a data sample or 
to compare the locations of two populations using two matching samples.\\
It is a good alternative of the \emph{t-test} when the mean is not of interest for the 
studied population.
\subparagraph{Frequentist}
Let $Y$ and $X$ be 2 random variables, and $\left(x_{i}, y_{i}\right)_{1\leq i\leq n}$
a paired sample. 
\begin{enumerate}
    \item $\forall i\in\inter{1}{n},~\left|x_{i}\right|$
    \item Sort the $\left(\left|x_{i}\right|\right)_{1\leq i\leq n}$ and assign a rank
        $\left(R_{i}\right)_{1\leq i\leq n}$
    \item The test statistic i $T=\su{i=1}{n}sgn\left(X_{i}\right)R_{i}$
    \item Produce a \emph{p-value} by computing T to its distribution under the null
        hypothesis.
\end{enumerate}
We will provide the logic for a one-sample test, the two-sample follows the same logic but
with 2 variables.

Assume the data consists of independent and identically distributed (IID) samples from a
distribution $F$ then consider 2 variables $(X_{1}, X_{2})\hookrightarrow IID(F)$
Define $p_{2} = \Prob{\dfrac{X_{1} + X_{2}}{2}>0} = 1-F^{(2)}(0)$
Then Wilcoxon signed-rank \emph{sum} $\rightarrow H_{0}:~p_{2} = \dfrac{1}{2}$
In restricting the distributions of interest we can reach more interpretable null and 
alternative hypotheses. On mildly restrictive is that $F^(2)$ has a unique median $\mu$. 
This median is called pseudo median of $F$
Then we have $H_{0}:~\mu=0$
\begin{itemize}
    \item  
\end{itemize}


\subparagraph{Assumptions}
\begin{itemize}
    \item Distribution $F$ is symmetric
\end{itemize}

\subparagraph{Strengths}
\subparagraph{Weaknesses}

TO COMPLETE

\paragraph{Mann-Whitney test}
Test for a randomly selected values $x$ and $y$ from 2 populations, $\Prob{x\leq y} = 
\Prob{x > y}$
\subparagraph{Frequentist}
Let $(n_{1}, n_{2})\mathcal{N}_{*}^{2}$ and $\left(x_{i}\right)_{1\leq i\leq n_{1}}$
and $\left(y_{i}\right)_{1\leq i\leq n_{2}}$ both samples independent of each other.\\
Then
$\begin{cases}
    U_{1} = n_{1}n_{2} + \frac{n_{1}(n_{1} + 1)}{2} - R_{1} \\
    U_{2} = n_{1}n_{2} + \frac{n_{2}(n_{2} + 2)}{2} - R_{2} 
\end{cases}$
$R_{1}, R_{2}$ being the sum of the ranks in groups 1 and 2. \\
Note that $AUC_{1}=\dfrac{U_{1}}{n_{1}n_{2}}$ meaning \emph{U}-statistics is related to
the area under the receiver operating characteristic.


\subparagraph{Assumptions}
\begin{itemize}
    \item All observation from both groups are independent
    \item Values are at least ordinal
    \item $H_{0}$: the distribution of both population is identical
    \item $H_{1}$: the 2 distribution of population are different
\end{itemize}

\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{Kruksal-Wallis test}
Non-parametrical to test if samples originate from the same distribution.
\subparagraph{Frequentist}
Let $N$ be the number of observations across all groups, $g$ number of groups, $n_{i}$ the
number of observation in the group $i$, $r_{ij}$ the rank of observation $j$ from group 
$i$.\\
And 
$\begin{cases}
    \overline{r}_{i\cdot} = \dfrac{\su{j=1}{n_{i}}r_{ij}}{n_{i}} \\
    \overline{r} = \dfrac{N + 1}{2}
\end{cases}$
\begin{itemize}
    \item Rank all data from all groups together
    \item $\left(N-1\right)\dfrac{\su{i=1}{g}n_{i}\left(\overline{r}_{i\cdot} - 
                \overline{r}\right)^{2}}{\su{i=1}{g}\su{j=1}{n_{j}}n_{i}\left(r_{ij} - 
        \overline{r}\right)^{2}}$
        To actually check the stochastic differences.
    \item A correction can be brought

\end{itemize}

\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{Friedman test}
\paragraph{Sperman test}
\subparagraph{Frequentist}
\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{Pearson correlation}
\subparagraph{Frequentist}
\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{Repeated-measures ANOVA}
\subparagraph{Frequentist}
\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{1-way ANOVA}
\subparagraph{Frequentist}
\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{Unpaired t-test}
\subparagraph{Frequentist}
\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{Paired t-test}
\subparagraph{Frequentist}
\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}

\paragraph{1-sample t-test}
\subparagraph{Frequentist}
\subparagraph{Assumptions}
\subparagraph{Strengths}
\subparagraph{Weaknesses}
